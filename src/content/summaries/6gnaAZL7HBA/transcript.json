{
  "videoId": "6gnaAZL7HBA",
  "language": "en",
  "source": "caption-extractor",
  "segments": [
    {
      "start": 0.08,
      "duration": 3.12,
      "text": "We're living in what should be a science"
    },
    {
      "start": 1.439,
      "duration": 3.84,
      "text": "fiction moment. Trillions of parameters"
    },
    {
      "start": 3.2,
      "duration": 4.8,
      "text": "in our models. The labs are spending on"
    },
    {
      "start": 5.279,
      "duration": 5.121,
      "text": "the order of 1% of GDP. Yet models will"
    },
    {
      "start": 8,
      "duration": 4.08,
      "text": "still feel unreliable where it matters."
    },
    {
      "start": 10.4,
      "duration": 3.92,
      "text": "Benchmarks might say genius. And"
    },
    {
      "start": 12.08,
      "duration": 4.8,
      "text": "everyday users might say useful idiot."
    },
    {
      "start": 14.32,
      "duration": 4.4,
      "text": "When you tell it to fix a bug, it fixes"
    },
    {
      "start": 16.88,
      "duration": 3.28,
      "text": "the bug and it reintroduces a bug. You"
    },
    {
      "start": 18.72,
      "duration": 2.88,
      "text": "tell it to fix that bug, it reintroduces"
    },
    {
      "start": 20.16,
      "duration": 3.199,
      "text": "the old bug and you go back and forth."
    },
    {
      "start": 21.6,
      "duration": 4.48,
      "text": "Ilia points the finger at training for"
    },
    {
      "start": 23.359,
      "duration": 4.561,
      "text": "this. He says pre-training is a very"
    },
    {
      "start": 26.08,
      "duration": 3.68,
      "text": "blunt instrument. You ingest all this"
    },
    {
      "start": 27.92,
      "duration": 3.119,
      "text": "text and what do you do with it? Right."
    },
    {
      "start": 29.76,
      "duration": 3.52,
      "text": "And and the refinements, the"
    },
    {
      "start": 31.039,
      "duration": 3.761,
      "text": "distortions, the skewing happens during"
    },
    {
      "start": 33.28,
      "duration": 2.88,
      "text": "reinforcement learning and post-"
    },
    {
      "start": 34.8,
      "duration": 3.36,
      "text": "training. And labs will design"
    },
    {
      "start": 36.16,
      "duration": 4.719,
      "text": "reinforcement learning environments to"
    },
    {
      "start": 38.16,
      "duration": 4.719,
      "text": "optimize for public benchmarks. And"
    },
    {
      "start": 40.879,
      "duration": 4.401,
      "text": "humans end up being reward hackers in"
    },
    {
      "start": 42.879,
      "duration": 4.641,
      "text": "this situation. Instead of the models"
    },
    {
      "start": 45.28,
      "duration": 4.48,
      "text": "gaming the reward, the researchers build"
    },
    {
      "start": 47.52,
      "duration": 3.92,
      "text": "training setups that just optimize for"
    },
    {
      "start": 49.76,
      "duration": 4.24,
      "text": "benchmark scores. And so when you"
    },
    {
      "start": 51.44,
      "duration": 4.4,
      "text": "combine that with poor generalization,"
    },
    {
      "start": 54,
      "duration": 3.92,
      "text": "you get models that look really good on"
    },
    {
      "start": 55.84,
      "duration": 4.399,
      "text": "tests and they can be really brittle"
    },
    {
      "start": 57.92,
      "duration": 4.4,
      "text": "when you step off the evaluation"
    },
    {
      "start": 60.239,
      "duration": 4.721,
      "text": "manifold or the evaluation part of the"
    },
    {
      "start": 62.32,
      "duration": 5.44,
      "text": "model. Now I want to call out here that"
    },
    {
      "start": 64.96,
      "duration": 5.92,
      "text": "this is something that we see not just"
    },
    {
      "start": 67.76,
      "duration": 5.679,
      "text": "in one model but to differing degrees in"
    },
    {
      "start": 70.88,
      "duration": 2.559,
      "text": "different models."
    }
  ],
  "fullText": "We're living in what should be a science fiction moment. Trillions of parameters in our models. The labs are spending on the order of 1% of GDP. Yet models will still feel unreliable where it matters. Benchmarks might say genius. And everyday users might say useful idiot. When you tell it to fix a bug, it fixes the bug and it reintroduces a bug. You tell it to fix that bug, it reintroduces the old bug and you go back and forth. Ilia points the finger at training for this. He says pre-training is a very blunt instrument. You ingest all this text and what do you do with it? Right. And and the refinements, the distortions, the skewing happens during reinforcement learning and post- training. And labs will design reinforcement learning environments to optimize for public benchmarks. And humans end up being reward hackers in this situation. Instead of the models gaming the reward, the researchers build training setups that just optimize for benchmark scores. And so when you combine that with poor generalization, you get models that look really good on tests and they can be really brittle when you step off the evaluation manifold or the evaluation part of the model. Now I want to call out here that this is something that we see not just in one model but to differing degrees in different models.",
  "fetchedAt": "2026-01-20T16:56:39.749Z"
}