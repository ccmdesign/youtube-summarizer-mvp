---
metadata:
  videoId: "DtePicx_kFY"
  title: "He Co-Invented the Transformer. Now: Continuous Thought Machines [Llion Jones / Luke Darlow]"
  description: "The Transformer architecture (which powers ChatGPT and nearly all modern AI) might be trapping the industry in a localized rut, preventing us from finding true intelligent reasoning, according to the person who co-invented it. Llion Jones and Luke Darlow, key figures at the research lab Sakana AI, join the show to make this provocative argument, and also introduce new research (CTM) which might lead the way forwards.


    We speak about \"Inventor's Remorse\" & The Trap of Success Despite being one of the original authors of the famous \"Attention Is All You Need\" paper that gave birth to the Transformer, Llion explains why he has largely stopped working on them. He argues that the industry is suffering from \"success capture\"—because Transformers work so well, everyone is focused on making small tweaks to the same architecture rather than discovering the next big leap.\ 


    **SPONSOR MESSAGES START**

    —

    Build your ideas with AI Studio from Google - http://ai.studio/build

    —

    Tufa AI Labs is hiring ML Research Engineers https://tufalabs.ai/\ 

    —

    cyber•Fund https://cyber.fund/?utm_source=mlst is a founder-led investment firm accelerating the cybernetic economy

    Hiring a SF VC Principal: https://talent.cyber.fund/companies/cyber-fund-2/jobs/57674170-ai-investment-principal#content?utm_source=mlst

    Submit investment deck: https://cyber.fund/contact?utm_source=mlst

    —

    **END**


    The \"Spiral\" Problem – Llion uses a striking visual analogy to explain what current AI is missing. If you ask a standard neural network to understand a spiral shape, it solves it by drawing tiny straight lines that just happen to look like a spiral. It \"fakes\" the shape without understanding the concept of spiraling. They argue that today's AI models are similar—they are incredible at mimicking intelligent answers without having an internal process of \"thinking\".


    Introducing the Continuous Thought Machine (CTM) Luke Darlow deep dives into their solution: a biology-inspired model that fundamentally changes how AI processes information.


    The Maze Analogy: Luke explains that standard AI tries to solve a maze by staring at the whole image and guessing the entire path instantly. Their new machine \"walks\" through the maze step-by-step.

    Thinking Time: This allows the AI to \"ponder.\" If a problem is hard, the model can naturally spend more time thinking about it before answering, effectively allowing it to correct its own mistakes and backtrack—something current Language Models struggle to do genuinely.

    The pair discuss the culture of Sakana AI, which is modeled after the early days of Google Brain/DeepMind. Llion nostalgically recalls that the Transformer wasn't born from a corporate mandate, but from random people talking over lunch about interesting problems.\ 


    https://sakana.ai/

    https://x.com/YesThisIsLion

    https://x.com/LearningLukeD


    TRANSCRIPT:

    https://app.rescript.info/public/share/crjzQ-Jo2FQsJc97xsBdfzfOIeMONpg0TFBuCgV2Fu8


    TOC:

    00:00:00 - Stepping Back from Transformers

    00:00:43 - Introduction to Continuous Thought Machines (CTM)

    00:01:09 - The Changing Atmosphere of AI Research

    00:04:13 - Sakana’s Philosophy: Research Freedom

    00:07:45 - The Local Minimum of Large Language Models

    00:18:30 - Representation Problems: The Spiral Example

    00:29:12 - Technical Deep Dive: CTM Architecture

    00:36:00 - Adaptive Computation & Maze Solving

    00:47:15 - Model Calibration & Uncertainty

    01:00:43 - Sudoku Bench: Measuring True Reasoning



    REFS:

    Why Greatness Cannot be planned [Kenneth Stanley]

    https://www.amazon.co.uk/Why-Greatness-Cannot-Planned-Objective/dp/3319155237

    https://www.youtube.com/watch?v=lhYGXYeMq_E\ 


    The Hardware Lottery [Sara Hooker]

    https://arxiv.org/abs/2009.06489

    https://www.youtube.com/watch?v=sQFxbQ7ade0\ 


    Continuous Thought Machines [Luke Darlow et al / Sakana]

    https://arxiv.org/abs/2505.05522

    https://sakana.ai/ctm/\ 

    https://youtu.be/5X9cjGLggv0 great walkthrough of algo by Yacine Mahdid


    LSTM: The Comeback Story? [Prof. Sepp Hochreiter]

    https://www.youtube.com/watch?v=8u2pW2zZLCs\ 


    Questioning Representational Optimism in Deep Learning: The Fractured Entangled Representation Hypothesis [Kumar/Stanley]

    https://arxiv.org/pdf/2505.11581\ 


    Intelligent Matrix Exponentiation [Thomas Fischbacher] (Spiral reference)

    https://arxiv.org/abs/2008.03936


    A Spline Theory of Deep Networks [Randall Balestriero]

    https://proceedings.mlr.press/v80/balestriero18b/balestriero18b.pdf\ 

    https://www.youtube.com/watch?v=86ib0sfdFtw\ 

    https://www.youtube.com/watch?v=l3O2J3LMxqI\ 


    On the Biology of a Large Language Model [Anthropic, Jack Lindsey et al]

    https://transformer-circuits.pub/2025/attribution-graphs/biology.html\ 


    The ARC Prize 2024 Winning Algorithm [Daniel Franzen and Jan Disselhoff] “The ARChitects”

    https://www.youtube.com/watch?v=mTX_sAq--zY


    Neural Turing Machine [Graves]

    https://arxiv.org/pdf/1410.5401\ 


    Adaptive Computation Time for Recurrent Neural Networks [Graves]

    https://arxiv.org/abs/1603.08983\ 


    Sudoko Bench [Sakana]\ 

    https://pub.sakana.ai/sudoku/"
  channel: "Machine Learning Street Talk"
  channelId: "UCMLtBahI5DMrt0NPvDSoIRQ"
  duration: "PT1H12M40S"
  publishedAt: "2025-11-23T17:11:59Z"
  thumbnailUrl: "https://i.ytimg.com/vi/DtePicx_kFY/hqdefault.jpg"
  youtubeUrl: "https://www.youtube.com/watch?v=DtePicx_kFY"
processedAt: "2026-01-12T23:28:39.628Z"
source: "youtube"
playlistId: "PL-SEjLl-bojVmsXOvG-TBp7DVv0McXJzn"
playlistName: "AI Summaries"
category: "ai"
tldr: "Llion Jones, co-inventor of the **Transformer**, is shifting focus away from the architecture to explore **Continuous Thought Machines (CTM)**.\ 

  - **Biological inspiration**: CTMs utilize neuron synchronization and internal recurrence rather than static attention layers.

  - **Native adaptive compute**: The system naturally spends more time on complex tasks without explicit penalties.

  - **Escaping t\n"
ai:
  provider: "gemini"
  model: "gemini-3-flash-preview"
  apiCalls: 1
  fallbackAttempts: 0
  inputTokens: 15137
  outputTokens: 1470
  totalTokens: 17150
  processingTimeMs: 14667
---

## Key Takeaways

Llion Jones and Luke Darlow argue that the AI industry is currently trapped in a **local minimum** of Transformer-based architectures, relying on brute-force scaling rather than structural innovation.

* **Continuous Thought Machines (CTM)**: A new architecture featuring an internal "thought dimension" that allows for sequential, latent reasoning before producing an output.

* **Neuron Level Models (NLM)**: Each individual neuron in a CTM is replaced by a small MLP, allowing neurons to possess their own internal dynamics and history.

* **Synchronization as Representation**: Instead of static state vectors, CTMs use the **synchronization** (dot product) of neuron activations over time to represent complex information.

* **Native Adaptive Compute**: Unlike Transformers, which use the same compute for every token, CTMs solve easy problems quickly and harder ones by "thinking" for more internal steps, resulting in near-perfect **model calibration**.

* **The Sudoku Bench**: A new challenge for AGI involving variant Sudokus that require **meta-reasoning** and the discovery of unique "break-ins" that current LLMs cannot solve.

## Summary

### The Post-Transformer Era and the Research Local Minimum
Llion Jones reflects on the creation of the **Transformer**, noting that it emerged from a "bottom-up" environment of research freedom that is increasingly rare in today's commercialized AI landscape. He expresses concern that the industry has entered a phase of "technology capture," where the overwhelming success of **Large Language Models (LLMs)** has stalled architectural exploration. Jones compares the current era to the pre-Transformer period of RNNs, where researchers made marginal improvements to LSTMs until a fundamental shift rendered those efforts redundant. He suggests that while Transformers are powerful **universal approximators**, they exhibit "jagged intelligence"—solving PhD-level problems while failing at basic logic—indicating a fundamental flaw in their representation of the world.

### Introduction to Continuous Thought Machines (CTM)
To address these flaws, Jones and Luke Darlow at Sakana AI developed the **Continuous Thought Machine (CTM)**. The architecture moves away from the static, layer-by-layer processing of Transformers toward a recurrent, biologically inspired system. The CTM is built on three pillars: an **internal thought dimension**, **Neuron Level Models (NLM)**, and **synchronization**. By allowing a model to process information across an internal temporal dimension, it can break down problems into sequential steps. This is demonstrated through a "Hello World" maze-solving task, where the model must trace a path step-by-step rather than predicting the entire solution in a single shot.

### Rethinking the Neuron and Representation
In a CTM, the traditional neuron (often a simple ReLU activation) is replaced with a **Neuron Level Model**, effectively a small MLP that processes its own history. This allows the system to maintain a continuous time series of activations. Crucially, the model's representation of a "thought" is not just the state of the neurons at a specific moment, but how those neurons **synchronize** with one another over time. By measuring the dot product of activation histories, the system creates a high-dimensional representation space that is far richer than standard state vectors. This approach mimics biological brain waves, where different frequencies and synchronization patterns correspond to different states of cognition.

### Adaptive Compute and Model Calibration
One of the most significant findings of the CTM research is its native ability for **adaptive computation**. In traditional AI, forcing a model to use less compute for easy tasks often requires complex loss functions or hyperparameters. In CTMs, the researchers use a loss function that monitors both performance and certainty. This naturally encourages the model to exit its "thought process" early for simple images (like a cat) while taking more time to disambiguate difficult classes. This results in nearly perfect **model calibration**, meaning the model's confidence scores accurately reflect its probability of being correct—a trait notably absent in modern deep learning models.

### Emergent Behaviors: Leapfrogging and Backtracking
During training on complex mazes, the CTM exhibited fascinating emergent behaviors that hint at human-like reasoning. When time-constrained, the model developed a **"leapfrogging" algorithm**, jumping ahead to a point in the maze and then tracing backward to fill in the path. It also showed signs of **backtracking**, where the internal activations would descend one path, realize it was a dead end, and then return to a previous junction to try another route. These behaviors are not explicitly programmed but fall out of the architecture’s recurrent, sequential nature.

### The Sudoku Bench and the Quest for AGI
Jones introduces **Sudoku Bench**, a dataset of variant Sudokus designed to test **meta-reasoning**. Unlike standard puzzles, these include unique natural language constraints (e.g., "one number in this rule is a lie") or overlaid mazes. Jones argues that current AI lacks the ability to find "break-ins"—the specific logical leap required to solve a handcrafted puzzle. By open-sourcing thousands of hours of expert reasoning from the "Cracking the Cryptic" You

Tube channel, Sakana AI aims to provide the "thought traces" necessary for models to move beyond simple pattern matching toward genuine deductive reasoning. This highlights the gap between current **LLMs** and a system capable of open-ended scientific discovery.

## Context

The video features **Llion Jones**, a co-author of the seminal 'Attention Is All You Need' paper and co-founder of **Sakana AI**, and **Luke Darlow**, a lead research scientist. This discussion is pivotal because it signals a strategic pivot by one of the architects of the current AI boom away from the Transformer architecture. Jones and Darlow argue that the industry's reliance on 'brute force' scaling of LLMs ignores fundamental requirements for intelligence, such as native adaptive compute and structured reasoning. 

This conversation contributes to the growing 'Beyond Transformers' discourse, which includes researchers looking into SSMs (State Space Models), Diffusion World Models, and neuro-symbolic hybrids. It is particularly relevant for ML engineers and researchers interested in **biologically inspired AI** and the limitations of current benchmarks. The episode provides a deep dive into Sakana AI's philosophy, which prioritizes 'following the gradient of interest' and research freedom—a philosophy heavily influenced by Kenneth Stanley’s 'Why Greatness Cannot Be Planned.'
