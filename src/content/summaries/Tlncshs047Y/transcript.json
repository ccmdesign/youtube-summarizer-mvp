{
  "videoId": "Tlncshs047Y",
  "language": "en",
  "source": "caption-extractor",
  "segments": [
    {
      "start": 3.12,
      "duration": 3.36,
      "text": "Everyone, happy holidays. It's it's"
    },
    {
      "start": 4.96,
      "duration": 3.52,
      "text": "really great to see you all. I'm very"
    },
    {
      "start": 6.48,
      "duration": 3.92,
      "text": "excited to help to share kind of this"
    },
    {
      "start": 8.48,
      "duration": 3.6,
      "text": "next step and uh the evolution of the"
    },
    {
      "start": 10.4,
      "duration": 2.88,
      "text": "software we're building. It's all been"
    },
    {
      "start": 12.08,
      "duration": 3.28,
      "text": "public, but it's getting to the point"
    },
    {
      "start": 13.28,
      "duration": 4.16,
      "text": "where it's uh really quite fun and quite"
    },
    {
      "start": 15.36,
      "duration": 4.4,
      "text": "interesting. And so, what is this Max"
    },
    {
      "start": 17.44,
      "duration": 4.24,
      "text": "thing like? I mean, I think that now"
    },
    {
      "start": 19.76,
      "duration": 4.88,
      "text": "people have kind of understood what Mojo"
    },
    {
      "start": 21.68,
      "duration": 4.88,
      "text": "is, but Max is less less well understood"
    },
    {
      "start": 24.64,
      "duration": 4.799,
      "text": "and we've been kind of as we're building"
    },
    {
      "start": 26.56,
      "duration": 4.24,
      "text": "it cautious to not like oversell. So"
    },
    {
      "start": 29.439,
      "duration": 3.361,
      "text": "before we dive into that, I'm going to"
    },
    {
      "start": 30.8,
      "duration": 3.919,
      "text": "kind of frame I think many of you know"
    },
    {
      "start": 32.8,
      "duration": 5.19,
      "text": "what we're about, but in case you're not"
    },
    {
      "start": 34.719,
      "duration": 4,
      "text": "aware of what modular is doing."
    },
    {
      "start": 37.99,
      "duration": 2.33,
      "text": "[clears throat] You know, today if you"
    },
    {
      "start": 38.719,
      "duration": 4.641,
      "text": "look at the modern AI stack that we all"
    },
    {
      "start": 40.32,
      "duration": 6.8,
      "text": "use, it's actually really cool, but it's"
    },
    {
      "start": 43.36,
      "duration": 5.28,
      "text": "also really complicated. Uh turns out"
    },
    {
      "start": 47.12,
      "duration": 4.08,
      "text": "that lots and lots and lots of people"
    },
    {
      "start": 48.64,
      "duration": 4.48,
      "text": "are working on AI today. Shocking."
    },
    {
      "start": 51.2,
      "duration": 3.28,
      "text": "You've heard this, right? So many"
    },
    {
      "start": 53.12,
      "duration": 4.16,
      "text": "people, so many companies, so much"
    },
    {
      "start": 54.48,
      "duration": 4.96,
      "text": "innovation, so much stuff going on. It's"
    },
    {
      "start": 57.28,
      "duration": 5.119,
      "text": "exciting, but it also leads to kind of a"
    },
    {
      "start": 59.44,
      "duration": 4.24,
      "text": "hot mess. There's all this stuff up and"
    },
    {
      "start": 62.399,
      "duration": 4.881,
      "text": "down the stack. Different serving"
    },
    {
      "start": 63.68,
      "duration": 5.6,
      "text": "frameworks, different uh APIs, different"
    },
    {
      "start": 67.28,
      "duration": 4.56,
      "text": "ways to program GPUs, different ways to"
    },
    {
      "start": 69.28,
      "duration": 3.92,
      "text": "do hosting, you could argue that modular"
    },
    {
      "start": 71.84,
      "duration": 3.04,
      "text": "is contributing to this problem, right?"
    },
    {
      "start": 73.2,
      "duration": 5.12,
      "text": "I mean, if there's 50 standards, you"
    },
    {
      "start": 74.88,
      "duration": 5.04,
      "text": "know, having 51 to go unify them all."
    },
    {
      "start": 78.32,
      "duration": 3.76,
      "text": "But there's a reason that we're doing"
    },
    {
      "start": 79.92,
      "duration": 4.4,
      "text": "this, right? And the reason is that we"
    },
    {
      "start": 82.08,
      "duration": 4.719,
      "text": "want to democratize AI. We want to"
    },
    {
      "start": 84.32,
      "duration": 6.159,
      "text": "democratize compute. Our mission is"
    },
    {
      "start": 86.799,
      "duration": 5.36,
      "text": "really to bring developers to the flops."
    },
    {
      "start": 90.479,
      "duration": 3.68,
      "text": "Take all these flops, all this"
    },
    {
      "start": 92.159,
      "duration": 3.6,
      "text": "innovation. We believe that at the end"
    },
    {
      "start": 94.159,
      "duration": 3.201,
      "text": "of Mo's law, hardware is only going to"
    },
    {
      "start": 95.759,
      "duration": 4.641,
      "text": "get wackier and more cool and more"
    },
    {
      "start": 97.36,
      "duration": 5.92,
      "text": "exciting. And yes, today's well-known"
    },
    {
      "start": 100.4,
      "duration": 5.039,
      "text": "GPUs are amazing and we want to provide"
    },
    {
      "start": 103.28,
      "duration": 4.08,
      "text": "access to them. But as we proceed, we"
    },
    {
      "start": 105.439,
      "duration": 3.841,
      "text": "want to really like rethink the software"
    },
    {
      "start": 107.36,
      "duration": 4.16,
      "text": "fundamentals, really break down the"
    },
    {
      "start": 109.28,
      "duration": 4.479,
      "text": "barriers and get more people into this"
    },
    {
      "start": 111.52,
      "duration": 3.68,
      "text": "ecosystem because right now it feels"
    },
    {
      "start": 113.759,
      "duration": 3.04,
      "text": "like only the biggest players in the"
    },
    {
      "start": 115.2,
      "duration": 3.599,
      "text": "world are able to do the fundamental"
    },
    {
      "start": 116.799,
      "duration": 3.92,
      "text": "work. And we think that's backwards. We"
    },
    {
      "start": 118.799,
      "duration": 3.761,
      "text": "think the innovation comes from the"
    },
    {
      "start": 120.719,
      "duration": 3.36,
      "text": "little guys, the people with they're"
    },
    {
      "start": 122.56,
      "duration": 2.72,
      "text": "closest to the applications, the people"
    },
    {
      "start": 124.079,
      "duration": 3.281,
      "text": "that are pushing the boundaries, the"
    },
    {
      "start": 125.28,
      "duration": 4.88,
      "text": "people are the hungriest. They may not"
    },
    {
      "start": 127.36,
      "duration": 4,
      "text": "have $15 million to go train an LLM. So"
    },
    {
      "start": 130.16,
      "duration": 2.88,
      "text": "if you're curious, I've written some"
    },
    {
      "start": 131.36,
      "duration": 3.76,
      "text": "blog posts. I need to get to writing the"
    },
    {
      "start": 133.04,
      "duration": 4.559,
      "text": "next one. But we've spent a lot of time"
    },
    {
      "start": 135.12,
      "duration": 3.92,
      "text": "talking about Mojo. Mojo is a good"
    },
    {
      "start": 137.599,
      "duration": 2.72,
      "text": "thing. I'm very excited about it. We've"
    },
    {
      "start": 139.04,
      "duration": 3.6,
      "text": "talked about that a number of times and"
    },
    {
      "start": 140.319,
      "duration": 4.64,
      "text": "it's a really good way to unify GPU"
    },
    {
      "start": 142.64,
      "duration": 6.08,
      "text": "programming and other accelerators."
    },
    {
      "start": 144.959,
      "duration": 5.761,
      "text": "But AI is more than just GPU kernels,"
    },
    {
      "start": 148.72,
      "duration": 4.4,
      "text": "right? AI is a much more complicated"
    },
    {
      "start": 150.72,
      "duration": 4.239,
      "text": "thing. You get modeling. So notably with"
    },
    {
      "start": 153.12,
      "duration": 3.92,
      "text": "PyTorch. I assume some people have used"
    },
    {
      "start": 154.959,
      "duration": 4.64,
      "text": "PyTorch here."
    },
    {
      "start": 157.04,
      "duration": 3.68,
      "text": "Um you if you want to use it in"
    },
    {
      "start": 159.599,
      "duration": 2.801,
      "text": "production, suddenly you care about"
    },
    {
      "start": 160.72,
      "duration": 2.4,
      "text": "performance. And so there's a whole"
    },
    {
      "start": 162.4,
      "duration": 2.479,
      "text": "bunch of different ways to get"
    },
    {
      "start": 163.12,
      "duration": 4.64,
      "text": "performance. A lot of ML error things,"
    },
    {
      "start": 164.879,
      "duration": 4.961,
      "text": "Triton, CUDA, lots of different ways to"
    },
    {
      "start": 167.76,
      "duration": 5.44,
      "text": "get performance. C++, Rust, lots and"
    },
    {
      "start": 169.84,
      "duration": 5.52,
      "text": "lots of things in this, but we need a AI"
    },
    {
      "start": 173.2,
      "duration": 4.48,
      "text": "is not itself useful in production"
    },
    {
      "start": 175.36,
      "duration": 3.76,
      "text": "unless it's fast. And finally, you"
    },
    {
      "start": 177.68,
      "duration": 2.88,
      "text": "actually need to serve this. A lot of a"
    },
    {
      "start": 179.12,
      "duration": 3.199,
      "text": "lot of data center workloads in"
    },
    {
      "start": 180.56,
      "duration": 3.039,
      "text": "particular need to be served. It turns"
    },
    {
      "start": 182.319,
      "duration": 2.56,
      "text": "out that when you get into this, you get"
    },
    {
      "start": 183.599,
      "duration": 2.961,
      "text": "into the complexity of serving lots of"
    },
    {
      "start": 184.879,
      "duration": 3.521,
      "text": "different users. And suddenly the"
    },
    {
      "start": 186.56,
      "duration": 3.679,
      "text": "problems compound. You get disagregated"
    },
    {
      "start": 188.4,
      "duration": 4.08,
      "text": "workloads. you get a lot of a lot of"
    },
    {
      "start": 190.239,
      "duration": 3.841,
      "text": "problems that don't exist with just AI"
    },
    {
      "start": 192.48,
      "duration": 5.44,
      "text": "being a forward pass like it used to be"
    },
    {
      "start": 194.08,
      "duration": 5.68,
      "text": "five five six years ago. So what we've"
    },
    {
      "start": 197.92,
      "duration": 3.52,
      "text": "learned as we leaned into this is we"
    },
    {
      "start": 199.76,
      "duration": 2.64,
      "text": "said hey we're going to build this mojo"
    },
    {
      "start": 201.44,
      "duration": 3.6,
      "text": "thing. We're going to build this cool"
    },
    {
      "start": 202.4,
      "duration": 5.04,
      "text": "way to get performance out of GPUs."
    },
    {
      "start": 205.04,
      "duration": 4.16,
      "text": "Let's go use PyTorch."
    },
    {
      "start": 207.44,
      "duration": 4.32,
      "text": "PieTorch is a good thing. I think many"
    },
    {
      "start": 209.2,
      "duration": 4.56,
      "text": "people love PyTorch. I love PyTorch. But"
    },
    {
      "start": 211.76,
      "duration": 4.08,
      "text": "PyTorch wasn't really designed for gen."
    },
    {
      "start": 213.76,
      "duration": 4.08,
      "text": "It wasn't really designed for inference."
    },
    {
      "start": 215.84,
      "duration": 4.319,
      "text": "And I think the world has also figured"
    },
    {
      "start": 217.84,
      "duration": 4.24,
      "text": "this out, right? We I think we're not"
    },
    {
      "start": 220.159,
      "duration": 4.08,
      "text": "unique in noticing this. We've seen"
    },
    {
      "start": 222.08,
      "duration": 4.64,
      "text": "tools like VLM and SG Lang and many"
    },
    {
      "start": 224.239,
      "duration": 5.441,
      "text": "other tools kind of come in and try to"
    },
    {
      "start": 226.72,
      "duration": 4.56,
      "text": "help PyTorch because fundamentally what"
    },
    {
      "start": 229.68,
      "duration": 3.04,
      "text": "PyTorch was designed for was for"
    },
    {
      "start": 231.28,
      "duration": 3.12,
      "text": "traditional AI. It was designed for"
    },
    {
      "start": 232.72,
      "duration": 3.519,
      "text": "model training. It's it's super"
    },
    {
      "start": 234.4,
      "duration": 3.28,
      "text": "important. But today's inference"
    },
    {
      "start": 236.239,
      "duration": 2.961,
      "text": "workloads are actually quite different."
    },
    {
      "start": 237.68,
      "duration": 2.96,
      "text": "They're only getting more complicated."
    },
    {
      "start": 239.2,
      "duration": 4.64,
      "text": "Getting the host and the accelerator to"
    },
    {
      "start": 240.64,
      "duration": 4.799,
      "text": "work together super important. Um, and"
    },
    {
      "start": 243.84,
      "duration": 4.64,
      "text": "things like PyTorch were not really"
    },
    {
      "start": 245.439,
      "duration": 4.641,
      "text": "designed for that kind of a world. Um,"
    },
    {
      "start": 248.48,
      "duration": 2.959,
      "text": "as you lean into this further and now"
    },
    {
      "start": 250.08,
      "duration": 3.359,
      "text": "you start caring about hardware, you"
    },
    {
      "start": 251.439,
      "duration": 3.761,
      "text": "start to realize tools and technologies"
    },
    {
      "start": 253.439,
      "duration": 5.04,
      "text": "like SG Lang and VLM while they're"
    },
    {
      "start": 255.2,
      "duration": 5.92,
      "text": "awesome, they also like are really just"
    },
    {
      "start": 258.479,
      "duration": 3.841,
      "text": "a gigantic bag of custom kernels. If"
    },
    {
      "start": 261.12,
      "duration": 3.04,
      "text": "they're a gigantic bag of custom"
    },
    {
      "start": 262.32,
      "duration": 4.08,
      "text": "kernels, well, generally they get built"
    },
    {
      "start": 264.16,
      "duration": 3.599,
      "text": "for the industryleading hardware. And"
    },
    {
      "start": 266.4,
      "duration": 3.12,
      "text": "then if you try to bring up a new chip,"
    },
    {
      "start": 267.759,
      "duration": 3.841,
      "text": "you try to integrate with anybody else,"
    },
    {
      "start": 269.52,
      "duration": 5.84,
      "text": "suddenly you run to this challenge of"
    },
    {
      "start": 271.6,
      "duration": 5.92,
      "text": "well, okay, I I can implement VLM, but"
    },
    {
      "start": 275.36,
      "duration": 3.68,
      "text": "step one, rewrite all of the kernels for"
    },
    {
      "start": 277.52,
      "duration": 3.84,
      "text": "all of the models. And wait a second,"
    },
    {
      "start": 279.04,
      "duration": 4,
      "text": "that this is not a reusable framework."
    },
    {
      "start": 281.36,
      "duration": 3.119,
      "text": "This is a very useful tool, of course,"
    },
    {
      "start": 283.04,
      "duration": 4.4,
      "text": "but it wasn't really designed for this"
    },
    {
      "start": 284.479,
      "duration": 4.561,
      "text": "modern era. So, you know, we've been"
    },
    {
      "start": 287.44,
      "duration": 2.8,
      "text": "working on this for quite some time and"
    },
    {
      "start": 289.04,
      "duration": 3.28,
      "text": "we've been working on the different bits"
    },
    {
      "start": 290.24,
      "duration": 4.72,
      "text": "and pieces and like innovating the space"
    },
    {
      "start": 292.32,
      "duration": 5.28,
      "text": "is very complicated. But so today we're"
    },
    {
      "start": 294.96,
      "duration": 5.28,
      "text": "excited to introduce and reintroduce the"
    },
    {
      "start": 297.6,
      "duration": 7.2,
      "text": "max framework. And so this is a fully"
    },
    {
      "start": 300.24,
      "duration": 6.799,
      "text": "new genai native AI modeling and serving"
    },
    {
      "start": 304.8,
      "duration": 5.119,
      "text": "framework and it's built from the first"
    },
    {
      "start": 307.039,
      "duration": 5.201,
      "text": "principles for the modern world."
    },
    {
      "start": 309.919,
      "duration": 3.921,
      "text": "Now, Max framework being built from the"
    },
    {
      "start": 312.24,
      "duration": 2.959,
      "text": "ground up like a lot of us have worked"
    },
    {
      "start": 313.84,
      "duration": 2.48,
      "text": "on a lot of these different systems."
    },
    {
      "start": 315.199,
      "duration": 2.481,
      "text": "We're paying attention to all the cool"
    },
    {
      "start": 316.32,
      "duration": 3.76,
      "text": "research papers. We're seeing all the"
    },
    {
      "start": 317.68,
      "duration": 3.6,
      "text": "innovation that's happening and we have"
    },
    {
      "start": 320.08,
      "duration": 3.28,
      "text": "the opportunity to build a next"
    },
    {
      "start": 321.28,
      "duration": 3.199,
      "text": "generation system. And so, when you have"
    },
    {
      "start": 323.36,
      "duration": 3.36,
      "text": "the opportunity to build a next"
    },
    {
      "start": 324.479,
      "duration": 3.521,
      "text": "generation generation system, well,"
    },
    {
      "start": 326.72,
      "duration": 2.88,
      "text": "first of all, you can bring the best"
    },
    {
      "start": 328,
      "duration": 2.8,
      "text": "ideas from everywhere. Like, we're"
    },
    {
      "start": 329.6,
      "duration": 3.2,
      "text": "shameless. We'll take good ideas"
    },
    {
      "start": 330.8,
      "duration": 4.239,
      "text": "wherever they come, right? But I also"
    },
    {
      "start": 332.8,
      "duration": 3.6,
      "text": "feel a burden of responsibility because"
    },
    {
      "start": 335.039,
      "duration": 3.201,
      "text": "if you're going to do this, you should"
    },
    {
      "start": 336.4,
      "duration": 4.16,
      "text": "do it well. You should think about the"
    },
    {
      "start": 338.24,
      "duration": 4.56,
      "text": "implications. You should try things and"
    },
    {
      "start": 340.56,
      "duration": 3.919,
      "text": "when you make a mistake and we have, you"
    },
    {
      "start": 342.8,
      "duration": 3.2,
      "text": "should take a step back and fix it,"
    },
    {
      "start": 344.479,
      "duration": 3.761,
      "text": "right? You should be really deliberate"
    },
    {
      "start": 346,
      "duration": 4.56,
      "text": "about building a large scale system. And"
    },
    {
      "start": 348.24,
      "duration": 3.76,
      "text": "so with Max, you've got all the"
    },
    {
      "start": 350.56,
      "duration": 2.96,
      "text": "performance of graphs because graphs are"
    },
    {
      "start": 352,
      "duration": 3.039,
      "text": "a great way to get high performance"
    },
    {
      "start": 353.52,
      "duration": 4.239,
      "text": "systems going and it's really important"
    },
    {
      "start": 355.039,
      "duration": 4.081,
      "text": "for particularly modern workloads. We"
    },
    {
      "start": 357.759,
      "duration": 2.641,
      "text": "want to be easy to use. We want to"
    },
    {
      "start": 359.12,
      "duration": 2.72,
      "text": "provide lots of models. We want to have"
    },
    {
      "start": 360.4,
      "duration": 4.079,
      "text": "an amazing graph compiler. We want to"
    },
    {
      "start": 361.84,
      "duration": 4.88,
      "text": "make it easy to to learn and and for"
    },
    {
      "start": 364.479,
      "duration": 3.761,
      "text": "people to build into this e ecosystem."
    },
    {
      "start": 366.72,
      "duration": 3.759,
      "text": "Turns out one of the best things about"
    },
    {
      "start": 368.24,
      "duration": 4,
      "text": "PyTorch is eager mode. That's amazing"
    },
    {
      "start": 370.479,
      "duration": 4.881,
      "text": "for exploration and discovery. And so"
    },
    {
      "start": 372.24,
      "duration": 5.04,
      "text": "you need a world-class eager mode. Also"
    },
    {
      "start": 375.36,
      "duration": 3.44,
      "text": "the performance of graphs, right? We"
    },
    {
      "start": 377.28,
      "duration": 3.44,
      "text": "want to be portable. Of course, we want"
    },
    {
      "start": 378.8,
      "duration": 3.28,
      "text": "to take advantage of Mojo because I I"
    },
    {
      "start": 380.72,
      "duration": 4.64,
      "text": "don't know. I think the mojo is pretty"
    },
    {
      "start": 382.08,
      "duration": 4.88,
      "text": "cool. And uh what we won't talk about"
    },
    {
      "start": 385.36,
      "duration": 3.279,
      "text": "today is that you know none of this"
    },
    {
      "start": 386.96,
      "duration": 4.239,
      "text": "technology is actually specific to Gen"
    },
    {
      "start": 388.639,
      "duration": 4,
      "text": "AI. This is where a lot of the pain is"
    },
    {
      "start": 391.199,
      "duration": 2.801,
      "text": "and this is what we can use to justify"
    },
    {
      "start": 392.639,
      "duration": 4.081,
      "text": "building these tools. But if you want to"
    },
    {
      "start": 394,
      "duration": 4.319,
      "text": "do quantum computing or biochemistry or"
    },
    {
      "start": 396.72,
      "duration": 4.16,
      "text": "digital signal processing or anything"
    },
    {
      "start": 398.319,
      "duration": 4.241,
      "text": "else totally wacky and crazy, it's all"
    },
    {
      "start": 400.88,
      "duration": 4.24,
      "text": "about pushing around a huge amount of"
    },
    {
      "start": 402.56,
      "duration": 4.639,
      "text": "data and building and orchestrating"
    },
    {
      "start": 405.12,
      "duration": 3.76,
      "text": "compute. And this these tools and these"
    },
    {
      "start": 407.199,
      "duration": 3.12,
      "text": "techniques should be native and should"
    },
    {
      "start": 408.88,
      "duration": 4.72,
      "text": "be able to work for that even though"
    },
    {
      "start": 410.319,
      "duration": 4.401,
      "text": "it's not modular focus. So today, as"
    },
    {
      "start": 413.6,
      "duration": 3.039,
      "text": "Michael introduced, we're going to be"
    },
    {
      "start": 414.72,
      "duration": 4.479,
      "text": "talking and pulling back the covers on"
    },
    {
      "start": 416.639,
      "duration": 4.481,
      "text": "say the art graph compiler and you"
    },
    {
      "start": 419.199,
      "duration": 3.84,
      "text": "ferris will nerd out about ML internals"
    },
    {
      "start": 421.12,
      "duration": 4.079,
      "text": "and other cool stuff like this and we'll"
    },
    {
      "start": 423.039,
      "duration": 4.16,
      "text": "learn how easy it is to program at LLMs"
    },
    {
      "start": 425.199,
      "duration": 3.521,
      "text": "and we'll talk about eager mode and how"
    },
    {
      "start": 427.199,
      "duration": 3.201,
      "text": "we're going to push the state-of-the-art"
    },
    {
      "start": 428.72,
      "duration": 3.12,
      "text": "the world forward here. I'm really"
    },
    {
      "start": 430.4,
      "duration": 3.68,
      "text": "excited about the deep dive talks we"
    },
    {
      "start": 431.84,
      "duration": 4.56,
      "text": "have later today. And so if you look so"
    },
    {
      "start": 434.08,
      "duration": 3.839,
      "text": "today, Max covers this full spectrum."
    },
    {
      "start": 436.4,
      "duration": 2.799,
      "text": "You can design and build models, which"
    },
    {
      "start": 437.919,
      "duration": 3.12,
      "text": "we'll talk about. You can get good"
    },
    {
      "start": 439.199,
      "duration": 2.641,
      "text": "performance, which we'll talk about. One"
    },
    {
      "start": 441.039,
      "duration": 2.16,
      "text": "thing we're not going to talk about"
    },
    {
      "start": 441.84,
      "duration": 3.52,
      "text": "today is serving because we didn't have"
    },
    {
      "start": 443.199,
      "duration": 3.521,
      "text": "time, but um but we'll focus on the"
    },
    {
      "start": 445.36,
      "duration": 3.76,
      "text": "first two and we can talk about the"
    },
    {
      "start": 446.72,
      "duration": 3.599,
      "text": "third one at some other time."
    },
    {
      "start": 449.12,
      "duration": 3.12,
      "text": "So, one of the other cool things about"
    },
    {
      "start": 450.319,
      "duration": 3.841,
      "text": "Max is it's pervasively open source. And"
    },
    {
      "start": 452.24,
      "duration": 4.16,
      "text": "so, if you go dive into our repository,"
    },
    {
      "start": 454.16,
      "duration": 3.92,
      "text": "you can see lots of different models and"
    },
    {
      "start": 456.4,
      "duration": 2.96,
      "text": "there's more all the time. This is this"
    },
    {
      "start": 458.08,
      "duration": 2.399,
      "text": "is something we're continuing to invest"
    },
    {
      "start": 459.36,
      "duration": 2.32,
      "text": "in. We're building it out. We're working"
    },
    {
      "start": 460.479,
      "duration": 2.56,
      "text": "with a lot of design partners and"
    },
    {
      "start": 461.68,
      "duration": 4.4,
      "text": "customers on this which is cool. We'll"
    },
    {
      "start": 463.039,
      "duration": 5.201,
      "text": "have more announcements later. Um, you"
    },
    {
      "start": 466.08,
      "duration": 4.8,
      "text": "can find lots of kernels and so I think"
    },
    {
      "start": 468.24,
      "duration": 4.64,
      "text": "we have the world's largest open-source"
    },
    {
      "start": 470.88,
      "duration": 3.999,
      "text": "portable kernel library across Nvidia,"
    },
    {
      "start": 472.88,
      "duration": 4.56,
      "text": "AMD, and now starting to turn on Apple,"
    },
    {
      "start": 474.879,
      "duration": 4.32,
      "text": "which is also really exciting and cool."
    },
    {
      "start": 477.44,
      "duration": 2.879,
      "text": "We've got uh the serving components,"
    },
    {
      "start": 479.199,
      "duration": 2.241,
      "text": "which we don't have time to talk about"
    },
    {
      "start": 480.319,
      "duration": 2.72,
      "text": "today, but they're all public. You can"
    },
    {
      "start": 481.44,
      "duration": 4.56,
      "text": "go check it out. And so make it super"
    },
    {
      "start": 483.039,
      "duration": 4.481,
      "text": "easy to build and distribute servers."
    },
    {
      "start": 486,
      "duration": 4.8,
      "text": "And so we're we're tackling this whole"
    },
    {
      "start": 487.52,
      "duration": 6.079,
      "text": "stack. And so one of the things we"
    },
    {
      "start": 490.8,
      "duration": 4.48,
      "text": "believe in is making this accessible."
    },
    {
      "start": 493.599,
      "duration": 3.04,
      "text": "And so yeah, you can pip install it, you"
    },
    {
      "start": 495.28,
      "duration": 3.599,
      "text": "can use condo, you can do all the"
    },
    {
      "start": 496.639,
      "duration": 3.521,
      "text": "favorite tools, make it really easy to"
    },
    {
      "start": 498.879,
      "duration": 3.04,
      "text": "get up and running. And we want to make"
    },
    {
      "start": 500.16,
      "duration": 4.56,
      "text": "this super fast and easy. We'll show"
    },
    {
      "start": 501.919,
      "duration": 5.441,
      "text": "that later. And so as you dive into this"
    },
    {
      "start": 504.72,
      "duration": 5.28,
      "text": "again, like your your curiosity is the"
    },
    {
      "start": 507.36,
      "duration": 4.399,
      "text": "only limitation to what you can do. And"
    },
    {
      "start": 510,
      "duration": 3.279,
      "text": "you can dive into the Mojo standard"
    },
    {
      "start": 511.759,
      "duration": 3.441,
      "text": "library or dive into kernels or modeling"
    },
    {
      "start": 513.279,
      "duration": 4.64,
      "text": "APIs. And we've also said, of course,"
    },
    {
      "start": 515.2,
      "duration": 4.719,
      "text": "we're open sourcing Mojo uh next year as"
    },
    {
      "start": 517.919,
      "duration": 3.761,
      "text": "well. And so this is all part of our big"
    },
    {
      "start": 519.919,
      "duration": 3.841,
      "text": "plan to like build into this and really"
    },
    {
      "start": 521.68,
      "duration": 4,
      "text": "push this entire ecosystem forward"
    },
    {
      "start": 523.76,
      "duration": 3.28,
      "text": "because we know the stakes are high. We"
    },
    {
      "start": 525.68,
      "duration": 2.88,
      "text": "know this is really important. We want"
    },
    {
      "start": 527.04,
      "duration": 4.96,
      "text": "to do and we want feel a great"
    },
    {
      "start": 528.56,
      "duration": 5.6,
      "text": "responsibility to do this well. So"
    },
    {
      "start": 532,
      "duration": 3.92,
      "text": "that's my part. I think this is a really"
    },
    {
      "start": 534.16,
      "duration": 3.52,
      "text": "exciting time. I think the AI is so"
    },
    {
      "start": 535.92,
      "duration": 4.08,
      "text": "critical and important to everything we"
    },
    {
      "start": 537.68,
      "duration": 3.92,
      "text": "do. I want us to be pushing this"
    },
    {
      "start": 540,
      "duration": 2.72,
      "text": "forward. I want us to be relentless. I"
    },
    {
      "start": 541.6,
      "duration": 2.48,
      "text": "want us to be challenging the"
    },
    {
      "start": 542.72,
      "duration": 2.799,
      "text": "state-of-the-art. I want us to be"
    },
    {
      "start": 544.08,
      "duration": 3.28,
      "text": "pulling in the best ideas from wherever"
    },
    {
      "start": 545.519,
      "duration": 4.561,
      "text": "they are. So if anybody has any"
    },
    {
      "start": 547.36,
      "duration": 3.52,
      "text": "questions, I'm happy to take them."
    },
    {
      "start": 550.08,
      "duration": 2.8,
      "text": ">> Yeah,"
    },
    {
      "start": 550.88,
      "duration": 4.24,
      "text": ">> thank you. I'm new to the entire module"
    },
    {
      "start": 552.88,
      "duration": 4.639,
      "text": "ecosystem and a burning question for me"
    },
    {
      "start": 555.12,
      "duration": 3.92,
      "text": "is um how's the support for the w"
    },
    {
      "start": 557.519,
      "duration": 3.361,
      "text": "Windows environment?"
    },
    {
      "start": 559.04,
      "duration": 3.84,
      "text": ">> Windows is one of the things I'm sad we"
    },
    {
      "start": 560.88,
      "duration": 5.6,
      "text": "don't have great support. We do support"
    },
    {
      "start": 562.88,
      "duration": 5.04,
      "text": "WSL and so that that exists um uh and so"
    },
    {
      "start": 566.48,
      "duration": 3.52,
      "text": "Mojo fully supports that and you can"
    },
    {
      "start": 567.92,
      "duration": 4,
      "text": "totally use that. We haven't had time to"
    },
    {
      "start": 570,
      "duration": 3.6,
      "text": "invest as much as I'd like into that. If"
    },
    {
      "start": 571.92,
      "duration": 3.52,
      "text": "you'd like, please do bug us about that"
    },
    {
      "start": 573.6,
      "duration": 4.4,
      "text": "and we can talk about it later. I I"
    },
    {
      "start": 575.44,
      "duration": 4.88,
      "text": "would definitely bug you about it. I I'm"
    },
    {
      "start": 578,
      "duration": 3.76,
      "text": "I feel like that's a very important"
    },
    {
      "start": 580.32,
      "duration": 2.959,
      "text": "missing piece in the industry."
    },
    {
      "start": 581.76,
      "duration": 3.92,
      "text": ">> Yep, I agree."
    },
    {
      "start": 583.279,
      "duration": 5.441,
      "text": ">> Um so this is actually complete news to"
    },
    {
      "start": 585.68,
      "duration": 4.4,
      "text": "me. Um like you know I actually thought"
    },
    {
      "start": 588.72,
      "duration": 4.64,
      "text": "modular was just like building an"
    },
    {
      "start": 590.08,
      "duration": 5.84,
      "text": "acceleration layer and uh just seeing"
    },
    {
      "start": 593.36,
      "duration": 5.12,
      "text": "that it's much much bigger than that uh"
    },
    {
      "start": 595.92,
      "duration": 4,
      "text": "like you know this event is great to get"
    },
    {
      "start": 598.48,
      "duration": 2.72,
      "text": "the news out but the news needs to be"
    },
    {
      "start": 599.92,
      "duration": 3.68,
      "text": "out a lot more."
    },
    {
      "start": 601.2,
      "duration": 4,
      "text": ">> Yeah. Well, so so you're surprised we're"
    },
    {
      "start": 603.6,
      "duration": 4.64,
      "text": "building also an entirely new AI"
    },
    {
      "start": 605.2,
      "duration": 4.48,
      "text": "framework. Um there's a reason we"
    },
    {
      "start": 608.24,
      "duration": 3.36,
      "text": "haven't been talking too much about it,"
    },
    {
      "start": 609.68,
      "duration": 4.64,
      "text": "right? So first of all is it's only now"
    },
    {
      "start": 611.6,
      "duration": 4.32,
      "text": "just becoming real, right? We've been"
    },
    {
      "start": 614.32,
      "duration": 3.12,
      "text": "building components and we've been using"
    },
    {
      "start": 615.92,
      "duration": 3.68,
      "text": "it. We've been talking about performance"
    },
    {
      "start": 617.44,
      "duration": 4.079,
      "text": "and results and deliverables end to end"
    },
    {
      "start": 619.6,
      "duration": 4.32,
      "text": "with this and it's been very important."
    },
    {
      "start": 621.519,
      "duration": 3.921,
      "text": "But I think now January like we're"
    },
    {
      "start": 623.92,
      "duration": 3.599,
      "text": "coming together. It's like getting"
    },
    {
      "start": 625.44,
      "duration": 4.24,
      "text": "actually pretty good. Now we're not"
    },
    {
      "start": 627.519,
      "duration": 3.361,
      "text": "doing training. So people say, \"Oh,"
    },
    {
      "start": 629.68,
      "duration": 3.279,
      "text": "okay. Well, I'm not interested in that.\""
    },
    {
      "start": 630.88,
      "duration": 5.68,
      "text": "And this whatever I Justin inference"
    },
    {
      "start": 632.959,
      "duration": 5.521,
      "text": "saying fine someday we will so we'll get"
    },
    {
      "start": 636.56,
      "duration": 3.68,
      "text": "there and this is this is part of the"
    },
    {
      "start": 638.48,
      "duration": 5.68,
      "text": "philosophy that I I bring at least which"
    },
    {
      "start": 640.24,
      "duration": 6.88,
      "text": "is um doing really hard things takes"
    },
    {
      "start": 644.16,
      "duration": 5.44,
      "text": "time and so you have to be careful I"
    },
    {
      "start": 647.12,
      "duration": 3.6,
      "text": "think we learned that with Mojo 0.1 you"
    },
    {
      "start": 649.6,
      "duration": 3.359,
      "text": "have to be careful when you kind of"
    },
    {
      "start": 650.72,
      "duration": 4,
      "text": "declare success because if people come"
    },
    {
      "start": 652.959,
      "duration": 3.44,
      "text": "in and it's not actually good"
    },
    {
      "start": 654.72,
      "duration": 3.04,
      "text": ">> that's the impression you'll make and so"
    },
    {
      "start": 656.399,
      "duration": 2.801,
      "text": "we've been building in the open and a"
    },
    {
      "start": 657.76,
      "duration": 3.92,
      "text": "lot of this stuff has been out there but"
    },
    {
      "start": 659.2,
      "duration": 4.72,
      "text": "we don't we even today We're like with"
    },
    {
      "start": 661.68,
      "duration": 3.68,
      "text": "y'all. We're uh sharing some of this"
    },
    {
      "start": 663.92,
      "duration": 3.68,
      "text": "stuff, but I think we'll make a lot more"
    },
    {
      "start": 665.36,
      "duration": 7.039,
      "text": "noise about this uh next year as well."
    },
    {
      "start": 667.6,
      "duration": 6.32,
      "text": ">> Nice. Thank you. I I lost track of the"
    },
    {
      "start": 672.399,
      "duration": 5.761,
      "text": "evolution"
    },
    {
      "start": 673.92,
      "duration": 6.08,
      "text": "of the license for Mojo and Max and the"
    },
    {
      "start": 678.16,
      "duration": 4,
      "text": "whole ecosystem."
    },
    {
      "start": 680,
      "duration": 5.6,
      "text": "And I'm nearing to a point that I need"
    },
    {
      "start": 682.16,
      "duration": 6.32,
      "text": "to start having convincing answers about"
    },
    {
      "start": 685.6,
      "duration": 5.84,
      "text": "where the license is going to be uh in"
    },
    {
      "start": 688.48,
      "duration": 5.359,
      "text": "the future, right? I I know it's a kind"
    },
    {
      "start": 691.44,
      "duration": 5.36,
      "text": "of an unfair question, but can you maybe"
    },
    {
      "start": 693.839,
      "duration": 5.44,
      "text": "share a little bit more how you would"
    },
    {
      "start": 696.8,
      "duration": 5.039,
      "text": "hope the license would unfold because I"
    },
    {
      "start": 699.279,
      "duration": 4.56,
      "text": "know it's free. uh we have to tell you"
    },
    {
      "start": 701.839,
      "duration": 4.321,
      "text": "that we are using it but there's also"
    },
    {
      "start": 703.839,
      "duration": 4.641,
      "text": "the idea that if you're doing using for"
    },
    {
      "start": 706.16,
      "duration": 4.56,
      "text": "inferences there might be a charge later"
    },
    {
      "start": 708.48,
      "duration": 3.76,
      "text": "on or something like that. So something"
    },
    {
      "start": 710.72,
      "duration": 2.16,
      "text": "more about the license would be very"
    },
    {
      "start": 712.24,
      "duration": 2.88,
      "text": "helpful."
    },
    {
      "start": 712.88,
      "duration": 4.32,
      "text": ">> Yeah. So um so you're saying like"
    },
    {
      "start": 715.12,
      "duration": 2.64,
      "text": "basically how does modular make money?"
    },
    {
      "start": 717.2,
      "duration": 4.319,
      "text": ">> Yeah."
    },
    {
      "start": 717.76,
      "duration": 5.68,
      "text": ">> So uh so we are building a managed cloud"
    },
    {
      "start": 721.519,
      "duration": 5.361,
      "text": "platform. If you have inference you want"
    },
    {
      "start": 723.44,
      "duration": 6.639,
      "text": "to manage and like come let us know. Uh"
    },
    {
      "start": 726.88,
      "duration": 6,
      "text": "but uh but Mojo is roughly competing"
    },
    {
      "start": 730.079,
      "duration": 4.641,
      "text": "with Rust or CUDA or other free"
    },
    {
      "start": 732.88,
      "duration": 3.12,
      "text": "technologies. Max is competing with"
    },
    {
      "start": 734.72,
      "duration": 2.96,
      "text": "other free technologies. We want to be"
    },
    {
      "start": 736,
      "duration": 2.8,
      "text": "out there, right? And so the licenses"
    },
    {
      "start": 737.68,
      "duration": 4.159,
      "text": "take it, use it, you could scale it"
    },
    {
      "start": 738.8,
      "duration": 5.2,
      "text": "arbitrarily on Nvidia, we do ask you to"
    },
    {
      "start": 741.839,
      "duration": 3.601,
      "text": "tell us about it because we'd like to"
    },
    {
      "start": 744,
      "duration": 4.32,
      "text": "like know about that and work with you"
    },
    {
      "start": 745.44,
      "duration": 5.28,
      "text": "and maybe do a joint blog post. Um if"
    },
    {
      "start": 748.32,
      "duration": 4,
      "text": "you would like to work with us, well, we"
    },
    {
      "start": 750.72,
      "duration": 3.119,
      "text": "have limited engineering and so then we"
    },
    {
      "start": 752.32,
      "duration": 2.88,
      "text": "could talk about relationships and"
    },
    {
      "start": 753.839,
      "duration": 3.201,
      "text": "things like that. But no, you can take"
    },
    {
      "start": 755.2,
      "duration": 4.079,
      "text": "this and use it. And so this is all"
    },
    {
      "start": 757.04,
      "duration": 4,
      "text": "quite important. Um, we do have other"
    },
    {
      "start": 759.279,
      "duration": 3.201,
      "text": "relationships and partnerships we'll be"
    },
    {
      "start": 761.04,
      "duration": 3.2,
      "text": "announcing next year which are really"
    },
    {
      "start": 762.48,
      "duration": 3.68,
      "text": "cool too. And so there's there's more to"
    },
    {
      "start": 764.24,
      "duration": 3.599,
      "text": "come."
    },
    {
      "start": 766.16,
      "duration": 3.359,
      "text": "Um,"
    },
    {
      "start": 767.839,
      "duration": 4,
      "text": "I had a project where I needed training"
    },
    {
      "start": 769.519,
      "duration": 4.641,
      "text": "and I really liked being able to use"
    },
    {
      "start": 771.839,
      "duration": 5.041,
      "text": "Mojo for the kernel stuff and I keep"
    },
    {
      "start": 774.16,
      "duration": 5.28,
      "text": "hearing this sentiment. There was some"
    },
    {
      "start": 776.88,
      "duration": 4.399,
      "text": "stable HLO thing in Zigg that says the"
    },
    {
      "start": 779.44,
      "duration": 4.079,
      "text": "same thing. Oh, we're just inference."
    },
    {
      "start": 781.279,
      "duration": 4.8,
      "text": "And I I just I'm at the point now where"
    },
    {
      "start": 783.519,
      "duration": 5.841,
      "text": "I have to make my language because I h I"
    },
    {
      "start": 786.079,
      "duration": 5.921,
      "text": "don't understand why does no one want to"
    },
    {
      "start": 789.36,
      "duration": 4.159,
      "text": "support training. You say it's hard, but"
    },
    {
      "start": 792,
      "duration": 4.16,
      "text": "uh"
    },
    {
      "start": 793.519,
      "duration": 4.641,
      "text": "I don't understand what you mean when"
    },
    {
      "start": 796.16,
      "duration": 5.359,
      "text": "you say that."
    },
    {
      "start": 798.16,
      "duration": 6.4,
      "text": ">> Okay, you're on to me. It turns out that"
    },
    {
      "start": 801.519,
      "duration": 5.281,
      "text": "uh training is also Matt Moles."
    },
    {
      "start": 804.56,
      "duration": 4,
      "text": "Turns out that again we're arbitrarily"
    },
    {
      "start": 806.8,
      "duration": 4.08,
      "text": "general. Turns out that even though"
    },
    {
      "start": 808.56,
      "duration": 4.64,
      "text": "modular again is underelling what we're"
    },
    {
      "start": 810.88,
      "duration": 3.759,
      "text": "doing, other people are using Max to do"
    },
    {
      "start": 813.2,
      "duration": 3.439,
      "text": "training. So if you'd like to do"
    },
    {
      "start": 814.639,
      "duration": 3.601,
      "text": "training, go check out Naba. It's a"
    },
    {
      "start": 816.639,
      "duration": 4,
      "text": "community project. It's super awesome."
    },
    {
      "start": 818.24,
      "duration": 5.279,
      "text": "It does both uh Jack style and PyTorch"
    },
    {
      "start": 820.639,
      "duration": 5.041,
      "text": "style training using Max"
    },
    {
      "start": 823.519,
      "duration": 4.721,
      "text": "today."
    },
    {
      "start": 825.68,
      "duration": 5.2,
      "text": ">> Nabla Nabla."
    },
    {
      "start": 828.24,
      "duration": 5.36,
      "text": "And so you can also ask us later. And so"
    },
    {
      "start": 830.88,
      "duration": 3.84,
      "text": "yeah, so turns out that again what we're"
    },
    {
      "start": 833.6,
      "duration": 2.96,
      "text": "trying to do is we're trying to like set"
    },
    {
      "start": 834.72,
      "duration": 3.04,
      "text": "expectations. There's a lot of people"
    },
    {
      "start": 836.56,
      "duration": 3.44,
      "text": "that have been very excited about what"
    },
    {
      "start": 837.76,
      "duration": 3.759,
      "text": "we're doing and then"
    },
    {
      "start": 840,
      "duration": 2.88,
      "text": "I make the mistake of saying yeah and"
    },
    {
      "start": 841.519,
      "duration": 3.281,
      "text": "we'll do it for and then people are"
    },
    {
      "start": 842.88,
      "duration": 3.44,
      "text": "saying today I'm like no no no no no I"
    },
    {
      "start": 844.8,
      "duration": 3.76,
      "text": "can't we we need to get some other"
    },
    {
      "start": 846.32,
      "duration": 3.759,
      "text": "things first but but this is the great"
    },
    {
      "start": 848.56,
      "duration": 3.12,
      "text": "thing about community right is that we"
    },
    {
      "start": 850.079,
      "duration": 3.521,
      "text": "modular people don't have to do all the"
    },
    {
      "start": 851.68,
      "duration": 5.12,
      "text": "work right you can go build that you can"
    },
    {
      "start": 853.6,
      "duration": 5.359,
      "text": "go work with uh Tilman and say hey like"
    },
    {
      "start": 856.8,
      "duration": 4.399,
      "text": "how about this and what about or"
    },
    {
      "start": 858.959,
      "duration": 4.481,
      "text": "whatever whatever your your jam is and"
    },
    {
      "start": 861.199,
      "duration": 5.361,
      "text": "and go work on that and it's just math"
    },
    {
      "start": 863.44,
      "duration": 3.12,
      "text": "right that's a beautiful"
    }
  ],
  "fullText": "Everyone, happy holidays. It's it's really great to see you all. I'm very excited to help to share kind of this next step and uh the evolution of the software we're building. It's all been public, but it's getting to the point where it's uh really quite fun and quite interesting. And so, what is this Max thing like? I mean, I think that now people have kind of understood what Mojo is, but Max is less less well understood and we've been kind of as we're building it cautious to not like oversell. So before we dive into that, I'm going to kind of frame I think many of you know what we're about, but in case you're not aware of what modular is doing. [clears throat] You know, today if you look at the modern AI stack that we all use, it's actually really cool, but it's also really complicated. Uh turns out that lots and lots and lots of people are working on AI today. Shocking. You've heard this, right? So many people, so many companies, so much innovation, so much stuff going on. It's exciting, but it also leads to kind of a hot mess. There's all this stuff up and down the stack. Different serving frameworks, different uh APIs, different ways to program GPUs, different ways to do hosting, you could argue that modular is contributing to this problem, right? I mean, if there's 50 standards, you know, having 51 to go unify them all. But there's a reason that we're doing this, right? And the reason is that we want to democratize AI. We want to democratize compute. Our mission is really to bring developers to the flops. Take all these flops, all this innovation. We believe that at the end of Mo's law, hardware is only going to get wackier and more cool and more exciting. And yes, today's well-known GPUs are amazing and we want to provide access to them. But as we proceed, we want to really like rethink the software fundamentals, really break down the barriers and get more people into this ecosystem because right now it feels like only the biggest players in the world are able to do the fundamental work. And we think that's backwards. We think the innovation comes from the little guys, the people with they're closest to the applications, the people that are pushing the boundaries, the people are the hungriest. They may not have $15 million to go train an LLM. So if you're curious, I've written some blog posts. I need to get to writing the next one. But we've spent a lot of time talking about Mojo. Mojo is a good thing. I'm very excited about it. We've talked about that a number of times and it's a really good way to unify GPU programming and other accelerators. But AI is more than just GPU kernels, right? AI is a much more complicated thing. You get modeling. So notably with PyTorch. I assume some people have used PyTorch here. Um you if you want to use it in production, suddenly you care about performance. And so there's a whole bunch of different ways to get performance. A lot of ML error things, Triton, CUDA, lots of different ways to get performance. C++, Rust, lots and lots of things in this, but we need a AI is not itself useful in production unless it's fast. And finally, you actually need to serve this. A lot of a lot of data center workloads in particular need to be served. It turns out that when you get into this, you get into the complexity of serving lots of different users. And suddenly the problems compound. You get disagregated workloads. you get a lot of a lot of problems that don't exist with just AI being a forward pass like it used to be five five six years ago. So what we've learned as we leaned into this is we said hey we're going to build this mojo thing. We're going to build this cool way to get performance out of GPUs. Let's go use PyTorch. PieTorch is a good thing. I think many people love PyTorch. I love PyTorch. But PyTorch wasn't really designed for gen. It wasn't really designed for inference. And I think the world has also figured this out, right? We I think we're not unique in noticing this. We've seen tools like VLM and SG Lang and many other tools kind of come in and try to help PyTorch because fundamentally what PyTorch was designed for was for traditional AI. It was designed for model training. It's it's super important. But today's inference workloads are actually quite different. They're only getting more complicated. Getting the host and the accelerator to work together super important. Um, and things like PyTorch were not really designed for that kind of a world. Um, as you lean into this further and now you start caring about hardware, you start to realize tools and technologies like SG Lang and VLM while they're awesome, they also like are really just a gigantic bag of custom kernels. If they're a gigantic bag of custom kernels, well, generally they get built for the industryleading hardware. And then if you try to bring up a new chip, you try to integrate with anybody else, suddenly you run to this challenge of well, okay, I I can implement VLM, but step one, rewrite all of the kernels for all of the models. And wait a second, that this is not a reusable framework. This is a very useful tool, of course, but it wasn't really designed for this modern era. So, you know, we've been working on this for quite some time and we've been working on the different bits and pieces and like innovating the space is very complicated. But so today we're excited to introduce and reintroduce the max framework. And so this is a fully new genai native AI modeling and serving framework and it's built from the first principles for the modern world. Now, Max framework being built from the ground up like a lot of us have worked on a lot of these different systems. We're paying attention to all the cool research papers. We're seeing all the innovation that's happening and we have the opportunity to build a next generation system. And so, when you have the opportunity to build a next generation generation system, well, first of all, you can bring the best ideas from everywhere. Like, we're shameless. We'll take good ideas wherever they come, right? But I also feel a burden of responsibility because if you're going to do this, you should do it well. You should think about the implications. You should try things and when you make a mistake and we have, you should take a step back and fix it, right? You should be really deliberate about building a large scale system. And so with Max, you've got all the performance of graphs because graphs are a great way to get high performance systems going and it's really important for particularly modern workloads. We want to be easy to use. We want to provide lots of models. We want to have an amazing graph compiler. We want to make it easy to to learn and and for people to build into this e ecosystem. Turns out one of the best things about PyTorch is eager mode. That's amazing for exploration and discovery. And so you need a world-class eager mode. Also the performance of graphs, right? We want to be portable. Of course, we want to take advantage of Mojo because I I don't know. I think the mojo is pretty cool. And uh what we won't talk about today is that you know none of this technology is actually specific to Gen AI. This is where a lot of the pain is and this is what we can use to justify building these tools. But if you want to do quantum computing or biochemistry or digital signal processing or anything else totally wacky and crazy, it's all about pushing around a huge amount of data and building and orchestrating compute. And this these tools and these techniques should be native and should be able to work for that even though it's not modular focus. So today, as Michael introduced, we're going to be talking and pulling back the covers on say the art graph compiler and you ferris will nerd out about ML internals and other cool stuff like this and we'll learn how easy it is to program at LLMs and we'll talk about eager mode and how we're going to push the state-of-the-art the world forward here. I'm really excited about the deep dive talks we have later today. And so if you look so today, Max covers this full spectrum. You can design and build models, which we'll talk about. You can get good performance, which we'll talk about. One thing we're not going to talk about today is serving because we didn't have time, but um but we'll focus on the first two and we can talk about the third one at some other time. So, one of the other cool things about Max is it's pervasively open source. And so, if you go dive into our repository, you can see lots of different models and there's more all the time. This is this is something we're continuing to invest in. We're building it out. We're working with a lot of design partners and customers on this which is cool. We'll have more announcements later. Um, you can find lots of kernels and so I think we have the world's largest open-source portable kernel library across Nvidia, AMD, and now starting to turn on Apple, which is also really exciting and cool. We've got uh the serving components, which we don't have time to talk about today, but they're all public. You can go check it out. And so make it super easy to build and distribute servers. And so we're we're tackling this whole stack. And so one of the things we believe in is making this accessible. And so yeah, you can pip install it, you can use condo, you can do all the favorite tools, make it really easy to get up and running. And we want to make this super fast and easy. We'll show that later. And so as you dive into this again, like your your curiosity is the only limitation to what you can do. And you can dive into the Mojo standard library or dive into kernels or modeling APIs. And we've also said, of course, we're open sourcing Mojo uh next year as well. And so this is all part of our big plan to like build into this and really push this entire ecosystem forward because we know the stakes are high. We know this is really important. We want to do and we want feel a great responsibility to do this well. So that's my part. I think this is a really exciting time. I think the AI is so critical and important to everything we do. I want us to be pushing this forward. I want us to be relentless. I want us to be challenging the state-of-the-art. I want us to be pulling in the best ideas from wherever they are. So if anybody has any questions, I'm happy to take them. >> Yeah, >> thank you. I'm new to the entire module ecosystem and a burning question for me is um how's the support for the w Windows environment? >> Windows is one of the things I'm sad we don't have great support. We do support WSL and so that that exists um uh and so Mojo fully supports that and you can totally use that. We haven't had time to invest as much as I'd like into that. If you'd like, please do bug us about that and we can talk about it later. I I would definitely bug you about it. I I'm I feel like that's a very important missing piece in the industry. >> Yep, I agree. >> Um so this is actually complete news to me. Um like you know I actually thought modular was just like building an acceleration layer and uh just seeing that it's much much bigger than that uh like you know this event is great to get the news out but the news needs to be out a lot more. >> Yeah. Well, so so you're surprised we're building also an entirely new AI framework. Um there's a reason we haven't been talking too much about it, right? So first of all is it's only now just becoming real, right? We've been building components and we've been using it. We've been talking about performance and results and deliverables end to end with this and it's been very important. But I think now January like we're coming together. It's like getting actually pretty good. Now we're not doing training. So people say, \"Oh, okay. Well, I'm not interested in that.\" And this whatever I Justin inference saying fine someday we will so we'll get there and this is this is part of the philosophy that I I bring at least which is um doing really hard things takes time and so you have to be careful I think we learned that with Mojo 0.1 you have to be careful when you kind of declare success because if people come in and it's not actually good >> that's the impression you'll make and so we've been building in the open and a lot of this stuff has been out there but we don't we even today We're like with y'all. We're uh sharing some of this stuff, but I think we'll make a lot more noise about this uh next year as well. >> Nice. Thank you. I I lost track of the evolution of the license for Mojo and Max and the whole ecosystem. And I'm nearing to a point that I need to start having convincing answers about where the license is going to be uh in the future, right? I I know it's a kind of an unfair question, but can you maybe share a little bit more how you would hope the license would unfold because I know it's free. uh we have to tell you that we are using it but there's also the idea that if you're doing using for inferences there might be a charge later on or something like that. So something more about the license would be very helpful. >> Yeah. So um so you're saying like basically how does modular make money? >> Yeah. >> So uh so we are building a managed cloud platform. If you have inference you want to manage and like come let us know. Uh but uh but Mojo is roughly competing with Rust or CUDA or other free technologies. Max is competing with other free technologies. We want to be out there, right? And so the licenses take it, use it, you could scale it arbitrarily on Nvidia, we do ask you to tell us about it because we'd like to like know about that and work with you and maybe do a joint blog post. Um if you would like to work with us, well, we have limited engineering and so then we could talk about relationships and things like that. But no, you can take this and use it. And so this is all quite important. Um, we do have other relationships and partnerships we'll be announcing next year which are really cool too. And so there's there's more to come. Um, I had a project where I needed training and I really liked being able to use Mojo for the kernel stuff and I keep hearing this sentiment. There was some stable HLO thing in Zigg that says the same thing. Oh, we're just inference. And I I just I'm at the point now where I have to make my language because I h I don't understand why does no one want to support training. You say it's hard, but uh I don't understand what you mean when you say that. >> Okay, you're on to me. It turns out that uh training is also Matt Moles. Turns out that again we're arbitrarily general. Turns out that even though modular again is underelling what we're doing, other people are using Max to do training. So if you'd like to do training, go check out Naba. It's a community project. It's super awesome. It does both uh Jack style and PyTorch style training using Max today. >> Nabla Nabla. And so you can also ask us later. And so yeah, so turns out that again what we're trying to do is we're trying to like set expectations. There's a lot of people that have been very excited about what we're doing and then I make the mistake of saying yeah and we'll do it for and then people are saying today I'm like no no no no no I can't we we need to get some other things first but but this is the great thing about community right is that we modular people don't have to do all the work right you can go build that you can go work with uh Tilman and say hey like how about this and what about or whatever whatever your your jam is and and go work on that and it's just math right that's a beautiful",
  "fetchedAt": "2026-01-18T18:33:05.509Z"
}