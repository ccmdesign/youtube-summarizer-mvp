{
  "videoId": "fY3-YeveBgA",
  "language": "en",
  "source": "caption-extractor",
  "segments": [
    {
      "start": 0.08,
      "duration": 5.36,
      "text": "Okay, so the first models from Quen for"
    },
    {
      "start": 2.879,
      "duration": 6.401,
      "text": "2026 have dropped. And these are the"
    },
    {
      "start": 5.44,
      "duration": 6.159,
      "text": "Quen 3 VL embedding models. And the"
    },
    {
      "start": 9.28,
      "duration": 4.56,
      "text": "whole sort of cool thing about these is"
    },
    {
      "start": 11.599,
      "duration": 4.321,
      "text": "that these are multimodal embedding"
    },
    {
      "start": 13.84,
      "duration": 6.24,
      "text": "models, meaning that they can process"
    },
    {
      "start": 15.92,
      "duration": 5.6,
      "text": "both text, images, even videos. So in"
    },
    {
      "start": 20.08,
      "duration": 3.68,
      "text": "this video, I'm going to go through what"
    },
    {
      "start": 21.52,
      "duration": 3.919,
      "text": "multimodal embeddings are. We'll talk a"
    },
    {
      "start": 23.76,
      "duration": 4.08,
      "text": "little bit about them, how they work, do"
    },
    {
      "start": 25.439,
      "duration": 5.281,
      "text": "a little refresher on the embeddings"
    },
    {
      "start": 27.84,
      "duration": 4.96,
      "text": "themselves, what the reranker models do,"
    },
    {
      "start": 30.72,
      "duration": 4.08,
      "text": "and look at how they can be used with"
    },
    {
      "start": 32.8,
      "duration": 3.919,
      "text": "the embeddings. And then afterwards,"
    },
    {
      "start": 34.8,
      "duration": 3.279,
      "text": "I'll jump into some code showing you"
    },
    {
      "start": 36.719,
      "duration": 4.081,
      "text": "just sort of playing around with some of"
    },
    {
      "start": 38.079,
      "duration": 4.8,
      "text": "these models. And I'll show you how you"
    },
    {
      "start": 40.8,
      "duration": 4.56,
      "text": "can actually speed this up for much"
    },
    {
      "start": 42.879,
      "duration": 6,
      "text": "faster retrieval by making use of the"
    },
    {
      "start": 45.36,
      "duration": 5.92,
      "text": "Matrioska embeddings. Okay, so what are"
    },
    {
      "start": 48.879,
      "duration": 4.481,
      "text": "multimodal embeddings? Well, let's start"
    },
    {
      "start": 51.28,
      "duration": 4.48,
      "text": "off with just a quick refresher about"
    },
    {
      "start": 53.36,
      "duration": 5.12,
      "text": "what embeddings are. So, embeddings are"
    },
    {
      "start": 55.76,
      "duration": 5.119,
      "text": "basically a numerical representation of"
    },
    {
      "start": 58.48,
      "duration": 3.84,
      "text": "meaning. And that basically means we"
    },
    {
      "start": 60.879,
      "duration": 3.841,
      "text": "take the content that we're going to"
    },
    {
      "start": 62.32,
      "duration": 6.479,
      "text": "pass into the model and it will give us"
    },
    {
      "start": 64.72,
      "duration": 6.56,
      "text": "a vector out that numerically represents"
    },
    {
      "start": 68.799,
      "duration": 5.36,
      "text": "that content. Often what this allows us"
    },
    {
      "start": 71.28,
      "duration": 5.6,
      "text": "to do is do all sorts of similarity"
    },
    {
      "start": 74.159,
      "duration": 4.721,
      "text": "comparisons. So while in general terms"
    },
    {
      "start": 76.88,
      "duration": 4.16,
      "text": "it's very hard to get a model to"
    },
    {
      "start": 78.88,
      "duration": 4.32,
      "text": "actually compare perhaps two paragraphs"
    },
    {
      "start": 81.04,
      "duration": 4.16,
      "text": "of text and see if they're about the"
    },
    {
      "start": 83.2,
      "duration": 4.959,
      "text": "same thing, if we convert them to"
    },
    {
      "start": 85.2,
      "duration": 5.2,
      "text": "vectors, it makes it a lot easier for us"
    },
    {
      "start": 88.159,
      "duration": 4.481,
      "text": "to then basically use something like"
    },
    {
      "start": 90.4,
      "duration": 4.56,
      "text": "cosine distance or another distance"
    },
    {
      "start": 92.64,
      "duration": 5.2,
      "text": "metric to basically look at these two"
    },
    {
      "start": 94.96,
      "duration": 4.88,
      "text": "vectors and see how close together they"
    },
    {
      "start": 97.84,
      "duration": 4.239,
      "text": "actually are. Now the multimodal leap"
    },
    {
      "start": 99.84,
      "duration": 4.8,
      "text": "here comes from traditionally this was"
    },
    {
      "start": 102.079,
      "duration": 6.561,
      "text": "all done with text, right? So embeddings"
    },
    {
      "start": 104.64,
      "duration": 6,
      "text": "were created mostly for taking text and"
    },
    {
      "start": 108.64,
      "duration": 4.159,
      "text": "turning that into some kind of vector"
    },
    {
      "start": 110.64,
      "duration": 5.759,
      "text": "representation. And then we had other"
    },
    {
      "start": 112.799,
      "duration": 5.761,
      "text": "sorts of creative ways of turning images"
    },
    {
      "start": 116.399,
      "duration": 5.441,
      "text": "into these vector representations as"
    },
    {
      "start": 118.56,
      "duration": 5.839,
      "text": "well. But both the text and the image"
    },
    {
      "start": 121.84,
      "duration": 4.72,
      "text": "lived in totally different universes."
    },
    {
      "start": 124.399,
      "duration": 4.161,
      "text": "They didn't live in the same vector"
    },
    {
      "start": 126.56,
      "duration": 4.64,
      "text": "space. So the whole revolutionary thing"
    },
    {
      "start": 128.56,
      "duration": 5.28,
      "text": "with multimodal embeddings is that we're"
    },
    {
      "start": 131.2,
      "duration": 5.039,
      "text": "getting it so that whether we pass in"
    },
    {
      "start": 133.84,
      "duration": 5.6,
      "text": "text, whether we pass in an image,"
    },
    {
      "start": 136.239,
      "duration": 6.72,
      "text": "whether we pass in video, the model has"
    },
    {
      "start": 139.44,
      "duration": 5.519,
      "text": "the ability to generate out a vector"
    },
    {
      "start": 142.959,
      "duration": 5.28,
      "text": "representation which should be"
    },
    {
      "start": 144.959,
      "duration": 6.241,
      "text": "semantically similar whether that is"
    },
    {
      "start": 148.239,
      "duration": 5.28,
      "text": "text about a cat, an image of a cat, or"
    },
    {
      "start": 151.2,
      "duration": 4.96,
      "text": "a video of a cat. and ideally perhaps"
    },
    {
      "start": 153.519,
      "duration": 5.36,
      "text": "even in the future have it so multimodal"
    },
    {
      "start": 156.16,
      "duration": 4.88,
      "text": "models could also process audio and it"
    },
    {
      "start": 158.879,
      "duration": 4.881,
      "text": "would then be able to put those sounds"
    },
    {
      "start": 161.04,
      "duration": 5.68,
      "text": "in the same vector space as the images"
    },
    {
      "start": 163.76,
      "duration": 5.68,
      "text": "and as the text. So with a multimodal"
    },
    {
      "start": 166.72,
      "duration": 5.36,
      "text": "embedding model the key insight here is"
    },
    {
      "start": 169.44,
      "duration": 5.68,
      "text": "that we want it to be a picture of a"
    },
    {
      "start": 172.08,
      "duration": 6.159,
      "text": "cat. We want to have it where the text"
    },
    {
      "start": 175.12,
      "duration": 6.16,
      "text": "that says a picture of a cat is actually"
    },
    {
      "start": 178.239,
      "duration": 6.321,
      "text": "going to be in the same kind of semantic"
    },
    {
      "start": 181.28,
      "duration": 6.08,
      "text": "representation and vector space as"
    },
    {
      "start": 184.56,
      "duration": 4.399,
      "text": "actually a photo of a cat. So this"
    },
    {
      "start": 187.36,
      "duration": 3.519,
      "text": "allows us to do a whole bunch of"
    },
    {
      "start": 188.959,
      "duration": 5.041,
      "text": "different things that if you think about"
    },
    {
      "start": 190.879,
      "duration": 6.08,
      "text": "it, even things like PDF files are not"
    },
    {
      "start": 194,
      "duration": 5.76,
      "text": "just text, right? Most rag systems"
    },
    {
      "start": 196.959,
      "duration": 5.601,
      "text": "basically just extract out the text,"
    },
    {
      "start": 199.76,
      "duration": 5.36,
      "text": "throw away the images, and then use that"
    },
    {
      "start": 202.56,
      "duration": 4.88,
      "text": "text for answering queries or something."
    },
    {
      "start": 205.12,
      "duration": 4.16,
      "text": "But with multimodal embeddings, we can"
    },
    {
      "start": 207.44,
      "duration": 5.439,
      "text": "actually get embeddings not just of the"
    },
    {
      "start": 209.28,
      "duration": 5.76,
      "text": "text, but of charts, of diagrams, of"
    },
    {
      "start": 212.879,
      "duration": 4,
      "text": "screenshots, of a whole bunch of"
    },
    {
      "start": 215.04,
      "duration": 3.44,
      "text": "different things. And if we've got a"
    },
    {
      "start": 216.879,
      "duration": 3.761,
      "text": "good model for this, it's going to make"
    },
    {
      "start": 218.48,
      "duration": 4.64,
      "text": "it so that they're in the same vector"
    },
    {
      "start": 220.64,
      "duration": 4.959,
      "text": "space so that we can do different kinds"
    },
    {
      "start": 223.12,
      "duration": 4.24,
      "text": "of similarity matching. The idea here is"
    },
    {
      "start": 225.599,
      "duration": 4,
      "text": "not totally new. It's been around for"
    },
    {
      "start": 227.36,
      "duration": 4.48,
      "text": "quite a while. We've seen some really"
    },
    {
      "start": 229.599,
      "duration": 5.441,
      "text": "big sort of powerful models like this"
    },
    {
      "start": 231.84,
      "duration": 5.28,
      "text": "being the clip model from OpenAI which"
    },
    {
      "start": 235.04,
      "duration": 4.72,
      "text": "drove a lot of the sort of interesting"
    },
    {
      "start": 237.12,
      "duration": 4.72,
      "text": "innovations with the early sort of Darly"
    },
    {
      "start": 239.76,
      "duration": 4,
      "text": "models and stuff like that. Clip was"
    },
    {
      "start": 241.84,
      "duration": 5.039,
      "text": "also used for things like stable"
    },
    {
      "start": 243.76,
      "duration": 5.039,
      "text": "diffusion as being a way of training a"
    },
    {
      "start": 246.879,
      "duration": 4.961,
      "text": "model so that we can get the images"
    },
    {
      "start": 248.799,
      "duration": 4.8,
      "text": "generated out to match the text that"
    },
    {
      "start": 251.84,
      "duration": 3.599,
      "text": "we're actually putting in. We've also"
    },
    {
      "start": 253.599,
      "duration": 4.241,
      "text": "seen that concept taken forward even"
    },
    {
      "start": 255.439,
      "duration": 4.401,
      "text": "more with models like SIGLIP that came"
    },
    {
      "start": 257.84,
      "duration": 4,
      "text": "out of Google. And it turns out there"
    },
    {
      "start": 259.84,
      "duration": 5.12,
      "text": "are a lot of AI patterns that you can"
    },
    {
      "start": 261.84,
      "duration": 5.359,
      "text": "actually use these models for. But what"
    },
    {
      "start": 264.96,
      "duration": 4.72,
      "text": "we're looking at today is not where this"
    },
    {
      "start": 267.199,
      "duration": 5.201,
      "text": "is baked into some kind of other model"
    },
    {
      "start": 269.68,
      "duration": 4.4,
      "text": "for some kind of generative images or"
    },
    {
      "start": 272.4,
      "duration": 5.28,
      "text": "something like that. What we're looking"
    },
    {
      "start": 274.08,
      "duration": 6.72,
      "text": "at here is the whole idea of using it"
    },
    {
      "start": 277.68,
      "duration": 5.36,
      "text": "for doing things like multimodal rag. So"
    },
    {
      "start": 280.8,
      "duration": 4.24,
      "text": "along with the multimodal embedding"
    },
    {
      "start": 283.04,
      "duration": 5.439,
      "text": "model that Quinn has released, they've"
    },
    {
      "start": 285.04,
      "duration": 5.599,
      "text": "also released a multimodal ranker model."
    },
    {
      "start": 288.479,
      "duration": 4.401,
      "text": "So embedding models are really good at"
    },
    {
      "start": 290.639,
      "duration": 4.321,
      "text": "the recall of finding a bunch of things"
    },
    {
      "start": 292.88,
      "duration": 4.08,
      "text": "that are similar to what we're actually"
    },
    {
      "start": 294.96,
      "duration": 4.239,
      "text": "looking at. Now the way this actually"
    },
    {
      "start": 296.96,
      "duration": 4.32,
      "text": "works generally is that these end up"
    },
    {
      "start": 299.199,
      "duration": 4,
      "text": "getting trained on what we call a by"
    },
    {
      "start": 301.28,
      "duration": 3.76,
      "text": "encoder model. So basically, we're"
    },
    {
      "start": 303.199,
      "duration": 4.56,
      "text": "trying to train a model that can either"
    },
    {
      "start": 305.04,
      "duration": 5.04,
      "text": "take the query about what type of cat"
    },
    {
      "start": 307.759,
      "duration": 4.88,
      "text": "this is and then match it with the"
    },
    {
      "start": 310.08,
      "duration": 5.119,
      "text": "content that was actually encoded to"
    },
    {
      "start": 312.639,
      "duration": 5.12,
      "text": "show semantically and in a sort of"
    },
    {
      "start": 315.199,
      "duration": 4.481,
      "text": "vector form what are the relevant things"
    },
    {
      "start": 317.759,
      "duration": 4.961,
      "text": "that are actually in a piece of text or"
    },
    {
      "start": 319.68,
      "duration": 5.92,
      "text": "in an image. The second part though of"
    },
    {
      "start": 322.72,
      "duration": 5.68,
      "text": "this system is the reranker model and"
    },
    {
      "start": 325.6,
      "duration": 5.36,
      "text": "that's all about sort of fine grain"
    },
    {
      "start": 328.4,
      "duration": 4.799,
      "text": "scoring of what the embedding model"
    },
    {
      "start": 330.96,
      "duration": 5.28,
      "text": "gives us back. So where we may set the"
    },
    {
      "start": 333.199,
      "duration": 6.401,
      "text": "embedding model to give us top 20"
    },
    {
      "start": 336.24,
      "duration": 6.32,
      "text": "candidates back the reanker then will go"
    },
    {
      "start": 339.6,
      "duration": 6.159,
      "text": "through our query and these documents"
    },
    {
      "start": 342.56,
      "duration": 5.919,
      "text": "and then fine grain work out which ones"
    },
    {
      "start": 345.759,
      "duration": 5.121,
      "text": "are the best match. So you can think of"
    },
    {
      "start": 348.479,
      "duration": 5.28,
      "text": "the embedding model as like a recall"
    },
    {
      "start": 350.88,
      "duration": 5.28,
      "text": "thing and the reranker models as like a"
    },
    {
      "start": 353.759,
      "duration": 4.561,
      "text": "precision thing. So why do we actually"
    },
    {
      "start": 356.16,
      "duration": 4,
      "text": "use both of them? The key thing to focus"
    },
    {
      "start": 358.32,
      "duration": 4.719,
      "text": "here and that the Quen team point out"
    },
    {
      "start": 360.16,
      "duration": 5.12,
      "text": "that the embedding model alone is fast,"
    },
    {
      "start": 363.039,
      "duration": 4.88,
      "text": "but you're only going to get about 85%"
    },
    {
      "start": 365.28,
      "duration": 5.039,
      "text": "precision. Running the re-ranker on the"
    },
    {
      "start": 367.919,
      "duration": 4.241,
      "text": "whole corpus is just way too slow. That"
    },
    {
      "start": 370.319,
      "duration": 3.44,
      "text": "alone is not going to work. But"
    },
    {
      "start": 372.16,
      "duration": 3.759,
      "text": "combining these where you get the"
    },
    {
      "start": 373.759,
      "duration": 4.241,
      "text": "embedding model to get you a bunch of"
    },
    {
      "start": 375.919,
      "duration": 5.041,
      "text": "relevant candidates and then the"
    },
    {
      "start": 378,
      "duration": 6.16,
      "text": "re-ranker model to actually select which"
    },
    {
      "start": 380.96,
      "duration": 5.92,
      "text": "of those candidates is your top one or"
    },
    {
      "start": 384.16,
      "duration": 5.36,
      "text": "your top three allows you to get a much"
    },
    {
      "start": 386.88,
      "duration": 5.36,
      "text": "better result overall. So if we dive"
    },
    {
      "start": 389.52,
      "duration": 4.56,
      "text": "into the Quen 3 VL embedding and"
    },
    {
      "start": 392.24,
      "duration": 4.399,
      "text": "reranker, we can see a number of"
    },
    {
      "start": 394.08,
      "duration": 5.52,
      "text": "interesting things here. So first off,"
    },
    {
      "start": 396.639,
      "duration": 5.201,
      "text": "these are actually built on the Quen 3"
    },
    {
      "start": 399.6,
      "duration": 4.159,
      "text": "VL foundation models. So that's the"
    },
    {
      "start": 401.84,
      "duration": 4.88,
      "text": "vision language models. Now currently"
    },
    {
      "start": 403.759,
      "duration": 5.681,
      "text": "they've got two sizes here. The 2B model"
    },
    {
      "start": 406.72,
      "duration": 4.8,
      "text": "and the 8B model. Both of them are an"
    },
    {
      "start": 409.44,
      "duration": 4.24,
      "text": "Apache 2 license and available on"
    },
    {
      "start": 411.52,
      "duration": 3.92,
      "text": "hugging face. And both of these have a"
    },
    {
      "start": 413.68,
      "duration": 3.6,
      "text": "number of really good properties here."
    },
    {
      "start": 415.44,
      "duration": 3.84,
      "text": "So obviously they can process standard"
    },
    {
      "start": 417.28,
      "duration": 4,
      "text": "text. So whether that's queries,"
    },
    {
      "start": 419.28,
      "duration": 4.96,
      "text": "documents, that kind of thing. They can"
    },
    {
      "start": 421.28,
      "duration": 4.96,
      "text": "process images and photos, diagrams,"
    },
    {
      "start": 424.24,
      "duration": 4.079,
      "text": "charts. The models have also been"
    },
    {
      "start": 426.24,
      "duration": 3.92,
      "text": "trained to be able to do things like"
    },
    {
      "start": 428.319,
      "duration": 4.16,
      "text": "screenshots. So if you've got UI"
    },
    {
      "start": 430.16,
      "duration": 4.4,
      "text": "captures, you've got document images of"
    },
    {
      "start": 432.479,
      "duration": 4.321,
      "text": "things, the models can actually do that"
    },
    {
      "start": 434.56,
      "duration": 4.479,
      "text": "as well. And then just as they can take"
    },
    {
      "start": 436.8,
      "duration": 4.799,
      "text": "sort of single images, they can actually"
    },
    {
      "start": 439.039,
      "duration": 4.961,
      "text": "put images together to do sort of video"
    },
    {
      "start": 441.599,
      "duration": 5.121,
      "text": "clips or presentations and things like"
    },
    {
      "start": 444,
      "duration": 4.639,
      "text": "that. And then finally, on top of that,"
    },
    {
      "start": 446.72,
      "duration": 3.84,
      "text": "you can actually mix these together. So"
    },
    {
      "start": 448.639,
      "duration": 3.68,
      "text": "you can have like a little bit of text"
    },
    {
      "start": 450.56,
      "duration": 3.52,
      "text": "followed by an image followed by"
    },
    {
      "start": 452.319,
      "duration": 4.481,
      "text": "something else. So, you could imagine"
    },
    {
      "start": 454.08,
      "duration": 4.559,
      "text": "that becomes really useful where you've"
    },
    {
      "start": 456.8,
      "duration": 4.72,
      "text": "got an image of something and you want"
    },
    {
      "start": 458.639,
      "duration": 4.721,
      "text": "to ask a question about that image first"
    },
    {
      "start": 461.52,
      "duration": 4.239,
      "text": "up and maybe you're trying to find"
    },
    {
      "start": 463.36,
      "duration": 5.04,
      "text": "either the closest text or the closest"
    },
    {
      "start": 465.759,
      "duration": 4.241,
      "text": "image in your particular database. So,"
    },
    {
      "start": 468.4,
      "duration": 4.639,
      "text": "some other interesting stats about these"
    },
    {
      "start": 470,
      "duration": 5.759,
      "text": "two, they support 30 plus languages. I"
    },
    {
      "start": 473.039,
      "duration": 4.801,
      "text": "love the fact that Quen has really"
    },
    {
      "start": 475.759,
      "duration": 4.321,
      "text": "decided to get behind the whole"
    },
    {
      "start": 477.84,
      "duration": 4.32,
      "text": "multilingual thing and not just support"
    },
    {
      "start": 480.08,
      "duration": 4.16,
      "text": "things like English and Chinese, which"
    },
    {
      "start": 482.16,
      "duration": 5.36,
      "text": "we see a lot of people actually doing."
    },
    {
      "start": 484.24,
      "duration": 4.959,
      "text": "The models themselves have a 32K context"
    },
    {
      "start": 487.52,
      "duration": 3.44,
      "text": "window, so you can actually put in"
    },
    {
      "start": 489.199,
      "duration": 3.84,
      "text": "pretty large documents and stuff like"
    },
    {
      "start": 490.96,
      "duration": 3.519,
      "text": "that in here. And then finally, the"
    },
    {
      "start": 493.039,
      "duration": 4.081,
      "text": "other thing that is really interesting"
    },
    {
      "start": 494.479,
      "duration": 4.801,
      "text": "is that they actually have matrioska"
    },
    {
      "start": 497.12,
      "duration": 4,
      "text": "representation learning here. Now, what"
    },
    {
      "start": 499.28,
      "duration": 3.359,
      "text": "does that actually mean? If we come in"
    },
    {
      "start": 501.12,
      "duration": 3.919,
      "text": "here and look at the embedding"
    },
    {
      "start": 502.639,
      "duration": 6.161,
      "text": "dimensions that they support out, so the"
    },
    {
      "start": 505.039,
      "duration": 6.801,
      "text": "big 8B model is supporting 4,096 size"
    },
    {
      "start": 508.8,
      "duration": 5.52,
      "text": "embeddings. The smaller 2B is half that."
    },
    {
      "start": 511.84,
      "duration": 5.6,
      "text": "But what the Metrioska embeddings mean"
    },
    {
      "start": 514.32,
      "duration": 6.079,
      "text": "is that we don't have to take the full"
    },
    {
      "start": 517.44,
      "duration": 5.2,
      "text": "dimension to actually do our searches."
    },
    {
      "start": 520.399,
      "duration": 3.921,
      "text": "If we wanted to search on something, so"
    },
    {
      "start": 522.64,
      "duration": 3.84,
      "text": "it's going to be much quicker. we can"
    },
    {
      "start": 524.32,
      "duration": 5.519,
      "text": "take that vector and just take the first"
    },
    {
      "start": 526.48,
      "duration": 5.84,
      "text": "10 24 numbers in that embedding and use"
    },
    {
      "start": 529.839,
      "duration": 5.44,
      "text": "that for a search. So this basically"
    },
    {
      "start": 532.32,
      "duration": 5.6,
      "text": "allows us to speed up the whole process."
    },
    {
      "start": 535.279,
      "duration": 4.641,
      "text": "Now if we come in and look at the MMEB"
    },
    {
      "start": 537.92,
      "duration": 4.8,
      "text": "leaderboards. So this is the massive"
    },
    {
      "start": 539.92,
      "duration": 4.8,
      "text": "multimodal embedding benchmark here. We"
    },
    {
      "start": 542.72,
      "duration": 4.799,
      "text": "can see coming down here that the Quen 3"
    },
    {
      "start": 544.72,
      "duration": 5.04,
      "text": "8B model is actually number one on the"
    },
    {
      "start": 547.519,
      "duration": 4.401,
      "text": "embedding leaderboard here. And we can"
    },
    {
      "start": 549.76,
      "duration": 4.96,
      "text": "see that the smaller one isn't that far"
    },
    {
      "start": 551.92,
      "duration": 4.64,
      "text": "behind. It's number five. And if we look"
    },
    {
      "start": 554.72,
      "duration": 3.76,
      "text": "at the scores for it, yes, they're going"
    },
    {
      "start": 556.56,
      "duration": 4.16,
      "text": "to be below what a model four times"
    },
    {
      "start": 558.48,
      "duration": 4.08,
      "text": "bigger is going to get, but we can see"
    },
    {
      "start": 560.72,
      "duration": 4.64,
      "text": "it's doing quite nicely here. And it's"
    },
    {
      "start": 562.56,
      "duration": 4.64,
      "text": "actually beating out a lot of sort of 7B"
    },
    {
      "start": 565.36,
      "duration": 3.52,
      "text": "models that have been released for this."
    },
    {
      "start": 567.2,
      "duration": 3.28,
      "text": "So, what are the real world use cases"
    },
    {
      "start": 568.88,
      "duration": 3.76,
      "text": "that you can actually use something like"
    },
    {
      "start": 570.48,
      "duration": 3.76,
      "text": "this? Well, first you could do a whole"
    },
    {
      "start": 572.64,
      "duration": 4,
      "text": "bunch of things around things like"
    },
    {
      "start": 574.24,
      "duration": 4.32,
      "text": "visual document search where if you've"
    },
    {
      "start": 576.64,
      "duration": 5.199,
      "text": "got documents that sort of traditional"
    },
    {
      "start": 578.56,
      "duration": 5.44,
      "text": "OCR just misses things out, doesn't do a"
    },
    {
      "start": 581.839,
      "duration": 5.44,
      "text": "great job, you can actually embed them"
    },
    {
      "start": 584,
      "duration": 4.959,
      "text": "as images and then do searches that way."
    },
    {
      "start": 587.279,
      "duration": 3.921,
      "text": "Another common use of this kind of"
    },
    {
      "start": 588.959,
      "duration": 4.32,
      "text": "system and these models is the whole"
    },
    {
      "start": 591.2,
      "duration": 3.84,
      "text": "sort of thing around e-commerce product"
    },
    {
      "start": 593.279,
      "duration": 4.161,
      "text": "search. So, you've probably seen on a"
    },
    {
      "start": 595.04,
      "duration": 4.08,
      "text": "lot of the top e-commerce sites that if"
    },
    {
      "start": 597.44,
      "duration": 3.76,
      "text": "you've got a picture of something, you"
    },
    {
      "start": 599.12,
      "duration": 3.839,
      "text": "can just drop that picture in and do a"
    },
    {
      "start": 601.2,
      "duration": 3.92,
      "text": "search to find all the different people"
    },
    {
      "start": 602.959,
      "duration": 3.841,
      "text": "that are actually selling it. So, in"
    },
    {
      "start": 605.12,
      "duration": 3.92,
      "text": "those cases, they're probably using"
    },
    {
      "start": 606.8,
      "duration": 4.719,
      "text": "image embeddings. But now, you can"
    },
    {
      "start": 609.04,
      "duration": 4.64,
      "text": "actually combine it and say, \"Okay, I"
    },
    {
      "start": 611.519,
      "duration": 4.241,
      "text": "want something like this product, but"
    },
    {
      "start": 613.68,
      "duration": 4,
      "text": "actually I want a green one instead of a"
    },
    {
      "start": 615.76,
      "duration": 3.36,
      "text": "red one.\" Another common use case that"
    },
    {
      "start": 617.68,
      "duration": 3.52,
      "text": "people are starting to use this kind of"
    },
    {
      "start": 619.12,
      "duration": 5.04,
      "text": "thing for is the whole idea of being"
    },
    {
      "start": 621.2,
      "duration": 4.96,
      "text": "able to find things in videos. So if"
    },
    {
      "start": 624.16,
      "duration": 4.16,
      "text": "you've got one particular reference"
    },
    {
      "start": 626.16,
      "duration": 3.84,
      "text": "frame or you want to be able to query"
    },
    {
      "start": 628.32,
      "duration": 3.6,
      "text": "something that happened on a"
    },
    {
      "start": 630,
      "duration": 4.56,
      "text": "surveillance video that is a few hours"
    },
    {
      "start": 631.92,
      "duration": 4.64,
      "text": "long, having those frames embedded"
    },
    {
      "start": 634.56,
      "duration": 3.68,
      "text": "allows you then to be able to do things"
    },
    {
      "start": 636.56,
      "duration": 3.519,
      "text": "like say, \"Show me the parts of the"
    },
    {
      "start": 638.24,
      "duration": 4.48,
      "text": "video where there were two people at the"
    },
    {
      "start": 640.079,
      "duration": 4.081,
      "text": "ATM instead of one person.\" So let's"
    },
    {
      "start": 642.72,
      "duration": 3.84,
      "text": "just jump into the code, have a play"
    },
    {
      "start": 644.16,
      "duration": 4.32,
      "text": "with these two models and actually see"
    },
    {
      "start": 646.56,
      "duration": 5.44,
      "text": "some of the examples of what they can"
    },
    {
      "start": 648.48,
      "duration": 5.52,
      "text": "do. Okay, so jumping into the collab,"
    },
    {
      "start": 652,
      "duration": 4.48,
      "text": "I'm going to basically just load up the"
    },
    {
      "start": 654,
      "duration": 5.92,
      "text": "2B embedding model in here. You can see"
    },
    {
      "start": 656.48,
      "duration": 4.88,
      "text": "that I'm just using a Tesla T4."
    },
    {
      "start": 659.92,
      "duration": 4.08,
      "text": "Actually, I'd probably prefer to be"
    },
    {
      "start": 661.36,
      "duration": 4.8,
      "text": "using an L4 so that I could run flash"
    },
    {
      "start": 664,
      "duration": 4.24,
      "text": "attention and some other things, but"
    },
    {
      "start": 666.16,
      "duration": 4.4,
      "text": "just to show you that you can do this on"
    },
    {
      "start": 668.24,
      "duration": 5.36,
      "text": "the free platform. So, I had to juggle"
    },
    {
      "start": 670.56,
      "duration": 5.6,
      "text": "around what to install here. I've got"
    },
    {
      "start": 673.6,
      "duration": 4.16,
      "text": "this basically set up according to how"
    },
    {
      "start": 676.16,
      "duration": 3.359,
      "text": "they wanted. I've just got a newer"
    },
    {
      "start": 677.76,
      "duration": 3.68,
      "text": "version of PyTorch, but it seems to be"
    },
    {
      "start": 679.519,
      "duration": 4,
      "text": "working fine. I got a bunch of code in"
    },
    {
      "start": 681.44,
      "duration": 4.48,
      "text": "there being able to do things like"
    },
    {
      "start": 683.519,
      "duration": 4.241,
      "text": "loading up the model, setting it up to"
    },
    {
      "start": 685.92,
      "duration": 3.84,
      "text": "be able to encode and generate"
    },
    {
      "start": 687.76,
      "duration": 3.759,
      "text": "embeddings, and then also setting it up"
    },
    {
      "start": 689.76,
      "duration": 3.44,
      "text": "to be able to do similarity checks"
    },
    {
      "start": 691.519,
      "duration": 3.44,
      "text": "between embeddings and stuff like that."
    },
    {
      "start": 693.2,
      "duration": 3.92,
      "text": "So you can see here we basically just"
    },
    {
      "start": 694.959,
      "duration": 4.401,
      "text": "load up the model and we can see that"
    },
    {
      "start": 697.12,
      "duration": 3.839,
      "text": "sure enough the embedding shape that's"
    },
    {
      "start": 699.36,
      "duration": 4.4,
      "text": "going to give back are going to be"
    },
    {
      "start": 700.959,
      "duration": 4.88,
      "text": "vectors of48"
    },
    {
      "start": 703.76,
      "duration": 4,
      "text": "here. So we can take some text, we can"
    },
    {
      "start": 705.839,
      "duration": 4.721,
      "text": "take some documents and sure enough our"
    },
    {
      "start": 707.76,
      "duration": 4.639,
      "text": "embedding process is working and we can"
    },
    {
      "start": 710.56,
      "duration": 3.92,
      "text": "just start to compare those and we can"
    },
    {
      "start": 712.399,
      "duration": 3.761,
      "text": "see okay we don't have a great sort of"
    },
    {
      "start": 714.48,
      "duration": 3.84,
      "text": "comparison there and this is actually"
    },
    {
      "start": 716.16,
      "duration": 3.919,
      "text": "not that useful at this point. So let's"
    },
    {
      "start": 718.32,
      "duration": 3.6,
      "text": "look at the first demo. So we can"
    },
    {
      "start": 720.079,
      "duration": 4.401,
      "text": "basically just load up a number of"
    },
    {
      "start": 721.92,
      "duration": 4.4,
      "text": "images. You can see the images here."
    },
    {
      "start": 724.48,
      "duration": 5.039,
      "text": "We've got one image we're calling beach"
    },
    {
      "start": 726.32,
      "duration": 7.199,
      "text": "dog, cat, laptop, mountain, city night,"
    },
    {
      "start": 729.519,
      "duration": 6.88,
      "text": "and food pizza. And for the embeddings,"
    },
    {
      "start": 733.519,
      "duration": 5.12,
      "text": "we're not using the names of the images."
    },
    {
      "start": 736.399,
      "duration": 4.481,
      "text": "We're actually going to embed the actual"
    },
    {
      "start": 738.639,
      "duration": 4.801,
      "text": "images themselves. And you can see here"
    },
    {
      "start": 740.88,
      "duration": 4.32,
      "text": "how we can actually go through and take"
    },
    {
      "start": 743.44,
      "duration": 3.92,
      "text": "those images that we've loaded up and"
    },
    {
      "start": 745.2,
      "duration": 4.319,
      "text": "actually generate embeddings out for"
    },
    {
      "start": 747.36,
      "duration": 4.88,
      "text": "those. We can then take some sort of"
    },
    {
      "start": 749.519,
      "duration": 4.801,
      "text": "text queries for this and we can"
    },
    {
      "start": 752.24,
      "duration": 4.56,
      "text": "actually start to do some comparisons."
    },
    {
      "start": 754.32,
      "duration": 4.639,
      "text": "If the query is a woman playing with her"
    },
    {
      "start": 756.8,
      "duration": 4.8,
      "text": "dog on a beach at sunset, we're going to"
    },
    {
      "start": 758.959,
      "duration": 4.401,
      "text": "get the beach dog image. Just going"
    },
    {
      "start": 761.6,
      "duration": 3.44,
      "text": "back, seeing that one there as our"
    },
    {
      "start": 763.36,
      "duration": 4.8,
      "text": "highest returned value. And you can see"
    },
    {
      "start": 765.04,
      "duration": 5.359,
      "text": "that's actually a lot higher than cat"
    },
    {
      "start": 768.16,
      "duration": 4.32,
      "text": "laptop, food, pizza, etc. Now, you"
    },
    {
      "start": 770.399,
      "duration": 3.601,
      "text": "shouldn't think of these as being"
    },
    {
      "start": 772.48,
      "duration": 3.68,
      "text": "percentages. They're not really"
    },
    {
      "start": 774,
      "duration": 4.079,
      "text": "percentages of how similar they are or"
    },
    {
      "start": 776.16,
      "duration": 3.679,
      "text": "anything like that. It's just a score."
    },
    {
      "start": 778.079,
      "duration": 3.76,
      "text": "Next up, we can see the a fluffy cat"
    },
    {
      "start": 779.839,
      "duration": 4.081,
      "text": "relaxing. And we can see sure enough,"
    },
    {
      "start": 781.839,
      "duration": 4.8,
      "text": "okay, that's going to come up with cat"
    },
    {
      "start": 783.92,
      "duration": 4.56,
      "text": "laptop again. Now, beach dog's gone down"
    },
    {
      "start": 786.639,
      "duration": 4.561,
      "text": "much lower in here. So, what we're doing"
    },
    {
      "start": 788.48,
      "duration": 5.599,
      "text": "is we're comparing the semantic"
    },
    {
      "start": 791.2,
      "duration": 5.28,
      "text": "similarity between this text and the"
    },
    {
      "start": 794.079,
      "duration": 4,
      "text": "actual image in here. Snowy mountain"
    },
    {
      "start": 796.48,
      "duration": 3.919,
      "text": "peaks, we're going to get the mountain"
    },
    {
      "start": 798.079,
      "duration": 4.081,
      "text": "image. City skyline, we get city night."
    },
    {
      "start": 800.399,
      "duration": 4.321,
      "text": "Delicious Italian food, we're going to"
    },
    {
      "start": 802.16,
      "duration": 4.64,
      "text": "get the pizza image in there. And if we"
    },
    {
      "start": 804.72,
      "duration": 4,
      "text": "plot this out, we can see that sure"
    },
    {
      "start": 806.8,
      "duration": 4.56,
      "text": "enough, you're going to get a nice"
    },
    {
      "start": 808.72,
      "duration": 4.88,
      "text": "alignment here of where these are"
    },
    {
      "start": 811.36,
      "duration": 5.44,
      "text": "matching up. That doesn't mean though"
    },
    {
      "start": 813.6,
      "duration": 4.88,
      "text": "that you're going to have these be"
    },
    {
      "start": 816.8,
      "duration": 3.52,
      "text": "really high all the time. We can see"
    },
    {
      "start": 818.48,
      "duration": 3.919,
      "text": "when we look at the city skyline at"
    },
    {
      "start": 820.32,
      "duration": 4.8,
      "text": "night, we don't really have a hugely"
    },
    {
      "start": 822.399,
      "duration": 4.88,
      "text": "high value here, but it is higher than"
    },
    {
      "start": 825.12,
      "duration": 4.32,
      "text": "the other images. This is why it returns"
    },
    {
      "start": 827.279,
      "duration": 4.641,
      "text": "this one the highest. Just jumping"
    },
    {
      "start": 829.44,
      "duration": 5.04,
      "text": "forward a bit, we can now make a mini"
    },
    {
      "start": 831.92,
      "duration": 4.64,
      "text": "sort of rag model or mini sort of rag"
    },
    {
      "start": 834.48,
      "duration": 5.359,
      "text": "system in here or retrieval system."
    },
    {
      "start": 836.56,
      "duration": 4.8,
      "text": "We're not actually using the LLM part to"
    },
    {
      "start": 839.839,
      "duration": 4.481,
      "text": "reform the answers or anything like"
    },
    {
      "start": 841.36,
      "duration": 5.52,
      "text": "that. So this class basically will just"
    },
    {
      "start": 844.32,
      "duration": 4.72,
      "text": "allow us to load things up, be able to"
    },
    {
      "start": 846.88,
      "duration": 5.6,
      "text": "do searches, be able to return things"
    },
    {
      "start": 849.04,
      "duration": 5.919,
      "text": "back in here. So using our little rag"
    },
    {
      "start": 852.48,
      "duration": 5.2,
      "text": "thing, we can add in some text. We can"
    },
    {
      "start": 854.959,
      "duration": 5.68,
      "text": "add some metadata. We can add in things"
    },
    {
      "start": 857.68,
      "duration": 5.04,
      "text": "like the golden retriever, pizza, Mount"
    },
    {
      "start": 860.639,
      "duration": 4.961,
      "text": "Everest, and then at the same time, we"
    },
    {
      "start": 862.72,
      "duration": 4.799,
      "text": "can also add in images, right? And we"
    },
    {
      "start": 865.6,
      "duration": 3.359,
      "text": "can put these captions in there. Once"
    },
    {
      "start": 867.519,
      "duration": 3.76,
      "text": "we've added each of those, we can"
    },
    {
      "start": 868.959,
      "duration": 4.081,
      "text": "actually just set up our sort of index,"
    },
    {
      "start": 871.279,
      "duration": 4.321,
      "text": "and we can say something like, okay,"
    },
    {
      "start": 873.04,
      "duration": 5.039,
      "text": "tell me about dogs and pets. And we're"
    },
    {
      "start": 875.6,
      "duration": 5.12,
      "text": "going to return the top three responses"
    },
    {
      "start": 878.079,
      "duration": 5.76,
      "text": "here. In a real system, we might decide"
    },
    {
      "start": 880.72,
      "duration": 5.44,
      "text": "that any responses below a certain"
    },
    {
      "start": 883.839,
      "duration": 3.68,
      "text": "score, we're never going to return back"
    },
    {
      "start": 886.16,
      "duration": 3.2,
      "text": "or something like that. But in this"
    },
    {
      "start": 887.519,
      "duration": 3.44,
      "text": "case, we're just taking the top three."
    },
    {
      "start": 889.36,
      "duration": 3.919,
      "text": "You can see here, tell me about dogs and"
    },
    {
      "start": 890.959,
      "duration": 4.401,
      "text": "pets. Okay, we've got the text response"
    },
    {
      "start": 893.279,
      "duration": 4.881,
      "text": "coming back. And then we've got text and"
    },
    {
      "start": 895.36,
      "duration": 4.32,
      "text": "image for our image coming back there."
    },
    {
      "start": 898.16,
      "duration": 3.28,
      "text": "And then we've got the third one. You"
    },
    {
      "start": 899.68,
      "duration": 3.44,
      "text": "can see that the score is really low. It"
    },
    {
      "start": 901.44,
      "duration": 3.68,
      "text": "doesn't really relate to what we've"
    },
    {
      "start": 903.12,
      "duration": 4.399,
      "text": "asked for here. Next up, we've got"
    },
    {
      "start": 905.12,
      "duration": 4.399,
      "text": "Italian cuisine and traditional recipes."
    },
    {
      "start": 907.519,
      "duration": 4.481,
      "text": "We've got the text one coming up and"
    },
    {
      "start": 909.519,
      "duration": 5.12,
      "text": "then the actual sort of pizza image"
    },
    {
      "start": 912,
      "duration": 4.16,
      "text": "itself coming up here. Lastly, we've got"
    },
    {
      "start": 914.639,
      "duration": 3.681,
      "text": "one tall mountains and hiking"
    },
    {
      "start": 916.16,
      "duration": 4.32,
      "text": "destinations. So, sure enough, Mount"
    },
    {
      "start": 918.32,
      "duration": 4.639,
      "text": "Everest comes back as text, but then"
    },
    {
      "start": 920.48,
      "duration": 4.479,
      "text": "we've got this image of mountains and"
    },
    {
      "start": 922.959,
      "duration": 4.081,
      "text": "we've got the mountains at night coming"
    },
    {
      "start": 924.959,
      "duration": 3.361,
      "text": "in here as well. So, I've put some code"
    },
    {
      "start": 927.04,
      "duration": 3.76,
      "text": "in there if you want to play with it"
    },
    {
      "start": 928.32,
      "duration": 4,
      "text": "yourself and actually upload some. Try"
    },
    {
      "start": 930.8,
      "duration": 3.839,
      "text": "it out. The next one I want to look at"
    },
    {
      "start": 932.32,
      "duration": 4.8,
      "text": "though is just image to image sort of"
    },
    {
      "start": 934.639,
      "duration": 5.44,
      "text": "search. So here you can see I'm"
    },
    {
      "start": 937.12,
      "duration": 4.88,
      "text": "uploading a picture of a dog and really"
    },
    {
      "start": 940.079,
      "duration": 4.961,
      "text": "we're actually I guess we're getting"
    },
    {
      "start": 942,
      "duration": 6.079,
      "text": "back both images and text but we're"
    },
    {
      "start": 945.04,
      "duration": 5.52,
      "text": "searching the query is this image. So"
    },
    {
      "start": 948.079,
      "duration": 4.721,
      "text": "there's no sort of text saying what is"
    },
    {
      "start": 950.56,
      "duration": 4.16,
      "text": "this dog or anything like that. So we"
    },
    {
      "start": 952.8,
      "duration": 4.08,
      "text": "put in this dog. It is a picture of a"
    },
    {
      "start": 954.72,
      "duration": 3.76,
      "text": "golden retriever. Sure enough the first"
    },
    {
      "start": 956.88,
      "duration": 3.6,
      "text": "thing that comes back is the text about"
    },
    {
      "start": 958.48,
      "duration": 4.479,
      "text": "the golden retriever. And the second"
    },
    {
      "start": 960.48,
      "duration": 4.56,
      "text": "thing is that we're getting the image"
    },
    {
      "start": 962.959,
      "duration": 4.481,
      "text": "coming back with this dog that I think"
    },
    {
      "start": 965.04,
      "duration": 4.56,
      "text": "could be a golden as well. Finally, just"
    },
    {
      "start": 967.44,
      "duration": 4.8,
      "text": "finishing up just showing you that you"
    },
    {
      "start": 969.6,
      "duration": 5.359,
      "text": "can use the Matrioska representation"
    },
    {
      "start": 972.24,
      "duration": 5.279,
      "text": "learning in here as well. Right? So here"
    },
    {
      "start": 974.959,
      "duration": 5.841,
      "text": "we can see we can reduce the size of the"
    },
    {
      "start": 977.519,
      "duration": 5.601,
      "text": "embeddings. So rather than 24,48, we can"
    },
    {
      "start": 980.8,
      "duration": 6.159,
      "text": "have them be 1024. We can go right down"
    },
    {
      "start": 983.12,
      "duration": 6.639,
      "text": "to just the first 64 numbers in here."
    },
    {
      "start": 986.959,
      "duration": 6.161,
      "text": "And we can actually test these and see,"
    },
    {
      "start": 989.759,
      "duration": 5.921,
      "text": "okay, if we are trying to compare the"
    },
    {
      "start": 993.12,
      "duration": 6,
      "text": "text, a dog playing on the beach with"
    },
    {
      "start": 995.68,
      "duration": 5.599,
      "text": "that beach dog, how does it actually go?"
    },
    {
      "start": 999.12,
      "duration": 6.32,
      "text": "And we can see that sure enough for the"
    },
    {
      "start": 1001.279,
      "duration": 6.321,
      "text": "64D, we're getting a score of 63. And"
    },
    {
      "start": 1005.44,
      "duration": 4.079,
      "text": "that's probably just going dog kind of"
    },
    {
      "start": 1007.6,
      "duration": 3.44,
      "text": "thing, right? That's why it may look"
    },
    {
      "start": 1009.519,
      "duration": 4.32,
      "text": "like it's better, but it's not actually"
    },
    {
      "start": 1011.04,
      "duration": 4.96,
      "text": "better. In that image also has things"
    },
    {
      "start": 1013.839,
      "duration": 3.761,
      "text": "about beaches. It has a human in there."
    },
    {
      "start": 1016,
      "duration": 3.759,
      "text": "has a lot of other things in there as"
    },
    {
      "start": 1017.6,
      "duration": 4.4,
      "text": "well. This is just going for the sort of"
    },
    {
      "start": 1019.759,
      "duration": 4.881,
      "text": "simplest thing there. But we can see"
    },
    {
      "start": 1022,
      "duration": 4.72,
      "text": "that all of them actually return back"
    },
    {
      "start": 1024.64,
      "duration": 5.36,
      "text": "pretty well. And you can see that the"
    },
    {
      "start": 1026.72,
      "duration": 5.359,
      "text": "biggest dimension is not that different"
    },
    {
      "start": 1030,
      "duration": 5.439,
      "text": "than the sort of half and quarter"
    },
    {
      "start": 1032.079,
      "duration": 4.72,
      "text": "dimensions in here. So going for 512D"
    },
    {
      "start": 1035.439,
      "duration": 4,
      "text": "and of course you should always test"
    },
    {
      "start": 1036.799,
      "duration": 4.561,
      "text": "this with your own system and your own"
    },
    {
      "start": 1039.439,
      "duration": 4.161,
      "text": "data and stuff like but it's much"
    },
    {
      "start": 1041.36,
      "duration": 6.319,
      "text": "quicker then for your sort of search"
    },
    {
      "start": 1043.6,
      "duration": 6,
      "text": "system to use smaller shorter embeddings"
    },
    {
      "start": 1047.679,
      "duration": 4.081,
      "text": "if they're going to be accurate enough"
    },
    {
      "start": 1049.6,
      "duration": 4.8,
      "text": "for what you want. And we can see in"
    },
    {
      "start": 1051.76,
      "duration": 4.799,
      "text": "this case that it does look like the"
    },
    {
      "start": 1054.4,
      "duration": 5.12,
      "text": "scores that we're seeing around the sort"
    },
    {
      "start": 1056.559,
      "duration": 5.841,
      "text": "of top three are very similar to each"
    },
    {
      "start": 1059.52,
      "duration": 5.92,
      "text": "other in here. So overall, these Quen 3"
    },
    {
      "start": 1062.4,
      "duration": 5.6,
      "text": "multimodal embeddings and the reranker"
    },
    {
      "start": 1065.44,
      "duration": 5.359,
      "text": "really make a nice system for building"
    },
    {
      "start": 1068,
      "duration": 4.88,
      "text": "out sort of things where you don't want"
    },
    {
      "start": 1070.799,
      "duration": 3.521,
      "text": "to just do OCR on a document or"
    },
    {
      "start": 1072.88,
      "duration": 4.56,
      "text": "something like that. You want to be able"
    },
    {
      "start": 1074.32,
      "duration": 5.28,
      "text": "to have an image representation as well."
    },
    {
      "start": 1077.44,
      "duration": 5.84,
      "text": "And you may actually index your content"
    },
    {
      "start": 1079.6,
      "duration": 6.16,
      "text": "both for images and for some kind of OCR"
    },
    {
      "start": 1083.28,
      "duration": 4.32,
      "text": "text version as well with this. But the"
    },
    {
      "start": 1085.76,
      "duration": 4.08,
      "text": "idea here is that this allows you to do"
    },
    {
      "start": 1087.6,
      "duration": 4.24,
      "text": "visual question answering for things"
    },
    {
      "start": 1089.84,
      "duration": 4.8,
      "text": "that we've seen and allows you to have a"
    },
    {
      "start": 1091.84,
      "duration": 5.44,
      "text": "high top K, bring things back and then"
    },
    {
      "start": 1094.64,
      "duration": 4.399,
      "text": "put them into the reanker to actually"
    },
    {
      "start": 1097.28,
      "duration": 3.92,
      "text": "filter down to the ones that are most"
    },
    {
      "start": 1099.039,
      "duration": 4.241,
      "text": "relevant to you. Anyway, let me know in"
    },
    {
      "start": 1101.2,
      "duration": 4.96,
      "text": "the comments if you've got any questions"
    },
    {
      "start": 1103.28,
      "duration": 4.72,
      "text": "about multimodal rag systems or about"
    },
    {
      "start": 1106.16,
      "duration": 3.44,
      "text": "building something like this. And"
    },
    {
      "start": 1108,
      "duration": 3.76,
      "text": "another thing I haven't even covered in"
    },
    {
      "start": 1109.6,
      "duration": 4.64,
      "text": "this video is that you can actually use"
    },
    {
      "start": 1111.76,
      "duration": 4.4,
      "text": "quantized versions of these models as"
    },
    {
      "start": 1114.24,
      "duration": 5.2,
      "text": "well. So you can already see that there"
    },
    {
      "start": 1116.16,
      "duration": 5.68,
      "text": "are llama CPP or the GGUF format"
    },
    {
      "start": 1119.44,
      "duration": 4.239,
      "text": "versions of these models up there."
    },
    {
      "start": 1121.84,
      "duration": 4.32,
      "text": "You'll probably lose a little bit from"
    },
    {
      "start": 1123.679,
      "duration": 4.961,
      "text": "the quantization, but overall that then"
    },
    {
      "start": 1126.16,
      "duration": 5.2,
      "text": "allows you to run these models locally"
    },
    {
      "start": 1128.64,
      "duration": 4.88,
      "text": "on your own computer, build your own rag"
    },
    {
      "start": 1131.36,
      "duration": 5.12,
      "text": "system with these two models and perhaps"
    },
    {
      "start": 1133.52,
      "duration": 6.08,
      "text": "another small sort of Quen 3 model for"
    },
    {
      "start": 1136.48,
      "duration": 6.079,
      "text": "the actual LLM part or any other sort of"
    },
    {
      "start": 1139.6,
      "duration": 4.079,
      "text": "LLM that you want to use. So anyway, if"
    },
    {
      "start": 1142.559,
      "duration": 2.401,
      "text": "you've got questions or if you've got"
    },
    {
      "start": 1143.679,
      "duration": 3.201,
      "text": "comments or something, please put them"
    },
    {
      "start": 1144.96,
      "duration": 3.599,
      "text": "in the comments below. I will certainly"
    },
    {
      "start": 1146.88,
      "duration": 3.919,
      "text": "check them for the first few days after"
    },
    {
      "start": 1148.559,
      "duration": 4.641,
      "text": "the videos come out. And if people are"
    },
    {
      "start": 1150.799,
      "duration": 5.281,
      "text": "interested in doing some more in-depth"
    },
    {
      "start": 1153.2,
      "duration": 4.88,
      "text": "building local multimodal rag stuff, let"
    },
    {
      "start": 1156.08,
      "duration": 3.44,
      "text": "me know as well. I can always look at"
    },
    {
      "start": 1158.08,
      "duration": 3.599,
      "text": "doing a video of how you would actually"
    },
    {
      "start": 1159.52,
      "duration": 4,
      "text": "put something like that together. All"
    },
    {
      "start": 1161.679,
      "duration": 3.521,
      "text": "right, as always, if you found the video"
    },
    {
      "start": 1163.52,
      "duration": 3.12,
      "text": "useful, please click like and subscribe,"
    },
    {
      "start": 1165.2,
      "duration": 4.88,
      "text": "and I will talk to you in the next"
    },
    {
      "start": 1166.64,
      "duration": 3.44,
      "text": "video. Bye for now."
    }
  ],
  "fullText": "Okay, so the first models from Quen for 2026 have dropped. And these are the Quen 3 VL embedding models. And the whole sort of cool thing about these is that these are multimodal embedding models, meaning that they can process both text, images, even videos. So in this video, I'm going to go through what multimodal embeddings are. We'll talk a little bit about them, how they work, do a little refresher on the embeddings themselves, what the reranker models do, and look at how they can be used with the embeddings. And then afterwards, I'll jump into some code showing you just sort of playing around with some of these models. And I'll show you how you can actually speed this up for much faster retrieval by making use of the Matrioska embeddings. Okay, so what are multimodal embeddings? Well, let's start off with just a quick refresher about what embeddings are. So, embeddings are basically a numerical representation of meaning. And that basically means we take the content that we're going to pass into the model and it will give us a vector out that numerically represents that content. Often what this allows us to do is do all sorts of similarity comparisons. So while in general terms it's very hard to get a model to actually compare perhaps two paragraphs of text and see if they're about the same thing, if we convert them to vectors, it makes it a lot easier for us to then basically use something like cosine distance or another distance metric to basically look at these two vectors and see how close together they actually are. Now the multimodal leap here comes from traditionally this was all done with text, right? So embeddings were created mostly for taking text and turning that into some kind of vector representation. And then we had other sorts of creative ways of turning images into these vector representations as well. But both the text and the image lived in totally different universes. They didn't live in the same vector space. So the whole revolutionary thing with multimodal embeddings is that we're getting it so that whether we pass in text, whether we pass in an image, whether we pass in video, the model has the ability to generate out a vector representation which should be semantically similar whether that is text about a cat, an image of a cat, or a video of a cat. and ideally perhaps even in the future have it so multimodal models could also process audio and it would then be able to put those sounds in the same vector space as the images and as the text. So with a multimodal embedding model the key insight here is that we want it to be a picture of a cat. We want to have it where the text that says a picture of a cat is actually going to be in the same kind of semantic representation and vector space as actually a photo of a cat. So this allows us to do a whole bunch of different things that if you think about it, even things like PDF files are not just text, right? Most rag systems basically just extract out the text, throw away the images, and then use that text for answering queries or something. But with multimodal embeddings, we can actually get embeddings not just of the text, but of charts, of diagrams, of screenshots, of a whole bunch of different things. And if we've got a good model for this, it's going to make it so that they're in the same vector space so that we can do different kinds of similarity matching. The idea here is not totally new. It's been around for quite a while. We've seen some really big sort of powerful models like this being the clip model from OpenAI which drove a lot of the sort of interesting innovations with the early sort of Darly models and stuff like that. Clip was also used for things like stable diffusion as being a way of training a model so that we can get the images generated out to match the text that we're actually putting in. We've also seen that concept taken forward even more with models like SIGLIP that came out of Google. And it turns out there are a lot of AI patterns that you can actually use these models for. But what we're looking at today is not where this is baked into some kind of other model for some kind of generative images or something like that. What we're looking at here is the whole idea of using it for doing things like multimodal rag. So along with the multimodal embedding model that Quinn has released, they've also released a multimodal ranker model. So embedding models are really good at the recall of finding a bunch of things that are similar to what we're actually looking at. Now the way this actually works generally is that these end up getting trained on what we call a by encoder model. So basically, we're trying to train a model that can either take the query about what type of cat this is and then match it with the content that was actually encoded to show semantically and in a sort of vector form what are the relevant things that are actually in a piece of text or in an image. The second part though of this system is the reranker model and that's all about sort of fine grain scoring of what the embedding model gives us back. So where we may set the embedding model to give us top 20 candidates back the reanker then will go through our query and these documents and then fine grain work out which ones are the best match. So you can think of the embedding model as like a recall thing and the reranker models as like a precision thing. So why do we actually use both of them? The key thing to focus here and that the Quen team point out that the embedding model alone is fast, but you're only going to get about 85% precision. Running the re-ranker on the whole corpus is just way too slow. That alone is not going to work. But combining these where you get the embedding model to get you a bunch of relevant candidates and then the re-ranker model to actually select which of those candidates is your top one or your top three allows you to get a much better result overall. So if we dive into the Quen 3 VL embedding and reranker, we can see a number of interesting things here. So first off, these are actually built on the Quen 3 VL foundation models. So that's the vision language models. Now currently they've got two sizes here. The 2B model and the 8B model. Both of them are an Apache 2 license and available on hugging face. And both of these have a number of really good properties here. So obviously they can process standard text. So whether that's queries, documents, that kind of thing. They can process images and photos, diagrams, charts. The models have also been trained to be able to do things like screenshots. So if you've got UI captures, you've got document images of things, the models can actually do that as well. And then just as they can take sort of single images, they can actually put images together to do sort of video clips or presentations and things like that. And then finally, on top of that, you can actually mix these together. So you can have like a little bit of text followed by an image followed by something else. So, you could imagine that becomes really useful where you've got an image of something and you want to ask a question about that image first up and maybe you're trying to find either the closest text or the closest image in your particular database. So, some other interesting stats about these two, they support 30 plus languages. I love the fact that Quen has really decided to get behind the whole multilingual thing and not just support things like English and Chinese, which we see a lot of people actually doing. The models themselves have a 32K context window, so you can actually put in pretty large documents and stuff like that in here. And then finally, the other thing that is really interesting is that they actually have matrioska representation learning here. Now, what does that actually mean? If we come in here and look at the embedding dimensions that they support out, so the big 8B model is supporting 4,096 size embeddings. The smaller 2B is half that. But what the Metrioska embeddings mean is that we don't have to take the full dimension to actually do our searches. If we wanted to search on something, so it's going to be much quicker. we can take that vector and just take the first 10 24 numbers in that embedding and use that for a search. So this basically allows us to speed up the whole process. Now if we come in and look at the MMEB leaderboards. So this is the massive multimodal embedding benchmark here. We can see coming down here that the Quen 3 8B model is actually number one on the embedding leaderboard here. And we can see that the smaller one isn't that far behind. It's number five. And if we look at the scores for it, yes, they're going to be below what a model four times bigger is going to get, but we can see it's doing quite nicely here. And it's actually beating out a lot of sort of 7B models that have been released for this. So, what are the real world use cases that you can actually use something like this? Well, first you could do a whole bunch of things around things like visual document search where if you've got documents that sort of traditional OCR just misses things out, doesn't do a great job, you can actually embed them as images and then do searches that way. Another common use of this kind of system and these models is the whole sort of thing around e-commerce product search. So, you've probably seen on a lot of the top e-commerce sites that if you've got a picture of something, you can just drop that picture in and do a search to find all the different people that are actually selling it. So, in those cases, they're probably using image embeddings. But now, you can actually combine it and say, \"Okay, I want something like this product, but actually I want a green one instead of a red one.\" Another common use case that people are starting to use this kind of thing for is the whole idea of being able to find things in videos. So if you've got one particular reference frame or you want to be able to query something that happened on a surveillance video that is a few hours long, having those frames embedded allows you then to be able to do things like say, \"Show me the parts of the video where there were two people at the ATM instead of one person.\" So let's just jump into the code, have a play with these two models and actually see some of the examples of what they can do. Okay, so jumping into the collab, I'm going to basically just load up the 2B embedding model in here. You can see that I'm just using a Tesla T4. Actually, I'd probably prefer to be using an L4 so that I could run flash attention and some other things, but just to show you that you can do this on the free platform. So, I had to juggle around what to install here. I've got this basically set up according to how they wanted. I've just got a newer version of PyTorch, but it seems to be working fine. I got a bunch of code in there being able to do things like loading up the model, setting it up to be able to encode and generate embeddings, and then also setting it up to be able to do similarity checks between embeddings and stuff like that. So you can see here we basically just load up the model and we can see that sure enough the embedding shape that's going to give back are going to be vectors of48 here. So we can take some text, we can take some documents and sure enough our embedding process is working and we can just start to compare those and we can see okay we don't have a great sort of comparison there and this is actually not that useful at this point. So let's look at the first demo. So we can basically just load up a number of images. You can see the images here. We've got one image we're calling beach dog, cat, laptop, mountain, city night, and food pizza. And for the embeddings, we're not using the names of the images. We're actually going to embed the actual images themselves. And you can see here how we can actually go through and take those images that we've loaded up and actually generate embeddings out for those. We can then take some sort of text queries for this and we can actually start to do some comparisons. If the query is a woman playing with her dog on a beach at sunset, we're going to get the beach dog image. Just going back, seeing that one there as our highest returned value. And you can see that's actually a lot higher than cat laptop, food, pizza, etc. Now, you shouldn't think of these as being percentages. They're not really percentages of how similar they are or anything like that. It's just a score. Next up, we can see the a fluffy cat relaxing. And we can see sure enough, okay, that's going to come up with cat laptop again. Now, beach dog's gone down much lower in here. So, what we're doing is we're comparing the semantic similarity between this text and the actual image in here. Snowy mountain peaks, we're going to get the mountain image. City skyline, we get city night. Delicious Italian food, we're going to get the pizza image in there. And if we plot this out, we can see that sure enough, you're going to get a nice alignment here of where these are matching up. That doesn't mean though that you're going to have these be really high all the time. We can see when we look at the city skyline at night, we don't really have a hugely high value here, but it is higher than the other images. This is why it returns this one the highest. Just jumping forward a bit, we can now make a mini sort of rag model or mini sort of rag system in here or retrieval system. We're not actually using the LLM part to reform the answers or anything like that. So this class basically will just allow us to load things up, be able to do searches, be able to return things back in here. So using our little rag thing, we can add in some text. We can add some metadata. We can add in things like the golden retriever, pizza, Mount Everest, and then at the same time, we can also add in images, right? And we can put these captions in there. Once we've added each of those, we can actually just set up our sort of index, and we can say something like, okay, tell me about dogs and pets. And we're going to return the top three responses here. In a real system, we might decide that any responses below a certain score, we're never going to return back or something like that. But in this case, we're just taking the top three. You can see here, tell me about dogs and pets. Okay, we've got the text response coming back. And then we've got text and image for our image coming back there. And then we've got the third one. You can see that the score is really low. It doesn't really relate to what we've asked for here. Next up, we've got Italian cuisine and traditional recipes. We've got the text one coming up and then the actual sort of pizza image itself coming up here. Lastly, we've got one tall mountains and hiking destinations. So, sure enough, Mount Everest comes back as text, but then we've got this image of mountains and we've got the mountains at night coming in here as well. So, I've put some code in there if you want to play with it yourself and actually upload some. Try it out. The next one I want to look at though is just image to image sort of search. So here you can see I'm uploading a picture of a dog and really we're actually I guess we're getting back both images and text but we're searching the query is this image. So there's no sort of text saying what is this dog or anything like that. So we put in this dog. It is a picture of a golden retriever. Sure enough the first thing that comes back is the text about the golden retriever. And the second thing is that we're getting the image coming back with this dog that I think could be a golden as well. Finally, just finishing up just showing you that you can use the Matrioska representation learning in here as well. Right? So here we can see we can reduce the size of the embeddings. So rather than 24,48, we can have them be 1024. We can go right down to just the first 64 numbers in here. And we can actually test these and see, okay, if we are trying to compare the text, a dog playing on the beach with that beach dog, how does it actually go? And we can see that sure enough for the 64D, we're getting a score of 63. And that's probably just going dog kind of thing, right? That's why it may look like it's better, but it's not actually better. In that image also has things about beaches. It has a human in there. has a lot of other things in there as well. This is just going for the sort of simplest thing there. But we can see that all of them actually return back pretty well. And you can see that the biggest dimension is not that different than the sort of half and quarter dimensions in here. So going for 512D and of course you should always test this with your own system and your own data and stuff like but it's much quicker then for your sort of search system to use smaller shorter embeddings if they're going to be accurate enough for what you want. And we can see in this case that it does look like the scores that we're seeing around the sort of top three are very similar to each other in here. So overall, these Quen 3 multimodal embeddings and the reranker really make a nice system for building out sort of things where you don't want to just do OCR on a document or something like that. You want to be able to have an image representation as well. And you may actually index your content both for images and for some kind of OCR text version as well with this. But the idea here is that this allows you to do visual question answering for things that we've seen and allows you to have a high top K, bring things back and then put them into the reanker to actually filter down to the ones that are most relevant to you. Anyway, let me know in the comments if you've got any questions about multimodal rag systems or about building something like this. And another thing I haven't even covered in this video is that you can actually use quantized versions of these models as well. So you can already see that there are llama CPP or the GGUF format versions of these models up there. You'll probably lose a little bit from the quantization, but overall that then allows you to run these models locally on your own computer, build your own rag system with these two models and perhaps another small sort of Quen 3 model for the actual LLM part or any other sort of LLM that you want to use. So anyway, if you've got questions or if you've got comments or something, please put them in the comments below. I will certainly check them for the first few days after the videos come out. And if people are interested in doing some more in-depth building local multimodal rag stuff, let me know as well. I can always look at doing a video of how you would actually put something like that together. All right, as always, if you found the video useful, please click like and subscribe, and I will talk to you in the next video. Bye for now.",
  "fetchedAt": "2026-01-18T18:33:42.258Z"
}