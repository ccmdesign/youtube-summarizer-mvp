{
  "videoId": "lXUZvyajciY",
  "language": "en",
  "source": "caption-extractor",
  "segments": [
    {
      "start": 48.56,
      "duration": 4.4,
      "text": "Today I'm speaking with Andrej Karpathy.\nAndrej, why do you say that this will be  "
    },
    {
      "start": 52.96,
      "duration": 5.52,
      "text": "the decade of agents and not the year of agents?\nFirst of all, thank you for having me here. I'm  "
    },
    {
      "start": 58.48,
      "duration": 4.528,
      "text": "excited to be here. The quote you've just \nmentioned, \"It's the decade of agents,\"  "
    },
    {
      "start": 63.008,
      "duration": 6.512,
      "text": "is actually a reaction to a pre-existing quote.\nI'm not actually sure who said this but they were  "
    },
    {
      "start": 69.52,
      "duration": 5.2,
      "text": "alluding to this being the year of agents with \nrespect to LLMs and how they were going to evolve. "
    },
    {
      "start": 76.24,
      "duration": 4.24,
      "text": "I was triggered by that because there's some \nover-prediction going on in the industry. "
    },
    {
      "start": 81.28,
      "duration": 4.24,
      "text": "In my mind, this is more accurately \ndescribed as the decade of agents. "
    },
    {
      "start": 85.52,
      "duration": 2.96,
      "text": "We have some very early agents that \nare extremely impressive and that I  "
    },
    {
      "start": 88.48,
      "duration": 5.76,
      "text": "use daily—Claude and Codex and so on—but I \nstill feel there's so much work to be done. "
    },
    {
      "start": 95.44,
      "duration": 2.8,
      "text": "My reaction is we'll be working \nwith these things for a decade. "
    },
    {
      "start": 98.24,
      "duration": 3.2,
      "text": "They're going to get better, \nand it's going to be wonderful. "
    },
    {
      "start": 102,
      "duration": 4.88,
      "text": "I was just reacting to the \ntimelines of the implication. "
    },
    {
      "start": 106.88,
      "duration": 3.84,
      "text": "What do you think will take a decade to \naccomplish? What are the bottlenecks? "
    },
    {
      "start": 111.84,
      "duration": 4.4,
      "text": "Actually making it work. When you're talking \nabout an agent, or what the labs have in mind  "
    },
    {
      "start": 116.24,
      "duration": 3.68,
      "text": "and maybe what I have in mind as well, you \nshould think of it almost like an employee or  "
    },
    {
      "start": 119.92,
      "duration": 4.16,
      "text": "an intern that you would hire to work with you.\nFor example, you work with some employees here. "
    },
    {
      "start": 124.72,
      "duration": 4,
      "text": "When would you prefer to have an agent like \nClaude or Codex do that work? Currently,  "
    },
    {
      "start": 128.72,
      "duration": 3.04,
      "text": "of course they can't. What would it \ntake for them to be able to do that? "
    },
    {
      "start": 131.76,
      "duration": 2.08,
      "text": "Why don't you do it today?\nThe reason you don't do it  "
    },
    {
      "start": 133.84,
      "duration": 3.52,
      "text": "today is because they just don't work.\nThey don't have enough intelligence,  "
    },
    {
      "start": 137.36,
      "duration": 3.52,
      "text": "they're not multimodal enough, they \ncan't do computer use and all this stuff. "
    },
    {
      "start": 140.88,
      "duration": 3.28,
      "text": "They don't do a lot of the things you've \nalluded to earlier. They don't have  "
    },
    {
      "start": 144.16,
      "duration": 2.96,
      "text": "continual learning. You can't just tell \nthem something and they'll remember it. "
    },
    {
      "start": 147.12,
      "duration": 2.72,
      "text": "They're cognitively lacking \nand it's just not working. "
    },
    {
      "start": 150.56,
      "duration": 2.24,
      "text": "It will take about a decade to \nwork through all of those issues. "
    },
    {
      "start": 152.8,
      "duration": 8.4,
      "text": "Interesting. As a professional podcaster \nand a viewer of AI from afar, it's easy  "
    },
    {
      "start": 162.08,
      "duration": 5.04,
      "text": "for me to identify what's lacking: continual \nlearning is lacking, or multimodality is lacking. "
    },
    {
      "start": 167.12,
      "duration": 5.6,
      "text": "But I don't really have a good way \nof trying to put a timeline on it. "
    },
    {
      "start": 172.72,
      "duration": 5.2,
      "text": "If somebody asks how long continual learning \nwill take, I have no prior about whether  "
    },
    {
      "start": 177.92,
      "duration": 3.52,
      "text": "this is a project that should take 5 \nyears, 10 years, or 50 years. Why a  "
    },
    {
      "start": 181.44,
      "duration": 6.24,
      "text": "decade? Why not one year? Why not 50 years?\nThis is where you get into a bit of my own  "
    },
    {
      "start": 187.68,
      "duration": 6.32,
      "text": "intuition, and doing a bit of an extrapolation \nwith respect to my own experience in the field. "
    },
    {
      "start": 194.64,
      "duration": 5.28,
      "text": "I've been in AI for almost two decades.\nIt's going to be 15 years or so, not that long. "
    },
    {
      "start": 199.92,
      "duration": 3.12,
      "text": "You had Richard Sutton here, \nwho was around for much longer. "
    },
    {
      "start": 203.04,
      "duration": 4.96,
      "text": "I do have about 15 years of experience of people \nmaking predictions, of seeing how they turned out. "
    },
    {
      "start": 208,
      "duration": 2.48,
      "text": "Also I was in the industry for \na while, I was in research,  "
    },
    {
      "start": 210.48,
      "duration": 1.92,
      "text": "and I've worked in the industry for a while. "
    },
    {
      "start": 212.4,
      "duration": 3.92,
      "text": "I have a general intuition \nthat I have left from that. "
    },
    {
      "start": 217.76,
      "duration": 6.08,
      "text": "I feel like the problems are tractable, they're \nsurmountable, but they're still difficult. "
    },
    {
      "start": 223.84,
      "duration": 3.2,
      "text": "If I just average it out, it \njust feels like a decade to me. "
    },
    {
      "start": 227.04,
      "duration": 3.84,
      "text": "This is quite interesting. I want \nto hear not only the history,  "
    },
    {
      "start": 230.88,
      "duration": 7.2,
      "text": "but what people in the room felt was about to \nhappen at various different breakthrough moments. "
    },
    {
      "start": 238.08,
      "duration": 4.96,
      "text": "What were the ways in which their feelings were \neither overly pessimistic or overly optimistic? "
    },
    {
      "start": 243.6,
      "duration": 3.52,
      "text": "Should we just go through each of them one by one?\nThat's a giant question because you're talking  "
    },
    {
      "start": 247.12,
      "duration": 3.76,
      "text": "about 15 years of stuff that happened.\nAI is so wonderful because there have been  "
    },
    {
      "start": 250.88,
      "duration": 5.36,
      "text": "a number of seismic shifts where the entire \nfield has suddenly looked a different way. "
    },
    {
      "start": 257.04,
      "duration": 4,
      "text": "I've maybe lived through two or three of those.\nI still think there will continue to be  "
    },
    {
      "start": 261.04,
      "duration": 4.16,
      "text": "some because they come with \nalmost surprising regularity. "
    },
    {
      "start": 265.2,
      "duration": 4.32,
      "text": "When my career began, when I started to work on \ndeep learning, when I became interested in deep  "
    },
    {
      "start": 269.52,
      "duration": 4.48,
      "text": "learning, this was by chance of being right next \nto Geoff Hinton at the University of Toronto. "
    },
    {
      "start": 274,
      "duration": 3.04,
      "text": "Geoff Hinton, of course, is \nthe godfather figure of AI. "
    },
    {
      "start": 277.04,
      "duration": 3.12,
      "text": "He was training all these neural networks.\nI thought it was incredible and interesting. "
    },
    {
      "start": 280.16,
      "duration": 3.52,
      "text": "This was not the main thing that \neveryone in AI was doing by far. "
    },
    {
      "start": 283.68,
      "duration": 5.76,
      "text": "This was a niche little subject on the side.\nThat's maybe the first dramatic seismic shift  "
    },
    {
      "start": 289.44,
      "duration": 4.56,
      "text": "that came with the AlexNet and so on.\nAlexNet reoriented everyone, and everyone  "
    },
    {
      "start": 294,
      "duration": 5.6,
      "text": "started to train neural networks, but it \nwas still very per-task, per specific task. "
    },
    {
      "start": 299.6,
      "duration": 4.8,
      "text": "Maybe I have an image classifier or I have a \nneural machine translator or something like that. "
    },
    {
      "start": 304.4,
      "duration": 6.56,
      "text": "People became very slowly interested in agents.\nPeople started to think, \"Okay, maybe we have a  "
    },
    {
      "start": 310.96,
      "duration": 3.28,
      "text": "check mark next to the visual cortex or something \nlike that, but what about the other parts of the  "
    },
    {
      "start": 314.24,
      "duration": 4.8,
      "text": "brain, and how can we get a full agent or a \nfull entity that can interact in the world?\" "
    },
    {
      "start": 319.6,
      "duration": 6.4,
      "text": "The Atari deep reinforcement learning shift \nin 2013 or so was part of that early effort  "
    },
    {
      "start": 326,
      "duration": 4.16,
      "text": "of agents, in my mind, because it was an \nattempt to try to get agents that not just  "
    },
    {
      "start": 330.16,
      "duration": 3.92,
      "text": "perceive the world, but also take actions and \ninteract and get rewards from environments. "
    },
    {
      "start": 334.08,
      "duration": 4.4,
      "text": "At the time, this was Atari games.\nI feel that was a misstep. "
    },
    {
      "start": 339.68,
      "duration": 5.92,
      "text": "It was a misstep that even the early OpenAI that \nI was a part of adopted because at that time,  "
    },
    {
      "start": 345.6,
      "duration": 4.56,
      "text": "the zeitgeist was reinforcement learning \nenvironments, games, game playing,  "
    },
    {
      "start": 350.16,
      "duration": 3.92,
      "text": "beat games, get lots of different types of \ngames, and OpenAI was doing a lot of that. "
    },
    {
      "start": 354.08,
      "duration": 7.36,
      "text": "That was another prominent part of AI where maybe \nfor two or three or four years, everyone was doing  "
    },
    {
      "start": 361.44,
      "duration": 5.36,
      "text": "reinforcement learning on games.\nThat was all a bit of a misstep. "
    },
    {
      "start": 366.8,
      "duration": 3.2,
      "text": "What I was trying to do at OpenAI is \nI was always a bit suspicious of games  "
    },
    {
      "start": 370,
      "duration": 3.92,
      "text": "as being this thing that would lead to AGI.\nBecause in my mind, you want something like  "
    },
    {
      "start": 373.92,
      "duration": 3.68,
      "text": "an accountant or something that's \ninteracting with the real world. "
    },
    {
      "start": 377.6,
      "duration": 6.24,
      "text": "I just didn't see how games add up to it.\nMy project at OpenAI, for example, was within the  "
    },
    {
      "start": 383.84,
      "duration": 6.24,
      "text": "scope of the Universe project, on an agent that \nwas using keyboard and mouse to operate web pages. "
    },
    {
      "start": 390.08,
      "duration": 3.76,
      "text": "I really wanted to have something that \ninteracts with the actual digital world  "
    },
    {
      "start": 393.84,
      "duration": 3.44,
      "text": "that can do knowledge work.\nIt just so turns out that this  "
    },
    {
      "start": 397.28,
      "duration": 3.92,
      "text": "was extremely early, way too early, so early \nthat we shouldn't have been working on that. "
    },
    {
      "start": 402.08,
      "duration": 5.12,
      "text": "Because if you're just stumbling your way around \nand keyboard mashing and mouse clicking and trying  "
    },
    {
      "start": 407.2,
      "duration": 4.88,
      "text": "to get rewards in these environments, your \nreward is too sparse and you just won't learn. "
    },
    {
      "start": 412.08,
      "duration": 2.8,
      "text": "You're going to burn a forest \ncomputing, and you're never  "
    },
    {
      "start": 414.88,
      "duration": 3.68,
      "text": "going to get something off the ground.\nWhat you're missing is this power of  "
    },
    {
      "start": 418.56,
      "duration": 3.76,
      "text": "representation in the neural network.\nFor example, today people are training  "
    },
    {
      "start": 422.32,
      "duration": 3.44,
      "text": "those computer-using agents, but they're \ndoing it on top of a large language model. "
    },
    {
      "start": 425.76,
      "duration": 2.96,
      "text": "You have to get the language model first, \nyou have to get the representations first,  "
    },
    {
      "start": 428.72,
      "duration": 3.04,
      "text": "and you have to do that by all the \npre-training and all the LLM stuff. "
    },
    {
      "start": 431.76,
      "duration": 6.16,
      "text": "I feel maybe loosely speaking, people \nkept trying to get the full thing too  "
    },
    {
      "start": 437.92,
      "duration": 4.88,
      "text": "early a few times, where people really try \nto go after agents too early, I would say. "
    },
    {
      "start": 442.8,
      "duration": 3.44,
      "text": "That was Atari and Universe \nand even my own experience. "
    },
    {
      "start": 446.24,
      "duration": 3.44,
      "text": "You actually have to do some things \nfirst before you get to those agents. "
    },
    {
      "start": 450.56,
      "duration": 5.52,
      "text": "Now the agents are a lot more competent, but maybe \nwe're still missing some parts of that stack. "
    },
    {
      "start": 456.08,
      "duration": 4.16,
      "text": "I would say those are the three major \nbuckets of what people were doing:  "
    },
    {
      "start": 460.24,
      "duration": 4.32,
      "text": "training neural nets per-tasks, \ntrying the first round of agents,  "
    },
    {
      "start": 464.56,
      "duration": 4.08,
      "text": "and then maybe the LLMs and seeking the \nrepresentation power of the neural networks  "
    },
    {
      "start": 468.64,
      "duration": 5.04,
      "text": "before you tack on everything else on top.\nInteresting. If I were to steelman the Sutton  "
    },
    {
      "start": 474.32,
      "duration": 3.52,
      "text": "perspective, it would be that humans \ncan just take on everything at once,  "
    },
    {
      "start": 477.84,
      "duration": 4.08,
      "text": "or even animals can take on everything at once.\nAnimals are maybe a better example because they  "
    },
    {
      "start": 481.92,
      "duration": 3.44,
      "text": "don't even have the scaffold of language.\nThey just get thrown out into the world,  "
    },
    {
      "start": 485.36,
      "duration": 3.36,
      "text": "and they just have to make sense \nof everything without any labels. "
    },
    {
      "start": 490,
      "duration": 4.32,
      "text": "The vision for AGI then should just be \nsomething which looks at sensory data,  "
    },
    {
      "start": 494.32,
      "duration": 4.48,
      "text": "looks at the computer screen, and it just \nfigures out what's going on from scratch. "
    },
    {
      "start": 498.8,
      "duration": 3.44,
      "text": "If a human were put in a similar situation and \nhad to be trained from scratch… This is like a  "
    },
    {
      "start": 502.24,
      "duration": 3.76,
      "text": "human growing up or an animal growing up.\nWhy shouldn't that be the vision for AI,  "
    },
    {
      "start": 506,
      "duration": 3.92,
      "text": "rather than this thing where we're \ndoing millions of years of training? "
    },
    {
      "start": 509.92,
      "duration": 5.68,
      "text": "That's a really good question. Sutton was \non your podcast and I saw the podcast and I  "
    },
    {
      "start": 515.6,
      "duration": 4.4,
      "text": "had a write-up about that podcast that \ngets into a bit of how I see things. "
    },
    {
      "start": 521.6,
      "duration": 4.64,
      "text": "I'm very careful to make analogies to \nanimals because they came about by a  "
    },
    {
      "start": 526.24,
      "duration": 3.6,
      "text": "very different optimization process.\nAnimals are evolved, and they come  "
    },
    {
      "start": 529.84,
      "duration": 6,
      "text": "with a huge amount of hardware that's built in.\nFor example, my example in the post was the zebra. "
    },
    {
      "start": 535.84,
      "duration": 3.44,
      "text": "A zebra gets born, and a few minutes later \nit's running around and following its mother. "
    },
    {
      "start": 539.28,
      "duration": 4.64,
      "text": "That's an extremely complicated thing to do. \nThat's not reinforcement learning. That's  "
    },
    {
      "start": 543.92,
      "duration": 4.24,
      "text": "something that's baked in. Evolution obviously \nhas some way of encoding the weights of our  "
    },
    {
      "start": 548.16,
      "duration": 4.48,
      "text": "neural nets in ATCGs, and I have no idea \nhow that works, but it apparently works. "
    },
    {
      "start": 554.64,
      "duration": 5.76,
      "text": "Brains just came from a very different process, \nand I'm very hesitant to take inspiration from it  "
    },
    {
      "start": 560.4,
      "duration": 5.36,
      "text": "because we're not actually running that process.\nIn my post, I said we're not building animals. "
    },
    {
      "start": 565.76,
      "duration": 4.48,
      "text": "We're building ghosts or spirits or \nwhatever people want to call it, because  "
    },
    {
      "start": 571.92,
      "duration": 5.6,
      "text": "we're not doing training by evolution.\nWe're doing training by imitation of humans  "
    },
    {
      "start": 577.52,
      "duration": 5.28,
      "text": "and the data that they've put on the Internet.\nYou end up with these ethereal spirit entities  "
    },
    {
      "start": 582.8,
      "duration": 2.56,
      "text": "because they're fully digital \nand they're mimicking humans. "
    },
    {
      "start": 585.36,
      "duration": 3.36,
      "text": "It's a different kind of intelligence.\nIf you imagine a space of intelligences,  "
    },
    {
      "start": 588.72,
      "duration": 4.56,
      "text": "we're starting off at a different point almost. \nWe're not really building animals. But it's also  "
    },
    {
      "start": 593.28,
      "duration": 3.52,
      "text": "possible to make them a bit more animal-like \nover time, and I think we should be doing that.  "
    },
    {
      "start": 598.48,
      "duration": 6.4,
      "text": "One more point. I do feel Sutton has a very...\nHis framework is, \"We want to build animals.\" "
    },
    {
      "start": 604.88,
      "duration": 2.8,
      "text": "I think that would be wonderful if we can \nget that to work. That would be amazing.  "
    },
    {
      "start": 607.68,
      "duration": 5.44,
      "text": "If there were a single algorithm that you \ncan just run on the Internet and it learns  "
    },
    {
      "start": 613.12,
      "duration": 5.68,
      "text": "everything, that would be incredible.\nI'm not sure that it exists and that's  "
    },
    {
      "start": 618.8,
      "duration": 4.64,
      "text": "certainly not what animals do, because \nanimals have this outer loop of evolution. "
    },
    {
      "start": 624.4,
      "duration": 4,
      "text": "A lot of what looks like learning is \nmore like maturation of the brain. "
    },
    {
      "start": 628.4,
      "duration": 4.48,
      "text": "I think there's very little \nreinforcement learning for animals. "
    },
    {
      "start": 632.88,
      "duration": 4.24,
      "text": "A lot of the reinforcement learning is more \nlike motor tasks; it's not intelligence tasks. "
    },
    {
      "start": 637.12,
      "duration": 3.36,
      "text": "So I actually kind of think humans \ndon’t really use RL, roughly speaking. "
    },
    {
      "start": 641.12,
      "duration": 1.84,
      "text": "Can you repeat the last sentence?\nA lot of that intelligence is  "
    },
    {
      "start": 642.96,
      "duration": 2.64,
      "text": "not motor task…it's what, sorry?\nA lot of the reinforcement learning, in my  "
    },
    {
      "start": 645.6,
      "duration": 6.08,
      "text": "perspective, would be things that are a lot more \nmotor-like, simple tasks like throwing a hoop. "
    },
    {
      "start": 653.44,
      "duration": 4,
      "text": "But I don't think that humans use reinforcement \nlearning for a lot of intelligence tasks  "
    },
    {
      "start": 657.44,
      "duration": 4.08,
      "text": "like problem-solving and so on.\nThat doesn't mean we shouldn't  "
    },
    {
      "start": 661.52,
      "duration": 4.4,
      "text": "do that for research, but I just feel \nlike that's what animals do or don't. "
    },
    {
      "start": 665.92,
      "duration": 3.68,
      "text": "I'm going to take a second to digest that \nbecause there are a lot of different ideas. "
    },
    {
      "start": 669.6,
      "duration": 4.64,
      "text": "Here’s one clarifying question I can \nask to understand the perspective. "
    },
    {
      "start": 675.28,
      "duration": 3.36,
      "text": "You suggest that evolution is doing \nthe kind of thing that pre-training  "
    },
    {
      "start": 678.64,
      "duration": 5.36,
      "text": "does in the sense of building something \nwhich can then understand the world. "
    },
    {
      "start": 684,
      "duration": 4.32,
      "text": "The difference is that evolution \nhas to be titrated in the case  "
    },
    {
      "start": 688.32,
      "duration": 9.04,
      "text": "of humans through three gigabytes of DNA.\nThat's very unlike the weights of a model. "
    },
    {
      "start": 697.36,
      "duration": 5.44,
      "text": "Literally, the weights of the model are \na brain, which obviously does not exist  "
    },
    {
      "start": 702.8,
      "duration": 3.76,
      "text": "in the sperm and the egg.\nSo it has to be grown. "
    },
    {
      "start": 706.56,
      "duration": 4.08,
      "text": "Also, the information for every single \nsynapse in the brain simply cannot exist  "
    },
    {
      "start": 710.64,
      "duration": 4.96,
      "text": "in the three gigabytes that exist in the DNA.\nEvolution seems closer to finding the algorithm  "
    },
    {
      "start": 715.6,
      "duration": 5.04,
      "text": "which then does the lifetime learning.\nNow, maybe the lifetime learning is  "
    },
    {
      "start": 720.64,
      "duration": 4,
      "text": "not analogous to RL, to your point.\nIs that compatible with the thing you  "
    },
    {
      "start": 724.64,
      "duration": 2.32,
      "text": "were saying, or would you disagree with that?\nI think so. I would agree with you that there's  "
    },
    {
      "start": 726.96,
      "duration": 2.08,
      "text": "some miraculous compression \ngoing on because obviously,  "
    },
    {
      "start": 729.04,
      "duration": 4.96,
      "text": "the weights of the neural net are not stored \nin ATCGs. There's some dramatic compression.  "
    },
    {
      "start": 734,
      "duration": 4.88,
      "text": "There are some learning algorithms encoded that \ntake over and do some of the learning online. "
    },
    {
      "start": 738.88,
      "duration": 4.72,
      "text": "I definitely agree with you on that.\nI would say I'm a lot more practically minded. "
    },
    {
      "start": 743.6,
      "duration": 2.48,
      "text": "I don't come at it from the \nperspective of, let's build animals. "
    },
    {
      "start": 746.08,
      "duration": 2.88,
      "text": "I come from it from the perspective \nof, let's build useful things. "
    },
    {
      "start": 748.96,
      "duration": 2.96,
      "text": "I have a hard hat on, and I'm just \nobserving that we're not going to do  "
    },
    {
      "start": 751.92,
      "duration": 5.04,
      "text": "evolution, because I don't know how to do that.\nBut it does turn out we can build these ghosts,  "
    },
    {
      "start": 756.96,
      "duration": 6.16,
      "text": "spirit-like entities, by imitating internet \ndocuments. This works. It's a way to bring you  "
    },
    {
      "start": 763.12,
      "duration": 5.28,
      "text": "up to something that has a lot of built-in \nknowledge and intelligence in some way,  "
    },
    {
      "start": 768.4,
      "duration": 3.52,
      "text": "similar to maybe what evolution has done.\nThat's why I call pre-training  "
    },
    {
      "start": 771.92,
      "duration": 4.24,
      "text": "this crappy evolution.\nIt's the practically possible version  "
    },
    {
      "start": 776.16,
      "duration": 5.04,
      "text": "with our technology and what we have available \nto us to get to a starting point where we can do  "
    },
    {
      "start": 781.2,
      "duration": 4.16,
      "text": "things like reinforcement learning and so on.\nJust to steelman the other perspective,  "
    },
    {
      "start": 785.36,
      "duration": 3.84,
      "text": "after doing this Sutton interview and thinking \nabout it a bit, he has an important point here. "
    },
    {
      "start": 789.2,
      "duration": 4.88,
      "text": "Evolution does not give us the knowledge, really.\nIt gives us the algorithm to find the knowledge,  "
    },
    {
      "start": 794.08,
      "duration": 5.36,
      "text": "and that seems different from pre-training.\nPerhaps the perspective is that pre-training helps  "
    },
    {
      "start": 799.44,
      "duration": 3.84,
      "text": "build the kind of entity which can learn better.\nIt teaches meta-learning, and therefore  "
    },
    {
      "start": 803.28,
      "duration": 5.04,
      "text": "it is similar to finding an algorithm.\nBut if it's \"Evolution gives us knowledge,  "
    },
    {
      "start": 808.32,
      "duration": 3.12,
      "text": "pre-training gives us knowledge,\" \nthat analogy seems to break down. "
    },
    {
      "start": 811.44,
      "duration": 3.6,
      "text": "It's subtle and I think you're right to \npush back on it, but basically the thing  "
    },
    {
      "start": 815.04,
      "duration": 3.84,
      "text": "that pre-training is doing, you're getting \nthe next-token predictor over the internet,  "
    },
    {
      "start": 818.88,
      "duration": 4.8,
      "text": "and you're training that into a neural net.\nIt's doing two things that are unrelated. "
    },
    {
      "start": 823.68,
      "duration": 2.4,
      "text": "Number one, it's picking up all \nthis knowledge, as I call it. "
    },
    {
      "start": 826.08,
      "duration": 5.04,
      "text": "Number two, it's actually becoming intelligent.\nBy observing the algorithmic patterns in the  "
    },
    {
      "start": 831.12,
      "duration": 4.56,
      "text": "internet, it boots up all these little circuits \nand algorithms inside the neural net to do things  "
    },
    {
      "start": 835.68,
      "duration": 5.12,
      "text": "like in-context learning and all this stuff.\nYou don't need or want the knowledge. "
    },
    {
      "start": 840.8,
      "duration": 4.48,
      "text": "I think that's probably holding back the neural \nnetworks overall because it's getting them to rely  "
    },
    {
      "start": 845.28,
      "duration": 4.48,
      "text": "on the knowledge a little too much sometimes.\nFor example, I feel agents, one thing they're  "
    },
    {
      "start": 849.76,
      "duration": 3.68,
      "text": "not very good at, is going off the data \nmanifold of what exists on the internet. "
    },
    {
      "start": 853.44,
      "duration": 3.92,
      "text": "If they had less knowledge or less \nmemory, maybe they would be better. "
    },
    {
      "start": 857.92,
      "duration": 3.76,
      "text": "What I think we have to do going forward—and \nthis would be part of the research paradigms—is  "
    },
    {
      "start": 863.68,
      "duration": 5.04,
      "text": "figure out ways to remove some of the knowledge \nand to keep what I call this cognitive core. "
    },
    {
      "start": 868.72,
      "duration": 5.36,
      "text": "It's this intelligent entity that is stripped from \nknowledge but contains the algorithms and contains  "
    },
    {
      "start": 874.08,
      "duration": 5.28,
      "text": "the magic of intelligence and problem-solving \nand the strategies of it and all this stuff. "
    },
    {
      "start": 879.36,
      "duration": 4.72,
      "text": "There's so much interesting stuff there. Let's \nstart with in-context learning. This is an  "
    },
    {
      "start": 884.08,
      "duration": 4.72,
      "text": "obvious point, but I think it's worth just \nsaying it explicitly and meditating on it. "
    },
    {
      "start": 888.8,
      "duration": 5.04,
      "text": "The situation in which these models seem the most \nintelligent—in which I talk to them and I'm like,  "
    },
    {
      "start": 893.84,
      "duration": 4.4,
      "text": "\"Wow, there's really something on the other end \nthat's responding to me thinking about things—is  "
    },
    {
      "start": 898.24,
      "duration": 3.6,
      "text": "if it makes a mistake it's like, \"Oh wait, that's \nthe wrong way to think about it. I'm backing up.\"  "
    },
    {
      "start": 901.84,
      "duration": 2.64,
      "text": "All that is happening in context.\nThat's where I feel like the real  "
    },
    {
      "start": 904.48,
      "duration": 6.08,
      "text": "intelligence is that you can visibly see.\nThat in-context learning process is  "
    },
    {
      "start": 910.56,
      "duration": 5.76,
      "text": "developed by gradient descent on pre-training.\nIt spontaneously meta-learns in-context learning,  "
    },
    {
      "start": 916.32,
      "duration": 5.28,
      "text": "but the in-context learning itself is not \ngradient descent, in the same way that our  "
    },
    {
      "start": 921.6,
      "duration": 4.96,
      "text": "lifetime intelligence as humans to be able \nto do things is conditioned by evolution  "
    },
    {
      "start": 926.56,
      "duration": 4.32,
      "text": "but our learning during our lifetime is \nhappening through some other process. "
    },
    {
      "start": 930.88,
      "duration": 3.6,
      "text": "I don't fully agree with that, but \nyou should continue your thought. "
    },
    {
      "start": 934.48,
      "duration": 2.4,
      "text": "Well, I'm very curious to understand \nhow that analogy breaks down. "
    },
    {
      "start": 936.88,
      "duration": 3.76,
      "text": "I'm hesitant to say that in-context \nlearning is not doing gradient descent. "
    },
    {
      "start": 941.76,
      "duration": 5.52,
      "text": "It's not doing explicit gradient descent.\nIn-context learning is pattern completion  "
    },
    {
      "start": 947.28,
      "duration": 3.04,
      "text": "within a token window.\nIt just turns out that there's  "
    },
    {
      "start": 950.32,
      "duration": 3.44,
      "text": "a huge amount of patterns on the internet.\nYou're right, the model learns to complete  "
    },
    {
      "start": 953.76,
      "duration": 4.24,
      "text": "the pattern, and that's inside the weights.\nThe weights of the neural network are trying  "
    },
    {
      "start": 958,
      "duration": 4.08,
      "text": "to discover patterns and complete the pattern.\nThere's some adaptation that happens inside  "
    },
    {
      "start": 962.08,
      "duration": 4.08,
      "text": "the neural network, which is magical \nand just falls out from the internet  "
    },
    {
      "start": 966.16,
      "duration": 4.4,
      "text": "just because there's a lot of patterns.\nI will say that there have been some papers  "
    },
    {
      "start": 970.56,
      "duration": 3.44,
      "text": "that I thought were interesting that look at \nthe mechanisms behind in-context learning. "
    },
    {
      "start": 974,
      "duration": 3.68,
      "text": "I do think it's possible that in-context \nlearning runs a small gradient descent loop  "
    },
    {
      "start": 977.68,
      "duration": 4.64,
      "text": "internally in the layers of the neural network.\nI recall one paper in particular where they were  "
    },
    {
      "start": 982.32,
      "duration": 8.32,
      "text": "doing linear regression using in-context learning.\nYour inputs into the neural network are XY pairs,  "
    },
    {
      "start": 990.64,
      "duration": 4.96,
      "text": "XY, XY, XY that happen to be on the line.\nThen you do X and you expect Y. "
    },
    {
      "start": 995.6,
      "duration": 5.04,
      "text": "The neural network, when you train it \nin this way, does linear regression. "
    },
    {
      "start": 1001.52,
      "duration": 4.72,
      "text": "Normally when you would run linear regression, you \nhave a small gradient descent optimizer that looks  "
    },
    {
      "start": 1006.24,
      "duration": 4.4,
      "text": "at XY, looks at an error, calculates the gradient \nof the weights and does the update a few times. "
    },
    {
      "start": 1010.64,
      "duration": 4.16,
      "text": "It just turns out that when they looked at the \nweights of that in-context learning algorithm,  "
    },
    {
      "start": 1014.8,
      "duration": 4.32,
      "text": "they found some analogies to \ngradient descent mechanics. "
    },
    {
      "start": 1019.12,
      "duration": 4.8,
      "text": "In fact, I think the paper was even stronger \nbecause they hardcoded the weights of a neural  "
    },
    {
      "start": 1023.92,
      "duration": 6.08,
      "text": "network to do gradient descent through \nattention and all the internals of the  "
    },
    {
      "start": 1030,
      "duration": 4.56,
      "text": "neural network. That's just my only pushback. \nWho knows how in-context learning works,  "
    },
    {
      "start": 1034.56,
      "duration": 5.36,
      "text": "but I think that it's probably doing a bit of some \nfunky gradient descent internally. I think that  "
    },
    {
      "start": 1039.92,
      "duration": 4.88,
      "text": "that's possible. I was only pushing back on your \nsaying that it's not doing in-context learning. "
    },
    {
      "start": 1044.8,
      "duration": 3.44,
      "text": "Who knows what it's doing, but it's probably maybe \ndoing something similar to it, but we don't know. "
    },
    {
      "start": 1048.24,
      "duration": 6.24,
      "text": "So then it's worth thinking okay, if \nin-context learning and pre-training  "
    },
    {
      "start": 1054.48,
      "duration": 4.8,
      "text": "are both implementing something like gradient \ndescent, why does it feel like with in-context  "
    },
    {
      "start": 1059.28,
      "duration": 5.36,
      "text": "learning we're getting to this continual \nlearning, real intelligence-like thing? "
    },
    {
      "start": 1064.64,
      "duration": 5.28,
      "text": "Whereas you don't get the analogous feeling just \nfrom pre-training. You could argue that. If it's  "
    },
    {
      "start": 1069.92,
      "duration": 3.12,
      "text": "the same algorithm, what could be different?\nOne way you could think about it is,  "
    },
    {
      "start": 1073.04,
      "duration": 7.2,
      "text": "how much information does the model store \nper information it receives from training? "
    },
    {
      "start": 1080.24,
      "duration": 3.6,
      "text": "If you look at pre-training, if \nyou look at Llama 3 for example,  "
    },
    {
      "start": 1083.84,
      "duration": 6.16,
      "text": "I think it's trained on 15 trillion tokens.\nIf you look at the 70B model, that would  "
    },
    {
      "start": 1090,
      "duration": 5.28,
      "text": "be the equivalent of 0.07 bits per \ntoken that it sees in pre-training,  "
    },
    {
      "start": 1095.28,
      "duration": 3.52,
      "text": "in terms of the information in the weights \nof the model compared to the tokens it reads. "
    },
    {
      "start": 1098.8,
      "duration": 4,
      "text": "Whereas if you look at the KV cache \nand how it grows per additional token  "
    },
    {
      "start": 1102.8,
      "duration": 6.48,
      "text": "in in-context learning, it's like 320 kilobytes.\nSo that's a 35 million-fold difference in how much  "
    },
    {
      "start": 1109.28,
      "duration": 5.6,
      "text": "information per token is assimilated by the model.\nI wonder if that's relevant at all. "
    },
    {
      "start": 1114.88,
      "duration": 4.88,
      "text": "I kind of agree. The way I usually put this is \nthat anything that happens during the training of  "
    },
    {
      "start": 1119.76,
      "duration": 5.84,
      "text": "the neural network, the knowledge is only a hazy \nrecollection of what happened in training time. "
    },
    {
      "start": 1125.6,
      "duration": 3.28,
      "text": "That's because the compression is dramatic.\nYou're taking 15 trillion tokens and you're  "
    },
    {
      "start": 1128.88,
      "duration": 2.8,
      "text": "compressing it to just your final neural \nnetwork of a few billion parameters. "
    },
    {
      "start": 1131.68,
      "duration": 2.16,
      "text": "Obviously it's a massive \namount of compression going on. "
    },
    {
      "start": 1134.56,
      "duration": 3.36,
      "text": "So I refer to it as a hazy \nrecollection of the internet documents. "
    },
    {
      "start": 1137.92,
      "duration": 2.8,
      "text": "Whereas anything that happens in the \ncontext window of the neural network—you're  "
    },
    {
      "start": 1140.72,
      "duration": 3.52,
      "text": "plugging in all the tokens and building up \nall those KV cache representations—is very  "
    },
    {
      "start": 1144.24,
      "duration": 3.76,
      "text": "directly accessible to the neural net.\nSo I compare the KV cache and the stuff  "
    },
    {
      "start": 1148,
      "duration": 3.6,
      "text": "that happens at test time to \nmore like a working memory. "
    },
    {
      "start": 1151.6,
      "duration": 4.64,
      "text": "All the stuff that's in the context window is \nvery directly accessible to the neural net. "
    },
    {
      "start": 1156.24,
      "duration": 4.88,
      "text": "There's always these almost surprising \nanalogies between LLMs and humans. "
    },
    {
      "start": 1161.12,
      "duration": 4.16,
      "text": "I find them surprising because we're not \ntrying to build a human brain directly. "
    },
    {
      "start": 1165.28,
      "duration": 2,
      "text": "We're just finding that this \nworks and we're doing it. "
    },
    {
      "start": 1167.28,
      "duration": 3.44,
      "text": "But I do think that anything \nthat's in the weights, it's a  "
    },
    {
      "start": 1170.72,
      "duration": 5.36,
      "text": "hazy recollection of what you read a year ago.\nAnything that you give it as a context at test  "
    },
    {
      "start": 1176.08,
      "duration": 4,
      "text": "time is directly in the working memory.\nThat's a very powerful analogy to  "
    },
    {
      "start": 1180.08,
      "duration": 2.08,
      "text": "think through things.\nWhen you, for example,  "
    },
    {
      "start": 1182.16,
      "duration": 3.84,
      "text": "go to an LLM and you ask it about some book \nand what happened in it, like Nick Lane's  "
    },
    {
      "start": 1186,
      "duration": 3.68,
      "text": "book or something like that, the LLM will often \ngive you some stuff which is roughly correct. "
    },
    {
      "start": 1189.68,
      "duration": 3.92,
      "text": "But if you give it the full chapter and ask it \nquestions, you're going to get much better results  "
    },
    {
      "start": 1193.6,
      "duration": 2.64,
      "text": "because it's now loaded in the \nworking memory of the model. "
    },
    {
      "start": 1196.24,
      "duration": 4.08,
      "text": "So a very long way of saying \nI agree and that's why. "
    },
    {
      "start": 1200.32,
      "duration": 2.56,
      "text": "Stepping back, what is the part \nabout human intelligence that we  "
    },
    {
      "start": 1203.76,
      "duration": 4.64,
      "text": "have most failed to replicate with these models? "
    },
    {
      "start": 1212.4,
      "duration": 4.72,
      "text": "Just a lot of it. So maybe one way to think \nabout it, I don't know if this is the best way,  "
    },
    {
      "start": 1217.12,
      "duration": 5.52,
      "text": "but I almost feel like — again, making these \nanalogies imperfect as they are — we've stumbled  "
    },
    {
      "start": 1222.64,
      "duration": 4.72,
      "text": "by with the transformer neural network, \nwhich is extremely powerful, very general. "
    },
    {
      "start": 1227.36,
      "duration": 4,
      "text": "You can train transformers on audio, or \nvideo, or text, or whatever you want,  "
    },
    {
      "start": 1231.36,
      "duration": 3.92,
      "text": "and it just learns patterns and they're \nvery powerful, and it works really well. "
    },
    {
      "start": 1235.28,
      "duration": 3.68,
      "text": "That to me almost indicates that this \nis some piece of cortical tissue. "
    },
    {
      "start": 1238.96,
      "duration": 3.68,
      "text": "It's something like that, because the \ncortex is famously very plastic as well. "
    },
    {
      "start": 1242.64,
      "duration": 5.44,
      "text": "You can rewire parts of brains.\nThere were the slightly gruesome experiments  "
    },
    {
      "start": 1248.08,
      "duration": 6.08,
      "text": "with rewiring the visual cortex to the auditory \ncortex, and this animal learned fine, et cetera. "
    },
    {
      "start": 1254.16,
      "duration": 4.4,
      "text": "So I think that this is cortical tissue.\nI think when we're doing reasoning and  "
    },
    {
      "start": 1258.56,
      "duration": 5.44,
      "text": "planning inside the neural networks, doing \nreasoning traces for thinking models,  "
    },
    {
      "start": 1264,
      "duration": 7.6,
      "text": "that's kind of like the prefrontal cortex.\nMaybe those are like little checkmarks,  "
    },
    {
      "start": 1271.6,
      "duration": 3.92,
      "text": "but I still think there are many brain \nparts and nuclei that are not explored. "
    },
    {
      "start": 1275.52,
      "duration": 2.96,
      "text": "For example, there's a basal ganglia doing a \nbit of reinforcement learning when we fine-tune  "
    },
    {
      "start": 1278.48,
      "duration": 5.12,
      "text": "the models on reinforcement learning. But where's \nthe hippocampus? Not obvious what that would be. "
    },
    {
      "start": 1283.6,
      "duration": 2.8,
      "text": "Some parts are probably not important.\nMaybe the cerebellum is not important  "
    },
    {
      "start": 1286.4,
      "duration": 2.72,
      "text": "to cognition, its thoughts, so \nmaybe we can skip some of it. "
    },
    {
      "start": 1289.12,
      "duration": 3.6,
      "text": "But I still think there's, for example, the \namygdala, all the emotions and instincts. "
    },
    {
      "start": 1293.44,
      "duration": 3.36,
      "text": "There's probably a bunch of other nuclei \nin the brain that are very ancient that  "
    },
    {
      "start": 1296.8,
      "duration": 4.48,
      "text": "I don't think we've really replicated.\nI don't know that we should be pursuing the  "
    },
    {
      "start": 1301.28,
      "duration": 4.24,
      "text": "building of an analog of a human brain.\nI'm an engineer mostly at heart. "
    },
    {
      "start": 1308.16,
      "duration": 4,
      "text": "Maybe another way to answer the question is that \nyou're not going to hire this thing as an intern. "
    },
    {
      "start": 1312.16,
      "duration": 3.36,
      "text": "It's missing a lot of it because it comes with \na lot of these cognitive deficits that we all  "
    },
    {
      "start": 1315.52,
      "duration": 4.88,
      "text": "intuitively feel when we talk to the models.\nSo it's not fully there yet. "
    },
    {
      "start": 1320.4,
      "duration": 4.08,
      "text": "You can look at it as not all the \nbrain parts are checked off yet. "
    },
    {
      "start": 1324.48,
      "duration": 5.6,
      "text": "This is maybe relevant to the question of thinking \nabout how fast these issues will be solved. "
    },
    {
      "start": 1330.8,
      "duration": 2.48,
      "text": "Sometimes people will say \nabout continual learning,  "
    },
    {
      "start": 1333.28,
      "duration": 6.64,
      "text": "\"Look, you could easily replicate this capability.\nJust as in-context learning emerged spontaneously  "
    },
    {
      "start": 1339.92,
      "duration": 5.04,
      "text": "as a result of pre-training, continual \nlearning over longer horizons will emerge  "
    },
    {
      "start": 1344.96,
      "duration": 5.76,
      "text": "spontaneously if the model is incentivized to \nrecollect information over longer horizons,  "
    },
    {
      "start": 1350.72,
      "duration": 8.96,
      "text": "or horizons longer than one session.\"\nSo if there's some outer loop RL which has  "
    },
    {
      "start": 1359.68,
      "duration": 6.4,
      "text": "many sessions within that outer loop, then this \ncontinual learning where it fine-tunes itself,  "
    },
    {
      "start": 1366.08,
      "duration": 3.44,
      "text": "or it writes to an external memory or \nsomething, will just emerge spontaneously. "
    },
    {
      "start": 1369.52,
      "duration": 4.24,
      "text": "Do you think things like that are plausible?\nI just don't have a prior over  "
    },
    {
      "start": 1373.76,
      "duration": 1.68,
      "text": "how plausible that is.\nHow likely is that to happen? "
    },
    {
      "start": 1375.44,
      "duration": 4.16,
      "text": "I don't know that I fully resonate with that.\nThese models, when you boot them up and they have  "
    },
    {
      "start": 1379.6,
      "duration": 3.44,
      "text": "zero tokens in the window, they're always \nrestarting from scratch where they were. "
    },
    {
      "start": 1383.04,
      "duration": 3.36,
      "text": "So I don't know in that \nworldview what it looks like. "
    },
    {
      "start": 1389.36,
      "duration": 4.24,
      "text": "Maybe making some analogies to humans—just because \nI think it's roughly concrete and interesting to  "
    },
    {
      "start": 1393.6,
      "duration": 3.6,
      "text": "think through—I feel like when I'm awake, I'm \nbuilding up a context window of stuff that's  "
    },
    {
      "start": 1397.2,
      "duration": 2.08,
      "text": "happening during the day.\nBut when I go to sleep,  "
    },
    {
      "start": 1399.28,
      "duration": 3.84,
      "text": "something magical happens where I don't \nthink that context window stays around. "
    },
    {
      "start": 1403.84,
      "duration": 3.28,
      "text": "There's some process of distillation \ninto the weights of my brain. "
    },
    {
      "start": 1407.84,
      "duration": 2.88,
      "text": "This happens during sleep and all this stuff.\nWe don't have an equivalent  "
    },
    {
      "start": 1410.72,
      "duration": 4.96,
      "text": "of that in large language models.\nThat's to me more adjacent to when you talk  "
    },
    {
      "start": 1415.68,
      "duration": 4.48,
      "text": "about continual learning and so on as absent.\nThese models don't really have a distillation  "
    },
    {
      "start": 1420.16,
      "duration": 7.76,
      "text": "phase of taking what happened, analyzing it \nobsessively, thinking through it, doing some  "
    },
    {
      "start": 1427.92,
      "duration": 4,
      "text": "synthetic data generation process and distilling \nit back into the weights, and maybe having  "
    },
    {
      "start": 1431.92,
      "duration": 9.28,
      "text": "a specific neural net per person. Maybe it's \na LoRA. It's not a full-weight neural network. "
    },
    {
      "start": 1441.2,
      "duration": 3.84,
      "text": "It's just some small sparse subset \nof the weights that are changed. "
    },
    {
      "start": 1445.04,
      "duration": 5.52,
      "text": "But we do want to create ways of creating \nthese individuals that have very long context. "
    },
    {
      "start": 1450.56,
      "duration": 4.32,
      "text": "It's not only remaining in the context window \nbecause the context windows grow very, very long. "
    },
    {
      "start": 1454.88,
      "duration": 2.8,
      "text": "Maybe we have some very elaborate, \nsparse attention over it. "
    },
    {
      "start": 1457.68,
      "duration": 4.48,
      "text": "But I still think that humans obviously have some \nprocess for distilling some of that knowledge  "
    },
    {
      "start": 1462.16,
      "duration": 5.28,
      "text": "into the weights. We're missing it. I do also \nthink that humans have some very elaborate,  "
    },
    {
      "start": 1467.44,
      "duration": 5.6,
      "text": "sparse attention scheme, which I think \nwe're starting to see some early hints of. "
    },
    {
      "start": 1473.04,
      "duration": 5.44,
      "text": "DeepSeek v3.2 just came out and I saw that they \nhave sparse attention as an example, and this is  "
    },
    {
      "start": 1478.48,
      "duration": 5.68,
      "text": "one way to have very, very long context windows.\nSo I feel like we are redoing a lot of the  "
    },
    {
      "start": 1484.16,
      "duration": 3.12,
      "text": "cognitive tricks that evolution came up \nwith through a very different process. "
    },
    {
      "start": 1487.28,
      "duration": 2.96,
      "text": "But we're going to converge on a \nsimilar architecture cognitively. "
    },
    {
      "start": 1491.04,
      "duration": 4.16,
      "text": "In 10 years, do you think it'll still be something \nlike a transformer, but with much more modified  "
    },
    {
      "start": 1495.2,
      "duration": 4.56,
      "text": "attention and more sparse MLPs and so forth?\nThe way I like to think about it is  "
    },
    {
      "start": 1500.56,
      "duration": 4.64,
      "text": "translation invariance in time.\nSo 10 years ago, where were we? 2015. "
    },
    {
      "start": 1505.2,
      "duration": 4.16,
      "text": "In 2015, we had convolutional neural networks \nprimarily, residual networks just came out. "
    },
    {
      "start": 1510.32,
      "duration": 3.68,
      "text": "So remarkably similar, I guess, but quite a \nbit different still. The transformer was not  "
    },
    {
      "start": 1514,
      "duration": 7.76,
      "text": "around. All these more modern tweaks \non the transformer were not around. "
    },
    {
      "start": 1521.76,
      "duration": 5.76,
      "text": "Maybe some of the things that we can bet on, I \nthink in 10 years by translational equivariance,  "
    },
    {
      "start": 1527.52,
      "duration": 3.68,
      "text": "is that we're still training giant neural \nnetworks with a forward backward pass and  "
    },
    {
      "start": 1531.2,
      "duration": 5.2,
      "text": "update through gradient descent, \nbut maybe it looks a bit different,  "
    },
    {
      "start": 1536.4,
      "duration": 5.68,
      "text": "and it's just that everything is much bigger.\nRecently I went back all the way to 1989 which  "
    },
    {
      "start": 1542.08,
      "duration": 6.72,
      "text": "was a fun exercise for me, a few years ago, \nbecause I was reproducing Yann LeCun's 1989  "
    },
    {
      "start": 1548.8,
      "duration": 3.92,
      "text": "convolutional network, which was the first neural \nnetwork I'm aware of trained via gradient descent,  "
    },
    {
      "start": 1552.72,
      "duration": 4.56,
      "text": "like modern neural network trained \ngradient descent on digit recognition. "
    },
    {
      "start": 1557.28,
      "duration": 2.72,
      "text": "I was just interested in \nhow I could modernize this. "
    },
    {
      "start": 1560,
      "duration": 1.84,
      "text": "How much of this is algorithms?\nHow much of this is data? "
    },
    {
      "start": 1561.84,
      "duration": 5.12,
      "text": "How much of this progress is compute and systems?\nI was able to very quickly halve the learning  "
    },
    {
      "start": 1566.96,
      "duration": 6.24,
      "text": "just by time traveling by 33 years.\nSo if I time travel by algorithms 33 years,  "
    },
    {
      "start": 1573.2,
      "duration": 4.8,
      "text": "I could adjust what Yann LeCun did \nin 1989, and I could halve the error. "
    },
    {
      "start": 1578,
      "duration": 3.36,
      "text": "But to get further gains, I \nhad to add a lot more data,  "
    },
    {
      "start": 1581.36,
      "duration": 4.56,
      "text": "I had to 10x the training set, and then I \nhad to add more computational optimizations. "
    },
    {
      "start": 1585.92,
      "duration": 4.56,
      "text": "I had to train for much longer with dropout \nand other regularization techniques. "
    },
    {
      "start": 1590.48,
      "duration": 3.36,
      "text": "So all these things have \nto improve simultaneously. "
    },
    {
      "start": 1595.12,
      "duration": 2.16,
      "text": "We're probably going to have a lot more \ndata, we're probably going to have a lot  "
    },
    {
      "start": 1597.28,
      "duration": 3.44,
      "text": "better hardware, probably going to have a lot \nbetter kernels and software, we're probably  "
    },
    {
      "start": 1600.72,
      "duration": 2.64,
      "text": "going to have better algorithms.\nAll of those, it's almost like  "
    },
    {
      "start": 1603.36,
      "duration": 4.96,
      "text": "no one of them is winning too much.\nAll of them are surprisingly equal. "
    },
    {
      "start": 1608.88,
      "duration": 5.92,
      "text": "This has been the trend for a while.\nSo to answer your question, I expect differences  "
    },
    {
      "start": 1614.8,
      "duration": 3.76,
      "text": "algorithmically to what's happening today.\nBut I do also expect that some of the  "
    },
    {
      "start": 1618.56,
      "duration": 3.04,
      "text": "things that have stuck around for a very \nlong time will probably still be there. "
    },
    {
      "start": 1621.6,
      "duration": 3.44,
      "text": "It's probably still a giant neural network trained \nwith gradient descent. That would be my guess. "
    },
    {
      "start": 1625.04,
      "duration": 8.8,
      "text": "It's surprising that all of those things \ntogether only halved the error, 30 years  "
    },
    {
      "start": 1633.84,
      "duration": 4.16,
      "text": "of progress…. Maybe half is a lot. Because if \nyou halve the error, that actually means that… "
    },
    {
      "start": 1638,
      "duration": 6.24,
      "text": "Half is a lot. But I guess what was shocking to me \nis everything needs to improve across the board:  "
    },
    {
      "start": 1644.24,
      "duration": 4.24,
      "text": "architecture, optimizer, loss function.\nIt also has improved across the board forever. "
    },
    {
      "start": 1648.48,
      "duration": 2.96,
      "text": "So I expect all those \nchanges to be alive and well. "
    },
    {
      "start": 1651.44,
      "duration": 2.88,
      "text": "Yeah. I was about to ask you a very \nsimilar question about nanochat. "
    },
    {
      "start": 1655.04,
      "duration": 5.04,
      "text": "Since you just coded it up recently, \nevery single step in the process of  "
    },
    {
      "start": 1660.08,
      "duration": 5.28,
      "text": "building a chatbot is fresh in your RAM.\nI'm curious if you had similar thoughts about,  "
    },
    {
      "start": 1666,
      "duration": 6,
      "text": "\"Oh, there was no one thing that was \nrelevant to going from GPT-2 to nanochat.\" "
    },
    {
      "start": 1672,
      "duration": 3.92,
      "text": "What are some surprising \ntakeaways from the experience? "
    },
    {
      "start": 1675.92,
      "duration": 3.12,
      "text": "Of building nanochat? So nanochat \nis a repository I released. "
    },
    {
      "start": 1679.04,
      "duration": 2.96,
      "text": "Was it yesterday or the day \nbefore? I can't remember. "
    },
    {
      "start": 1683.12,
      "duration": 2.96,
      "text": "We can see the sleep \ndeprivation that went into the… "
    },
    {
      "start": 1689.04,
      "duration": 3.84,
      "text": "It's trying to be the simplest complete \nrepository that covers the whole pipeline  "
    },
    {
      "start": 1692.88,
      "duration": 5.52,
      "text": "end-to-end of building a ChatGPT clone.\nSo you have all of the steps, not just  "
    },
    {
      "start": 1698.4,
      "duration": 3.76,
      "text": "any individual step, which is a bunch.\nI worked on all the individual steps  "
    },
    {
      "start": 1702.16,
      "duration": 3.76,
      "text": "in the past and released small pieces \nof code that show you how that's done  "
    },
    {
      "start": 1705.92,
      "duration": 5.44,
      "text": "in an algorithmic sense, in simple code.\nBut this handles the entire pipeline. "
    },
    {
      "start": 1712.4,
      "duration": 3.76,
      "text": "In terms of learning, I don't \nknow that I necessarily found  "
    },
    {
      "start": 1716.16,
      "duration": 4.56,
      "text": "something that I learned from it.\nI already had in my mind how you build it. "
    },
    {
      "start": 1720.72,
      "duration": 7.76,
      "text": "This is just the process of mechanically building \nit and making it clean enough so that people can  "
    },
    {
      "start": 1728.48,
      "duration": 4.64,
      "text": "learn from it and that they find it useful.\nWhat is the best way for somebody  "
    },
    {
      "start": 1733.12,
      "duration": 1.84,
      "text": "to learn from it?\nIs it to just delete  "
    },
    {
      "start": 1734.96,
      "duration": 3.36,
      "text": "all the code and try to reimplement from \nscratch, try to add modifications to it? "
    },
    {
      "start": 1739.52,
      "duration": 3.84,
      "text": "That's a great question. Basically \nit's about 8,000 lines of code  "
    },
    {
      "start": 1743.36,
      "duration": 3.68,
      "text": "that takes you through the entire pipeline.\nI would probably put it on the right monitor. "
    },
    {
      "start": 1747.04,
      "duration": 4.4,
      "text": "If you have two monitors, you put it on the right.\nYou want to build it from scratch,  "
    },
    {
      "start": 1751.44,
      "duration": 3.28,
      "text": "you build it from the start.\nYou're not allowed to copy-paste, you're allowed  "
    },
    {
      "start": 1754.72,
      "duration": 2.96,
      "text": "to reference, you're not allowed to copy-paste.\nMaybe that's how I would do it. "
    },
    {
      "start": 1758.24,
      "duration": 3.44,
      "text": "But I also think the repository \nby itself is a pretty large beast. "
    },
    {
      "start": 1763.52,
      "duration": 4.08,
      "text": "When you write this code, you don't go from top \nto bottom, you go from chunks and you grow the  "
    },
    {
      "start": 1767.6,
      "duration": 3.84,
      "text": "chunks, and that information is absent.\nYou wouldn't know where to start. "
    },
    {
      "start": 1771.44,
      "duration": 4.32,
      "text": "So it's not just a final repository that's \nneeded, it's the building of the repository,  "
    },
    {
      "start": 1775.76,
      "duration": 4.64,
      "text": "which is a complicated chunk-growing process.\nSo that part is not there yet. "
    },
    {
      "start": 1780.4,
      "duration": 6.48,
      "text": "I would love to add that probably later this week.\nIt's probably a video or something like that. "
    },
    {
      "start": 1789.2,
      "duration": 4.24,
      "text": "Roughly speaking, that's what I would try to do.\nBuild the stuff yourself, but don't allow  "
    },
    {
      "start": 1793.44,
      "duration": 2.24,
      "text": "yourself copy-paste.\nI do think that  "
    },
    {
      "start": 1795.68,
      "duration": 4.24,
      "text": "there's two types of knowledge, almost.\nThere's the high-level surface knowledge, but when  "
    },
    {
      "start": 1799.92,
      "duration": 4.24,
      "text": "you build something from scratch, you're forced to \ncome to terms with what you don't understand and  "
    },
    {
      "start": 1804.16,
      "duration": 4,
      "text": "you don't know that you don't understand it.\nIt always leads to a deeper understanding. "
    },
    {
      "start": 1809.68,
      "duration": 4.08,
      "text": "It's the only way to build.\nIf I can't build it, I don't understand it. "
    },
    {
      "start": 1813.76,
      "duration": 5.68,
      "text": "That’s a Feynman quote, I believe.\nI 100% have always believed this very  "
    },
    {
      "start": 1819.44,
      "duration": 4.24,
      "text": "strongly, because there are all these micro \nthings that are just not properly arranged  "
    },
    {
      "start": 1823.68,
      "duration": 2,
      "text": "and you don't really have the knowledge.\nYou just think you have the knowledge. "
    },
    {
      "start": 1825.68,
      "duration": 3.28,
      "text": "So don't write blog posts, don't \ndo slides, don't do any of that. "
    },
    {
      "start": 1828.96,
      "duration": 2.8,
      "text": "Build the code, arrange it, get it to work.\nIt's the only way to go. Otherwise,  "
    },
    {
      "start": 1831.76,
      "duration": 3.68,
      "text": "you're missing knowledge.\nYou tweeted out that coding models  "
    },
    {
      "start": 1835.44,
      "duration": 5.68,
      "text": "were of very little help to you in assembling \nthis repository. I'm curious why that was. "
    },
    {
      "start": 1843.44,
      "duration": 3.12,
      "text": "I guess I built the repository over \na period of a bit more than a month. "
    },
    {
      "start": 1846.56,
      "duration": 3.92,
      "text": "I would say there are three major classes \nof how people interact with code right now. "
    },
    {
      "start": 1850.48,
      "duration": 4.32,
      "text": "Some people completely reject all of LLMs \nand they are just writing by scratch. "
    },
    {
      "start": 1854.8,
      "duration": 2.16,
      "text": "This is probably not the \nright thing to do anymore. "
    },
    {
      "start": 1858.16,
      "duration": 4.24,
      "text": "The intermediate part, which is where I am, is \nyou still write a lot of things from scratch,  "
    },
    {
      "start": 1862.4,
      "duration": 4.48,
      "text": "but you use the autocomplete that's \navailable now from these models. "
    },
    {
      "start": 1866.88,
      "duration": 3.36,
      "text": "So when you start writing out a little \npiece of it, it will autocomplete for  "
    },
    {
      "start": 1870.24,
      "duration": 2.32,
      "text": "you and you can just tap through.\nMost of the time it's correct,  "
    },
    {
      "start": 1872.56,
      "duration": 2.96,
      "text": "sometimes it's not, and you edit it.\nBut you're still very much the  "
    },
    {
      "start": 1876.64,
      "duration": 5.04,
      "text": "architect of what you're writing.\nThen there's the vibe coding: \"Hi,  "
    },
    {
      "start": 1881.68,
      "duration": 6.4,
      "text": "please implement this or that,\" enter, and then \nlet the model do it. That's the agents. I do feel  "
    },
    {
      "start": 1888.08,
      "duration": 5.12,
      "text": "like the agents work in very specific settings, \nand I would use them in specific settings. "
    },
    {
      "start": 1893.2,
      "duration": 4.08,
      "text": "But these are all tools available to you \nand you have to learn what they're good at,  "
    },
    {
      "start": 1897.28,
      "duration": 3.28,
      "text": "what they're not good at, and when to use them.\nSo the agents are pretty good, for example,  "
    },
    {
      "start": 1900.56,
      "duration": 4,
      "text": "if you're doing boilerplate stuff.\nBoilerplate code that's just  "
    },
    {
      "start": 1904.56,
      "duration": 3.76,
      "text": "copy-paste stuff, they're very good at that.\nThey're very good at stuff that occurs very often  "
    },
    {
      "start": 1908.32,
      "duration": 5.28,
      "text": "on the Internet because there are lots of examples \nof it in the training sets of these models. "
    },
    {
      "start": 1915.2,
      "duration": 3.04,
      "text": "There are features of things where \nthe models will do very well. "
    },
    {
      "start": 1918.24,
      "duration": 4.8,
      "text": "I would say nanochat is not an example of \nthose because it's a fairly unique repository. "
    },
    {
      "start": 1923.04,
      "duration": 5.84,
      "text": "There's not that much code in the way that \nI've structured it. It's not boilerplate code.  "
    },
    {
      "start": 1928.88,
      "duration": 4.32,
      "text": "It's intellectually intense code almost, and \neverything has to be very precisely arranged. "
    },
    {
      "start": 1933.2,
      "duration": 8,
      "text": "The models have so many cognitive deficits.\nOne example, they kept misunderstanding the code  "
    },
    {
      "start": 1942.48,
      "duration": 3.84,
      "text": "because they have too much memory from \nall the typical ways of doing things on  "
    },
    {
      "start": 1946.32,
      "duration": 4.72,
      "text": "the Internet that I just wasn't adopting.\nThe models, for example—I don't know if I  "
    },
    {
      "start": 1951.04,
      "duration": 5.92,
      "text": "want to get into the full details—but they kept \nthinking I'm writing normal code, and I'm not. "
    },
    {
      "start": 1956.96,
      "duration": 4.8,
      "text": "Maybe one example?\nYou have eight GPUs  "
    },
    {
      "start": 1961.76,
      "duration": 2.8,
      "text": "that are all doing forward, backwards.\nThe way to synchronize gradients between  "
    },
    {
      "start": 1964.56,
      "duration": 4.16,
      "text": "them is to use a Distributed Data Parallel \ncontainer of PyTorch, which automatically  "
    },
    {
      "start": 1969.52,
      "duration": 2.88,
      "text": "as you're doing the backward, it will start \ncommunicating and synchronizing gradients. "
    },
    {
      "start": 1972.4,
      "duration": 4.24,
      "text": "I didn't use DDP because I didn't want \nto use it, because it's not necessary. "
    },
    {
      "start": 1976.64,
      "duration": 5.84,
      "text": "I threw it out and wrote my own synchronization \nroutine that's inside the step of the optimizer. "
    },
    {
      "start": 1982.48,
      "duration": 4.24,
      "text": "The models were trying to get me to \nuse the DDP container. They were very  "
    },
    {
      "start": 1986.72,
      "duration": 4.56,
      "text": "concerned. This gets way too technical, \nbut I wasn't using that container because  "
    },
    {
      "start": 1991.28,
      "duration": 2.72,
      "text": "I don't need it and I have a custom \nimplementation of something like it. "
    },
    {
      "start": 1994,
      "duration": 1.76,
      "text": "They just couldn't internalize \nthat you had your own. "
    },
    {
      "start": 1996.64,
      "duration": 6.56,
      "text": "They couldn't get past that. They kept trying to \nmess up the style. They're way too over-defensive.  "
    },
    {
      "start": 2003.2,
      "duration": 3.92,
      "text": "They make all these try-catch statements.\nThey keep trying to make a production code base,  "
    },
    {
      "start": 2007.12,
      "duration": 2.8,
      "text": "and I have a bunch of assumptions \nin my code, and it's okay. "
    },
    {
      "start": 2011.92,
      "duration": 4.64,
      "text": "I don't need all this extra stuff in there.\nSo I feel like they're bloating the code base,  "
    },
    {
      "start": 2016.56,
      "duration": 2.32,
      "text": "bloating the complexity, they keep \nmisunderstanding, they're using  "
    },
    {
      "start": 2018.88,
      "duration": 7.92,
      "text": "deprecated APIs a bunch of times. It's a total \nmess. It's just not net useful. I can go in,  "
    },
    {
      "start": 2026.8,
      "duration": 4.16,
      "text": "I can clean it up, but it's not net useful.\nI also feel like it's annoying to have to  "
    },
    {
      "start": 2030.96,
      "duration": 3.2,
      "text": "type out what I want in English \nbecause it's too much typing. "
    },
    {
      "start": 2034.16,
      "duration": 4.48,
      "text": "If I just navigate to the part of the code that I \nwant, and I go where I know the code has to appear  "
    },
    {
      "start": 2038.64,
      "duration": 3.68,
      "text": "and I start typing out the first few letters, \nautocomplete gets it and just gives you the code. "
    },
    {
      "start": 2043.84,
      "duration": 3.2,
      "text": "This is a very high information \nbandwidth to specify what you want. "
    },
    {
      "start": 2047.04,
      "duration": 3.04,
      "text": "You point to the code where you want \nit, you type out the first few pieces,  "
    },
    {
      "start": 2050.08,
      "duration": 5.36,
      "text": "and the model will complete it.\nSo what I mean is, these models  "
    },
    {
      "start": 2055.44,
      "duration": 6.8,
      "text": "are good in certain parts of the stack.\nThere are two examples where I use the  "
    },
    {
      "start": 2062.24,
      "duration": 4.32,
      "text": "models that I think are illustrative.\nOne was when I generated the report. "
    },
    {
      "start": 2066.56,
      "duration": 3.68,
      "text": "That's more boilerplate-y, so I \npartially vibe-coded some of that stuff. "
    },
    {
      "start": 2070.24,
      "duration": 4.08,
      "text": "That was fine because it's not \nmission-critical stuff, and it works fine. "
    },
    {
      "start": 2074.32,
      "duration": 3.6,
      "text": "The other part is when I was \nrewriting the tokenizer in Rust. "
    },
    {
      "start": 2077.92,
      "duration": 3.2,
      "text": "I'm not as good at Rust \nbecause I'm fairly new to Rust. "
    },
    {
      "start": 2081.12,
      "duration": 4.72,
      "text": "So there's a bit of vibe coding going on \nwhen I was writing some of the Rust code. "
    },
    {
      "start": 2085.84,
      "duration": 3.36,
      "text": "But I had a Python implementation that I \nfully understand, and I'm just making sure  "
    },
    {
      "start": 2089.2,
      "duration": 3.92,
      "text": "I'm making a more efficient version of it, and \nI have tests so I feel safer doing that stuff. "
    },
    {
      "start": 2096.24,
      "duration": 6.32,
      "text": "They increase accessibility to languages or \nparadigms that you might not be as familiar with. "
    },
    {
      "start": 2102.56,
      "duration": 4,
      "text": "I think they're very helpful there as well.\nThere's a ton of Rust code out there,  "
    },
    {
      "start": 2106.56,
      "duration": 2.72,
      "text": "the models are pretty good at it.\nI happen to not know that much about it,  "
    },
    {
      "start": 2109.28,
      "duration": 3.68,
      "text": "so the models are very useful there.\nThe reason this question is so interesting  "
    },
    {
      "start": 2112.96,
      "duration": 6.24,
      "text": "is because the main story people \nhave about AI exploding and getting  "
    },
    {
      "start": 2119.2,
      "duration": 5.6,
      "text": "to superintelligence pretty rapidly is AI \nautomating AI engineering and AI research. "
    },
    {
      "start": 2125.52,
      "duration": 2.235,
      "text": "They'll look at the fact that you can have \nClaude Code and make entire applications,  "
    },
    {
      "start": 2127.755,
      "duration": 5.045,
      "text": "CRUD applications, from scratch and think, \"If \nyou had this same capability inside of OpenAI  "
    },
    {
      "start": 2132.8,
      "duration": 5.68,
      "text": "and DeepMind and everything, just imagine \na thousand of you or a million of you in  "
    },
    {
      "start": 2138.48,
      "duration": 5.04,
      "text": "parallel, finding little architectural tweaks.\"\nIt's quite interesting to hear you say that this  "
    },
    {
      "start": 2143.52,
      "duration": 4.96,
      "text": "is the thing they're asymmetrically worse at.\nIt's quite relevant to forecasting whether  "
    },
    {
      "start": 2148.48,
      "duration": 4.48,
      "text": "the AI 2027-type explosion is \nlikely to happen anytime soon. "
    },
    {
      "start": 2153.52,
      "duration": 4.88,
      "text": "That's a good way of putting it, and you're \ngetting at why my timelines are a bit longer.  "
    },
    {
      "start": 2158.4,
      "duration": 5.68,
      "text": "You're right. They're not very good at code \nthat has never been written before, maybe it's  "
    },
    {
      "start": 2164.08,
      "duration": 3.68,
      "text": "one way to put it, which is what we're trying \nto achieve when we're building these models. "
    },
    {
      "start": 2167.76,
      "duration": 7.04,
      "text": "Very naive question, but the architectural \ntweaks that you're adding to nanochat,  "
    },
    {
      "start": 2174.8,
      "duration": 2.56,
      "text": "they're in a paper somewhere, right?\nThey might even be in a repo somewhere. "
    },
    {
      "start": 2180.32,
      "duration": 4.96,
      "text": "Is it surprising that they aren't able to \nintegrate that into whenever you're like,  "
    },
    {
      "start": 2185.28,
      "duration": 4.48,
      "text": "\"Add RoPE embeddings\" or something, \nthey do that in the wrong way? "
    },
    {
      "start": 2189.76,
      "duration": 4.56,
      "text": "It's tough. They know, but they don't fully know.\nThey don't know how to fully integrate it into  "
    },
    {
      "start": 2194.32,
      "duration": 3.12,
      "text": "the repo and your style and your code and \nyour place, and some of the custom things  "
    },
    {
      "start": 2197.44,
      "duration": 3.76,
      "text": "that you're doing and how it fits with \nall the assumptions of the repository. "
    },
    {
      "start": 2202.88,
      "duration": 4.08,
      "text": "They do have some knowledge, but they \nhaven't gotten to the place where they  "
    },
    {
      "start": 2206.96,
      "duration": 6,
      "text": "can integrate it and make sense of it.\nA lot of the stuff continues to improve. "
    },
    {
      "start": 2214.08,
      "duration": 3.2,
      "text": "Currently, the state-of-the-art \nmodel that I go to is the GPT-5 Pro,  "
    },
    {
      "start": 2217.92,
      "duration": 3.36,
      "text": "and that's a very powerful model.\nIf I have 20 minutes,  "
    },
    {
      "start": 2221.28,
      "duration": 4.72,
      "text": "I will copy-paste my entire repo and I go to \nGPT-5 Pro, the oracle, for some questions. "
    },
    {
      "start": 2226,
      "duration": 3.68,
      "text": "Often it's not too bad and surprisingly \ngood compared to what existed a year ago. "
    },
    {
      "start": 2231.76,
      "duration": 9.12,
      "text": "Overall, the models are not there.\nI feel like the industry is making too  "
    },
    {
      "start": 2240.88,
      "duration": 6.24,
      "text": "big of a jump and is trying to pretend like this \nis amazing, and it's not. It's slop. They're not  "
    },
    {
      "start": 2247.12,
      "duration": 2.72,
      "text": "coming to terms with it, and maybe they're \ntrying to fundraise or something like that. "
    },
    {
      "start": 2249.84,
      "duration": 3.92,
      "text": "I'm not sure what's going on, but we're \nat this intermediate stage. The models are  "
    },
    {
      "start": 2253.76,
      "duration": 4.56,
      "text": "amazing. They still need a lot of work.\nFor now, autocomplete is my sweet spot. "
    },
    {
      "start": 2258.32,
      "duration": 2.88,
      "text": "But sometimes, for some types of \ncode, I will go to an LLM agent. "
    },
    {
      "start": 2263.04,
      "duration": 5.28,
      "text": "Here's another reason this is really interesting.\nThrough the history of programming, there have  "
    },
    {
      "start": 2268.32,
      "duration": 6.96,
      "text": "been many productivity improvements—compilers, \nlinting, better programming languages—which  "
    },
    {
      "start": 2275.92,
      "duration": 3.6,
      "text": "have increased programmer productivity \nbut have not led to an explosion. "
    },
    {
      "start": 2280.88,
      "duration": 3.52,
      "text": "That sounds very much like the \nautocomplete tab, and this other  "
    },
    {
      "start": 2284.4,
      "duration": 4.64,
      "text": "category is just automation of the programmer.\nIt's interesting you're seeing more in the  "
    },
    {
      "start": 2289.04,
      "duration": 4.64,
      "text": "category of the historical analogies \nof better compilers or something. "
    },
    {
      "start": 2293.68,
      "duration": 5.44,
      "text": "Maybe this gets to one other thought.\nI have a hard time differentiating where  "
    },
    {
      "start": 2299.12,
      "duration": 3.92,
      "text": "AI begins and stops because I see \nAI as fundamentally an extension of  "
    },
    {
      "start": 2303.04,
      "duration": 5.6,
      "text": "computing in a pretty fundamental way.\nI see a continuum of this recursive  "
    },
    {
      "start": 2308.64,
      "duration": 6.56,
      "text": "self-improvement or speeding up programmers \nall the way from the beginning: code editors,  "
    },
    {
      "start": 2317.44,
      "duration": 6.32,
      "text": "syntax highlighting, or checking even of \nthe types, like data type checking—all  "
    },
    {
      "start": 2324.88,
      "duration": 3.28,
      "text": "these tools that we've built for \neach other. Even search engines.  "
    },
    {
      "start": 2328.16,
      "duration": 6.88,
      "text": "Why aren't search engines part of AI? Ranking \nis AI. At some point, Google, even early on,  "
    },
    {
      "start": 2335.04,
      "duration": 4.08,
      "text": "was thinking of themselves as an AI company doing \nGoogle Search engine, which is totally fair. "
    },
    {
      "start": 2339.12,
      "duration": 4.96,
      "text": "I see it as a lot more of a continuum than other \npeople do, and it's hard for me to draw the line. "
    },
    {
      "start": 2344.08,
      "duration": 3.28,
      "text": "I feel like we're now getting a much \nbetter autocomplete, and now we're also  "
    },
    {
      "start": 2347.36,
      "duration": 4.4,
      "text": "getting some agents which are these loopy \nthings, but they go off-rails sometimes. "
    },
    {
      "start": 2353.52,
      "duration": 4.56,
      "text": "What's going on is that the human is progressively \ndoing a bit less and less of the low-level stuff. "
    },
    {
      "start": 2358.08,
      "duration": 2.8,
      "text": "We're not writing the assembly \ncode because we have compilers. "
    },
    {
      "start": 2360.88,
      "duration": 2.96,
      "text": "Compilers will take my high-level \nlanguage in C and write the assembly code. "
    },
    {
      "start": 2363.84,
      "duration": 4.96,
      "text": "We're abstracting ourselves very, very slowly.\nThere's this what I call \"autonomy slider,\" where  "
    },
    {
      "start": 2368.8,
      "duration": 3.84,
      "text": "more and more stuff is automated—of the stuff that \ncan be automated at any point in time—and we're  "
    },
    {
      "start": 2372.64,
      "duration": 5.12,
      "text": "doing a bit less and less and raising ourselves \nin the layer of abstraction over the automation. "
    },
    {
      "start": 2453.92,
      "duration": 2.96,
      "text": "Let's talk about RL a bit.\nYou tweeted some very  "
    },
    {
      "start": 2456.88,
      "duration": 4,
      "text": "interesting things about this.\nConceptually, how should we think about  "
    },
    {
      "start": 2460.88,
      "duration": 6.48,
      "text": "the way that humans are able to build a rich world \nmodel just from interacting with our environment,  "
    },
    {
      "start": 2467.36,
      "duration": 6,
      "text": "and in ways that seem almost irrespective of \nthe final reward at the end of the episode? "
    },
    {
      "start": 2473.36,
      "duration": 4.48,
      "text": "If somebody is starting a business, and at \nthe end of 10 years, she finds out whether  "
    },
    {
      "start": 2477.84,
      "duration": 4.64,
      "text": "the business succeeded or failed, we say that \nshe's earned a bunch of wisdom and experience. "
    },
    {
      "start": 2482.48,
      "duration": 3.44,
      "text": "But it's not because the log probs of every \nsingle thing that happened over the last 10  "
    },
    {
      "start": 2485.92,
      "duration": 3.92,
      "text": "years are up-weighted or down-weighted.\nSomething much more deliberate and  "
    },
    {
      "start": 2489.84,
      "duration": 3.36,
      "text": "rich is happening.\nWhat is the ML analogy, and how does that  "
    },
    {
      "start": 2493.2,
      "duration": 3.44,
      "text": "compare to what we're doing with LLMs right now?\nMaybe the way I would put it is that humans don't  "
    },
    {
      "start": 2496.64,
      "duration": 4,
      "text": "use reinforcement learning, as I said.\nI think they do something different. "
    },
    {
      "start": 2502.64,
      "duration": 5.52,
      "text": "Reinforcement learning is a lot worse than I \nthink the average person thinks. Reinforcement  "
    },
    {
      "start": 2508.16,
      "duration": 4.96,
      "text": "learning is terrible. It just so happens \nthat everything that we had before it is  "
    },
    {
      "start": 2513.12,
      "duration": 5.44,
      "text": "much worse because previously we were just \nimitating people, so it has all these issues. "
    },
    {
      "start": 2519.6,
      "duration": 4.4,
      "text": "In reinforcement learning, say you're solving \na math problem, because it's very simple. "
    },
    {
      "start": 2524,
      "duration": 2.88,
      "text": "You're given a math problem and \nyou're trying to find the solution. "
    },
    {
      "start": 2528.16,
      "duration": 5.12,
      "text": "In reinforcement learning, you will \ntry lots of things in parallel first. "
    },
    {
      "start": 2533.92,
      "duration": 4.72,
      "text": "You're given a problem, you try hundreds of \ndifferent attempts. These attempts can be complex.  "
    },
    {
      "start": 2538.64,
      "duration": 3.76,
      "text": "They can be like, \"Oh, let me try this, let me try \nthat, this didn't work, that didn't work,\" etc. "
    },
    {
      "start": 2542.4,
      "duration": 3.44,
      "text": "Then maybe you get an answer.\nNow you check the back of the book and you see,  "
    },
    {
      "start": 2545.84,
      "duration": 4.24,
      "text": "\"Okay, the correct answer is this.\"\nYou can see that this one, this one,  "
    },
    {
      "start": 2550.08,
      "duration": 3.76,
      "text": "and that one got the correct answer, \nbut these other 97 of them didn't. "
    },
    {
      "start": 2553.84,
      "duration": 4,
      "text": "Literally what reinforcement learning does is it \ngoes to the ones that worked really well and every  "
    },
    {
      "start": 2557.84,
      "duration": 4.8,
      "text": "single thing you did along the way, every single \ntoken gets upweighted like, \"Do more of this.\" "
    },
    {
      "start": 2562.64,
      "duration": 3.92,
      "text": "The problem with that is people will say \nthat your estimator has high variance,  "
    },
    {
      "start": 2566.56,
      "duration": 6.88,
      "text": "but it's just noisy. It's noisy. It almost assumes \nthat every single little piece of the solution  "
    },
    {
      "start": 2573.44,
      "duration": 3.52,
      "text": "that you made that arrived at the right answer \nwas the correct thing to do, which is not true. "
    },
    {
      "start": 2576.96,
      "duration": 3.68,
      "text": "You may have gone down the wrong alleys \nuntil you arrived at the right solution. "
    },
    {
      "start": 2580.64,
      "duration": 3.36,
      "text": "Every single one of those incorrect things you \ndid, as long as you got to the correct solution,  "
    },
    {
      "start": 2584,
      "duration": 4.4,
      "text": "will be upweighted as, \"Do more of this.\" \nIt's terrible. It's noise. You've done all  "
    },
    {
      "start": 2588.4,
      "duration": 5.12,
      "text": "this work only to find, at the end, you get a \nsingle number of like, \"Oh, you did correct.\" "
    },
    {
      "start": 2594.16,
      "duration": 4,
      "text": "Based on that, you weigh that entire \ntrajectory as like, upweight or downweight. "
    },
    {
      "start": 2599.04,
      "duration": 3.36,
      "text": "The way I like to put it is you're \nsucking supervision through a straw. "
    },
    {
      "start": 2602.4,
      "duration": 2.48,
      "text": "You've done all this work that \ncould be a minute of rollout,  "
    },
    {
      "start": 2604.88,
      "duration": 4.32,
      "text": "and you're sucking the bits of supervision of the \nfinal reward signal through a straw and you're  "
    },
    {
      "start": 2613.92,
      "duration": 3.2,
      "text": "broadcasting that across the entire trajectory \nand using that to upweight or downweight that  "
    },
    {
      "start": 2617.12,
      "duration": 2.8,
      "text": "trajectory. It's just stupid and \ncrazy. A human would never do this. "
    },
    {
      "start": 2619.92,
      "duration": 3.2,
      "text": "Number one, a human would \nnever do hundreds of rollouts. "
    },
    {
      "start": 2623.12,
      "duration": 4.48,
      "text": "Number two, when a person finds a solution, \nthey will have a pretty complicated process  "
    },
    {
      "start": 2627.6,
      "duration": 4.24,
      "text": "of review of, \"Okay, I think these parts I \ndid well, these parts I did not do that well. "
    },
    {
      "start": 2631.84,
      "duration": 3.6,
      "text": "I should probably do this or that.\" \nThey think through things. There's  "
    },
    {
      "start": 2635.44,
      "duration": 4.8,
      "text": "nothing in current LLMs that does this. \nThere's no equivalent of it. But I do see  "
    },
    {
      "start": 2640.24,
      "duration": 3.68,
      "text": "papers popping out that are trying to do this \nbecause it's obvious to everyone in the field. "
    },
    {
      "start": 2645.28,
      "duration": 3.84,
      "text": "The first imitation learning, by the way, was \nextremely surprising and miraculous and amazing,  "
    },
    {
      "start": 2649.12,
      "duration": 4.96,
      "text": "that we can fine-tune by imitation on humans. \nThat was incredible. Because in the beginning,  "
    },
    {
      "start": 2654.08,
      "duration": 4.16,
      "text": "all we had was base models. Base models \nare autocomplete. It wasn't obvious to  "
    },
    {
      "start": 2658.24,
      "duration": 5.12,
      "text": "me at the time, and I had to learn this.\nThe paper that blew my mind was InstructGPT,  "
    },
    {
      "start": 2664.16,
      "duration": 3.84,
      "text": "because it pointed out that you can take \nthe pretrained model, which is autocomplete,  "
    },
    {
      "start": 2668,
      "duration": 4.56,
      "text": "and if you just fine-tune it on text that looks \nlike conversations, the model will very rapidly  "
    },
    {
      "start": 2672.56,
      "duration": 4.24,
      "text": "adapt to become very conversational, and it \nkeeps all the knowledge from pre-training. "
    },
    {
      "start": 2676.8,
      "duration": 4.32,
      "text": "This blew my mind because I didn't understand \nthat stylistically, it can adjust so quickly  "
    },
    {
      "start": 2681.12,
      "duration": 5.36,
      "text": "and become an assistant to a user through just \na few loops of fine-tuning on that kind of data. "
    },
    {
      "start": 2686.48,
      "duration": 5.36,
      "text": "It was very miraculous to me that that worked. \nSo incredible. That was two to three years of  "
    },
    {
      "start": 2691.84,
      "duration": 6.4,
      "text": "work. Now came RL. And RL allows you to do a bit \nbetter than just imitation learning because you  "
    },
    {
      "start": 2698.24,
      "duration": 3.92,
      "text": "can have these reward functions and you \ncan hill-climb on the reward functions. "
    },
    {
      "start": 2702.72,
      "duration": 3.44,
      "text": "Some problems have just correct answers, you \ncan hill-climb on that without getting expert  "
    },
    {
      "start": 2706.16,
      "duration": 4.8,
      "text": "trajectories to imitate. So that's amazing. The \nmodel can also discover solutions that a human  "
    },
    {
      "start": 2710.96,
      "duration": 8.24,
      "text": "might never come up with. This is incredible. \nYet, it's still stupid. We need more. I saw a  "
    },
    {
      "start": 2719.2,
      "duration": 6,
      "text": "paper from Google yesterday that tried to \nhave this reflect & review idea in mind. "
    },
    {
      "start": 2725.2,
      "duration": 5.12,
      "text": "Was it the memory bank paper or something? I don't \nknow. I've seen a few papers along these lines. "
    },
    {
      "start": 2730.32,
      "duration": 7.04,
      "text": "So I expect there to be some major update to how \nwe do algorithms for LLMs coming in that realm. "
    },
    {
      "start": 2737.36,
      "duration": 5.12,
      "text": "I think we need three or four or \nfive more, something like that. "
    },
    {
      "start": 2742.48,
      "duration": 5.44,
      "text": "You're so good at coming up with evocative \nphrases. \"Sucking supervision through a  "
    },
    {
      "start": 2747.92,
      "duration": 8.4,
      "text": "straw.\" It's so good. You're saying the problem \nwith outcome-based reward is that you have this  "
    },
    {
      "start": 2756.32,
      "duration": 5.2,
      "text": "huge trajectory, and then at the end, you're \ntrying to learn every single possible thing  "
    },
    {
      "start": 2761.52,
      "duration": 3.92,
      "text": "about what you should do and what you should \nlearn about the world from that one final bit. "
    },
    {
      "start": 2767.28,
      "duration": 4.24,
      "text": "Given the fact that this is obvious, why hasn't \nprocess-based supervision as an alternative been  "
    },
    {
      "start": 2771.52,
      "duration": 4.24,
      "text": "a successful way to make models more capable?\nWhat has been preventing us from using  "
    },
    {
      "start": 2775.76,
      "duration": 2.24,
      "text": "this alternative paradigm?\nProcess-based supervision just  "
    },
    {
      "start": 2778,
      "duration": 3.2,
      "text": "refers to the fact that we're not going to \nhave a reward function only at the very end. "
    },
    {
      "start": 2781.2,
      "duration": 3.12,
      "text": "After you've done 10 minutes of work, I'm not \ngoing to tell you you did well or not well. "
    },
    {
      "start": 2784.32,
      "duration": 2.64,
      "text": "I'm going to tell you at every single \nstep of the way how well you're doing. "
    },
    {
      "start": 2788,
      "duration": 4.08,
      "text": "The reason we don't have that is \nit's tricky how you do that properly. "
    },
    {
      "start": 2792.08,
      "duration": 2.88,
      "text": "You have partial solutions and you \ndon't know how to assign credit. "
    },
    {
      "start": 2794.96,
      "duration": 4.32,
      "text": "So when you get the right answer, it's just \nan equality match to the answer. It’s very  "
    },
    {
      "start": 2799.28,
      "duration": 5.36,
      "text": "simple to implement. If you're doing process \nsupervision, how do you assign in an automatable  "
    },
    {
      "start": 2804.64,
      "duration": 3.2,
      "text": "way, a partial credit assignment?\nIt's not obvious how you do it. "
    },
    {
      "start": 2807.84,
      "duration": 3.12,
      "text": "Lots of labs are trying to \ndo it with these LLM judges. "
    },
    {
      "start": 2810.96,
      "duration": 2.96,
      "text": "You get LLMs to try to do it.\nYou prompt an LLM, \"Hey,  "
    },
    {
      "start": 2813.92,
      "duration": 2.8,
      "text": "look at a partial solution of a student.\nHow well do you think they're doing if the  "
    },
    {
      "start": 2816.72,
      "duration": 5.84,
      "text": "answer is this?\" and they try to tune the prompt.\nThe reason that this is tricky is quite subtle. "
    },
    {
      "start": 2822.56,
      "duration": 5.12,
      "text": "It's the fact that anytime you use an LLM to \nassign a reward, those LLMs are giant things  "
    },
    {
      "start": 2827.68,
      "duration": 3.68,
      "text": "with billions of parameters, and they're gameable.\nIf you're reinforcement learning with respect to  "
    },
    {
      "start": 2831.36,
      "duration": 3.96,
      "text": "them, you will find adversarial examples \nfor your LLM judges, almost guaranteed. "
    },
    {
      "start": 2835.32,
      "duration": 3.24,
      "text": "So you can't do this for too long.\nYou do maybe 10 steps or 20 steps, and maybe  "
    },
    {
      "start": 2838.56,
      "duration": 6.48,
      "text": "it will work, but you can't do 100 or 1,000.\nI understand it's not obvious, but basically  "
    },
    {
      "start": 2845.04,
      "duration": 5.6,
      "text": "the model will find little cracks.\nIt will find all these spurious  "
    },
    {
      "start": 2850.64,
      "duration": 4.32,
      "text": "things in the nooks and crannies of the \ngiant model and find a way to cheat it. "
    },
    {
      "start": 2854.96,
      "duration": 7.68,
      "text": "One example that's prominently in my mind, this \nwas probably public, if you're using an LLM judge  "
    },
    {
      "start": 2862.64,
      "duration": 4.64,
      "text": "for a reward, you just give it a solution from a \nstudent and ask it if the student did well or not. "
    },
    {
      "start": 2867.28,
      "duration": 2,
      "text": "We were training with \nreinforcement learning against  "
    },
    {
      "start": 2869.28,
      "duration": 6.24,
      "text": "that reward function, and it worked really well.\nThen, suddenly, the reward became extremely large. "
    },
    {
      "start": 2875.52,
      "duration": 4.32,
      "text": "It was a massive jump, and it did perfect.\nYou're looking at it like, \"Wow, this means  "
    },
    {
      "start": 2879.84,
      "duration": 5.44,
      "text": "the student is perfect in all these problems. \nIt's fully solved math.\" But when you look at  "
    },
    {
      "start": 2885.28,
      "duration": 3.2,
      "text": "the completions that you're getting from \nthe model, they are complete nonsense. "
    },
    {
      "start": 2888.48,
      "duration": 1.92,
      "text": "They start out okay, and then \nthey change to \"dhdhdhdh.\" "
    },
    {
      "start": 2891.36,
      "duration": 3.36,
      "text": "It's just like, \"Oh, okay, let's take two plus \nthree and we do this and this, and then dhdhdhdh.\" "
    },
    {
      "start": 2895.84,
      "duration": 1.36,
      "text": "You're looking at it, and \nit's like, this is crazy. "
    },
    {
      "start": 2897.2,
      "duration": 4.56,
      "text": "How is it getting a reward of one or 100%?\nYou look at the LLM judge, and it turns out  "
    },
    {
      "start": 2901.76,
      "duration": 5.44,
      "text": "that \"dhdhdhdh\" is an adversarial example for \nthe model, and it assigns 100% probability to it. "
    },
    {
      "start": 2907.2,
      "duration": 3.04,
      "text": "It's just because this is an \nout-of-sample example to the LLM. "
    },
    {
      "start": 2910.24,
      "duration": 3.76,
      "text": "It's never seen it during training, \nand you're in pure generalization land. "
    },
    {
      "start": 2914,
      "duration": 3.44,
      "text": "It's never seen it during training, and in \nthe pure generalization land, you can find  "
    },
    {
      "start": 2917.44,
      "duration": 4.8,
      "text": "these examples that break it.\nYou're basically training  "
    },
    {
      "start": 2922.24,
      "duration": 4.56,
      "text": "the LLM to be a prompt injection model.\nNot even that. Prompt injection is way too fancy. "
    },
    {
      "start": 2926.8,
      "duration": 2.16,
      "text": "You're finding adversarial \nexamples, as they're called. "
    },
    {
      "start": 2928.96,
      "duration": 6.48,
      "text": "These are nonsensical solutions that are obviously \nwrong, but the model thinks they are amazing. "
    },
    {
      "start": 2935.44,
      "duration": 4.08,
      "text": "To the extent you think this is the \nbottleneck to making RL more functional,  "
    },
    {
      "start": 2939.52,
      "duration": 4.88,
      "text": "then that will require making LLMs better judges, \nif you want to do this in an automated way. "
    },
    {
      "start": 2945.2,
      "duration": 2.16,
      "text": "Is it just going to be some sort \nof GAN-like approach where you  "
    },
    {
      "start": 2947.36,
      "duration": 4,
      "text": "have to train models to be more robust?\nThe labs are probably doing all that. "
    },
    {
      "start": 2951.92,
      "duration": 2.8,
      "text": "The obvious thing is, \"dhdhdhdh\" \nshould not get 100% reward. "
    },
    {
      "start": 2954.72,
      "duration": 3.04,
      "text": "Okay, well, take \"dhdhdhdh,\" put it \nin the training set of the LLM judge,  "
    },
    {
      "start": 2957.76,
      "duration": 3.92,
      "text": "and say this is not 100%, this is 0%.\nYou can do this, but every time you do  "
    },
    {
      "start": 2961.68,
      "duration": 3.12,
      "text": "this, you get a new LLM, and it \nstill has adversarial examples. "
    },
    {
      "start": 2964.8,
      "duration": 4.56,
      "text": "There's an infinity of adversarial examples.\nProbably if you iterate this a few times, it'll  "
    },
    {
      "start": 2969.36,
      "duration": 3.44,
      "text": "probably be harder and harder to find adversarial \nexamples, but I'm not 100% sure because this thing  "
    },
    {
      "start": 2972.8,
      "duration": 5.76,
      "text": "has a trillion parameters or whatnot.\nI bet you the labs are trying. "
    },
    {
      "start": 2981.92,
      "duration": 4.24,
      "text": "I still think we need other ideas.\nInteresting. Do you have some shape  "
    },
    {
      "start": 2986.16,
      "duration": 8.08,
      "text": "of what the other idea could be?\nThis idea of a review solution  "
    },
    {
      "start": 2994.24,
      "duration": 4.08,
      "text": "encompassing synthetic examples such that \nwhen you train on them, you get better,  "
    },
    {
      "start": 2998.32,
      "duration": 2.48,
      "text": "and meta-learn it in some way.\nI think there are some papers  "
    },
    {
      "start": 3000.8,
      "duration": 3.52,
      "text": "that I'm starting to see pop out.\nI am only at a stage of reading abstracts  "
    },
    {
      "start": 3004.32,
      "duration": 4.56,
      "text": "because a lot of these papers are just ideas.\nSomeone has to make it work on a frontier  "
    },
    {
      "start": 3008.88,
      "duration": 4.4,
      "text": "LLM lab scale in full generality \nbecause when you see these papers,  "
    },
    {
      "start": 3013.28,
      "duration": 4.56,
      "text": "they pop up, and it's just a bit noisy.\nThey're cool ideas, but I haven't seen  "
    },
    {
      "start": 3017.84,
      "duration": 5.84,
      "text": "anyone convincingly show that this is possible.\nThat said, the LLM labs are fairly closed,  "
    },
    {
      "start": 3023.68,
      "duration": 9.52,
      "text": "so who knows what they're doing now.\nI can conceptualize how you would be able  "
    },
    {
      "start": 3033.2,
      "duration": 3.6,
      "text": "to train on synthetic examples or synthetic \nproblems that you have made for yourself. "
    },
    {
      "start": 3036.8,
      "duration": 3.92,
      "text": "But there seems to be another thing humans \ndo—maybe sleep is this, maybe daydreaming is  "
    },
    {
      "start": 3040.72,
      "duration": 5.44,
      "text": "this—which is not necessarily to come up \nwith fake problems, but just to reflect. "
    },
    {
      "start": 3047.28,
      "duration": 4.24,
      "text": "I'm not sure what the ML analogy is for \ndaydreaming or sleeping, or just reflecting. "
    },
    {
      "start": 3051.52,
      "duration": 3.12,
      "text": "I haven't come up with a new problem.\nObviously, the very basic analogy would just  "
    },
    {
      "start": 3054.64,
      "duration": 4.96,
      "text": "be fine-tuning on reflection bits, but I feel like \nin practice that probably wouldn't work that well. "
    },
    {
      "start": 3060.24,
      "duration": 4.88,
      "text": "Do you have some take on what \nthe analogy of this thing is? "
    },
    {
      "start": 3065.12,
      "duration": 4.48,
      "text": "I do think that we're missing some aspects there.\nAs an example, let’s take reading a book. "
    },
    {
      "start": 3071.36,
      "duration": 4.32,
      "text": "Currently when LLMs are reading a book, what that \nmeans is we stretch out the sequence of text,  "
    },
    {
      "start": 3075.68,
      "duration": 3.44,
      "text": "and the model is predicting the next token, \nand it's getting some knowledge from that. "
    },
    {
      "start": 3079.12,
      "duration": 2.4,
      "text": "That's not really what humans do.\nWhen you're reading a book,  "
    },
    {
      "start": 3081.52,
      "duration": 4.32,
      "text": "I don't even feel like the book is exposition \nI'm supposed to be attending to and training on. "
    },
    {
      "start": 3085.84,
      "duration": 4.24,
      "text": "The book is a set of prompts for \nme to do synthetic data generation,  "
    },
    {
      "start": 3090.08,
      "duration": 3.12,
      "text": "or for you to get to a book club \nand talk about it with your friends. "
    },
    {
      "start": 3093.2,
      "duration": 3.76,
      "text": "It's by manipulating that information \nthat you actually gain that knowledge. "
    },
    {
      "start": 3097.68,
      "duration": 4.32,
      "text": "We have no equivalent of that with LLMs. They \ndon't really do that. I'd love to see during  "
    },
    {
      "start": 3102,
      "duration": 4.32,
      "text": "pre-training some stage that thinks through \nthe material and tries to reconcile it with  "
    },
    {
      "start": 3106.32,
      "duration": 5.84,
      "text": "what it already knows, and thinks through it \nfor some amount of time and gets that to work. "
    },
    {
      "start": 3112.16,
      "duration": 2.56,
      "text": "There's no equivalence of any of this. \nThis is all research. There are some  "
    },
    {
      "start": 3114.72,
      "duration": 4.8,
      "text": "subtle—very subtle that I think are very hard \nto understand—reasons why it's not trivial. "
    },
    {
      "start": 3119.52,
      "duration": 4.64,
      "text": "If I can just describe one: why can't we \njust synthetically generate and train on it? "
    },
    {
      "start": 3124.16,
      "duration": 3.2,
      "text": "Because every synthetic example, if \nI just give synthetic generation of  "
    },
    {
      "start": 3127.36,
      "duration": 3.44,
      "text": "the model thinking about a book, you look \nat it and you're like, \"This looks great. "
    },
    {
      "start": 3130.8,
      "duration": 2.16,
      "text": "Why can't I train on it?\"\nYou could try, but the model  "
    },
    {
      "start": 3132.96,
      "duration": 4.08,
      "text": "will get much worse if you continue trying.\nThat's because all of the samples you get  "
    },
    {
      "start": 3137.04,
      "duration": 4.48,
      "text": "from models are silently collapsed.\nSilently—it is not obvious if you look  "
    },
    {
      "start": 3141.52,
      "duration": 4.96,
      "text": "at any individual example of it—they occupy \na very tiny manifold of the possible space of  "
    },
    {
      "start": 3147.2,
      "duration": 3.28,
      "text": "thoughts about content.\nThe LLMs, when they come off,  "
    },
    {
      "start": 3150.48,
      "duration": 2.8,
      "text": "they're what we call \"collapsed.\"\nThey have a collapsed data distribution. "
    },
    {
      "start": 3154.96,
      "duration": 3.44,
      "text": "One easy way to see it is to go to \nChatGPT and ask it, \"Tell me a joke.\" "
    },
    {
      "start": 3158.4,
      "duration": 3.04,
      "text": "It only has like three jokes.\nIt's not giving you the whole breadth  "
    },
    {
      "start": 3161.44,
      "duration": 5.92,
      "text": "of possible jokes. It knows like three jokes. \nThey're silently collapsed. You're not getting  "
    },
    {
      "start": 3167.36,
      "duration": 5.2,
      "text": "the richness and the diversity and the entropy \nfrom these models as you would get from humans. "
    },
    {
      "start": 3172.56,
      "duration": 3.04,
      "text": "Humans are a lot noisier, but \nat least they're not biased,  "
    },
    {
      "start": 3176.56,
      "duration": 4.16,
      "text": "in a statistical sense. They're not silently \ncollapsed. They maintain a huge amount of entropy. "
    },
    {
      "start": 3180.72,
      "duration": 4.8,
      "text": "So how do you get synthetic data generation to \nwork despite the collapse and while maintaining  "
    },
    {
      "start": 3185.52,
      "duration": 4.32,
      "text": "the entropy? That’s a research problem.\nJust to make sure I understood, the reason  "
    },
    {
      "start": 3189.84,
      "duration": 3.36,
      "text": "that the collapse is relevant to synthetic data \ngeneration is because you want to be able to  "
    },
    {
      "start": 3193.2,
      "duration": 6.88,
      "text": "come up with synthetic problems or reflections \nwhich are not already in your data distribution? "
    },
    {
      "start": 3200.08,
      "duration": 5.6,
      "text": "I guess what I'm saying is, say we have a chapter \nof a book and I ask an LLM to think about it,  "
    },
    {
      "start": 3206.56,
      "duration": 1.84,
      "text": "it will give you something \nthat looks very reasonable. "
    },
    {
      "start": 3208.4,
      "duration": 3.52,
      "text": "But if I ask it 10 times, you'll \nnotice that all of them are the same. "
    },
    {
      "start": 3211.92,
      "duration": 7.92,
      "text": "You can't just keep scaling \"reflection\" \non the same amount of prompt information  "
    },
    {
      "start": 3219.84,
      "duration": 4,
      "text": "and then get returns from that.\nAny individual sample will look okay,  "
    },
    {
      "start": 3223.84,
      "duration": 4,
      "text": "but the distribution of it is quite terrible.\nIt's quite terrible in such a way that if  "
    },
    {
      "start": 3227.84,
      "duration": 3.04,
      "text": "you continue training on too much of \nyour own stuff, you actually collapse. "
    },
    {
      "start": 3230.88,
      "duration": 2.8,
      "text": "I think that there's possibly \nno fundamental solution to this. "
    },
    {
      "start": 3234.24,
      "duration": 5.76,
      "text": "I also think humans collapse over time. \nThese analogies are surprisingly good.  "
    },
    {
      "start": 3240,
      "duration": 6,
      "text": "Humans collapse during the course of their lives.\nThis is why children, they haven't overfit yet. "
    },
    {
      "start": 3246,
      "duration": 3.84,
      "text": "They will say stuff that will shock you \nbecause you can see where they're coming from,  "
    },
    {
      "start": 3249.84,
      "duration": 4.64,
      "text": "but it's just not the thing people say, \nbecause they're not yet collapsed. But we're  "
    },
    {
      "start": 3254.48,
      "duration": 5.68,
      "text": "collapsed. We end up revisiting the same thoughts.\nWe end up saying more and more of the same stuff,  "
    },
    {
      "start": 3260.16,
      "duration": 3.84,
      "text": "and the learning rates go down, and \nthe collapse continues to get worse,  "
    },
    {
      "start": 3264,
      "duration": 4.48,
      "text": "and then everything deteriorates.\nHave you seen this super interesting  "
    },
    {
      "start": 3268.48,
      "duration": 5.68,
      "text": "paper that dreaming is a way of preventing \nthis kind of overfitting and collapse? "
    },
    {
      "start": 3274.16,
      "duration": 7.12,
      "text": "The reason dreaming is evolutionary adaptive \nis to put you in weird situations that are  "
    },
    {
      "start": 3281.84,
      "duration": 2.72,
      "text": "very unlike your day-to-day reality, so \nas to prevent this kind of overfitting. "
    },
    {
      "start": 3284.56,
      "duration": 3.52,
      "text": "It's an interesting idea. I do think \nthat when you're generating things  "
    },
    {
      "start": 3288.08,
      "duration": 3.28,
      "text": "in your head and then you're attending to \nit, you're training on your own samples,  "
    },
    {
      "start": 3291.36,
      "duration": 2.32,
      "text": "you're training on your synthetic data.\nIf you do it for too long,  "
    },
    {
      "start": 3293.68,
      "duration": 6.24,
      "text": "you go off-rails and you collapse way too much.\nYou always have to seek entropy in your life. "
    },
    {
      "start": 3301.36,
      "duration": 3.76,
      "text": "Talking to other people is a great \nsource of entropy, and things like that. "
    },
    {
      "start": 3305.12,
      "duration": 4.72,
      "text": "So maybe the brain has also built some internal \nmechanisms for increasing the amount of entropy  "
    },
    {
      "start": 3311.36,
      "duration": 5.36,
      "text": "in that process. That's an interesting idea.\nThis is a very ill-formed thought so I’ll  "
    },
    {
      "start": 3316.72,
      "duration": 3.92,
      "text": "just put it out and let you react to it.\nThe best learners that we are aware of,  "
    },
    {
      "start": 3320.64,
      "duration": 5.04,
      "text": "which are children, are extremely \nbad at recollecting information. "
    },
    {
      "start": 3325.68,
      "duration": 3.52,
      "text": "In fact, at the very earliest stages of \nchildhood, you will forget everything. "
    },
    {
      "start": 3329.2,
      "duration": 3.44,
      "text": "You're just an amnesiac about everything \nthat happens before a certain year date. "
    },
    {
      "start": 3332.64,
      "duration": 3.44,
      "text": "But you're extremely good at picking up \nnew languages and learning from the world. "
    },
    {
      "start": 3336.08,
      "duration": 2.88,
      "text": "Maybe there's some element of being \nable to see the forest for the trees. "
    },
    {
      "start": 3338.96,
      "duration": 5.12,
      "text": "Whereas if you compare it to the opposite end \nof the spectrum, you have LLM pre-training,  "
    },
    {
      "start": 3344.08,
      "duration": 3.84,
      "text": "where these models will literally be \nable to regurgitate word-for-word what  "
    },
    {
      "start": 3347.92,
      "duration": 4.48,
      "text": "is the next thing in a Wikipedia page.\nBut their ability to learn abstract  "
    },
    {
      "start": 3352.4,
      "duration": 3.52,
      "text": "concepts really quickly, the way \na child can, is much more limited. "
    },
    {
      "start": 3355.92,
      "duration": 3.92,
      "text": "Then adults are somewhere in between, where \nthey don't have the flexibility of childhood  "
    },
    {
      "start": 3359.84,
      "duration": 6.08,
      "text": "learning, but they can memorize facts and \ninformation in a way that is harder for kids. "
    },
    {
      "start": 3365.92,
      "duration": 2.24,
      "text": "I don't know if there's something \ninteresting about that spectrum. "
    },
    {
      "start": 3368.16,
      "duration": 2.48,
      "text": "I think there's something very \ninteresting about that, 100%. "
    },
    {
      "start": 3370.64,
      "duration": 5.84,
      "text": "I do think that humans have a lot more of \nan element, compared to LLMs, of seeing  "
    },
    {
      "start": 3376.48,
      "duration": 2.72,
      "text": "the forest for the trees.\nWe're not actually that good  "
    },
    {
      "start": 3379.2,
      "duration": 6.08,
      "text": "at memorization, which is actually a feature.\nBecause we're not that good at memorization, we're  "
    },
    {
      "start": 3385.92,
      "duration": 7.92,
      "text": "forced to find patterns in a more general sense.\nLLMs in comparison are extremely good  "
    },
    {
      "start": 3393.84,
      "duration": 1.6,
      "text": "at memorization.\nThey will recite  "
    },
    {
      "start": 3395.44,
      "duration": 4.8,
      "text": "passages from all these training sources.\nYou can give them completely nonsensical data. "
    },
    {
      "start": 3401.28,
      "duration": 3.36,
      "text": "You can hash some amount of text or something \nlike that, you get a completely random sequence. "
    },
    {
      "start": 3404.64,
      "duration": 4,
      "text": "If you train on it, even just for a single \niteration or two, it can suddenly regurgitate  "
    },
    {
      "start": 3408.64,
      "duration": 3.04,
      "text": "the entire thing. It will memorize it. \nThere's no way a person can read a single  "
    },
    {
      "start": 3411.68,
      "duration": 6.72,
      "text": "sequence of random numbers and recite it to you.\nThat's a feature, not a bug, because it forces  "
    },
    {
      "start": 3418.4,
      "duration": 4.96,
      "text": "you to only learn the generalizable components.\nWhereas LLMs are distracted by all the memory  "
    },
    {
      "start": 3423.36,
      "duration": 3.2,
      "text": "that they have of the pre-training \ndocuments, and it's probably very  "
    },
    {
      "start": 3426.56,
      "duration": 4,
      "text": "distracting to them in a certain sense.\nSo that's why when I talk about the  "
    },
    {
      "start": 3430.56,
      "duration": 3.12,
      "text": "cognitive core, I want to remove the \nmemory, which is what we talked about. "
    },
    {
      "start": 3433.68,
      "duration": 3.92,
      "text": "I'd love to have them have less memory \nso that they have to look things up,  "
    },
    {
      "start": 3437.6,
      "duration": 5.2,
      "text": "and they only maintain the algorithms for \nthought, and the idea of an experiment,  "
    },
    {
      "start": 3442.8,
      "duration": 5.52,
      "text": "and all this cognitive glue of acting.\nAnd this is also relevant to preventing  "
    },
    {
      "start": 3448.32,
      "duration": 6.72,
      "text": "model collapse?\nLet me think. I'm  "
    },
    {
      "start": 3455.04,
      "duration": 5.28,
      "text": "not sure. It's almost like a separate axis.\nThe models are way too good at memorization,  "
    },
    {
      "start": 3460.32,
      "duration": 6.16,
      "text": "and somehow we should remove that.\nPeople are much worse, but it's a good thing. "
    },
    {
      "start": 3466.48,
      "duration": 4.24,
      "text": "What is a solution to model collapse?\nThere are very naive things you could attempt. "
    },
    {
      "start": 3472.4,
      "duration": 3.36,
      "text": "The distribution over logits \nshould be wider or something. "
    },
    {
      "start": 3475.76,
      "duration": 2.72,
      "text": "There are many naive things you could try.\nWhat ends up being the problem  "
    },
    {
      "start": 3478.48,
      "duration": 4.4,
      "text": "with the naive approaches?\nThat's a great question. You can imagine having  "
    },
    {
      "start": 3482.88,
      "duration": 3.92,
      "text": "a regularization for entropy and things like that.\nI guess they just don't work as well empirically  "
    },
    {
      "start": 3486.8,
      "duration": 6.24,
      "text": "because right now the models are collapsed.\nBut I will say most of the tasks that we  "
    },
    {
      "start": 3493.04,
      "duration": 5.76,
      "text": "want from them don't actually demand diversity.\nThat’s probably the answer to what's going on. "
    },
    {
      "start": 3500,
      "duration": 2.72,
      "text": "The frontier labs are trying \nto make the models useful. "
    },
    {
      "start": 3502.72,
      "duration": 3.6,
      "text": "I feel like the diversity of \nthe outputs is not so much... "
    },
    {
      "start": 3506.32,
      "duration": 3.2,
      "text": "Number one, it's much harder to work with and \nevaluate and all this stuff, but maybe it's not  "
    },
    {
      "start": 3509.52,
      "duration": 4,
      "text": "what's capturing most of the value.\nIn fact, it's actively penalized.  "
    },
    {
      "start": 3514.16,
      "duration": 5.2,
      "text": "If you're super creative in RL, it's not good.\nYeah. Or maybe if you're doing a lot of writing,  "
    },
    {
      "start": 3519.36,
      "duration": 3.12,
      "text": "help from LLMs and stuff like that, it's probably \nbad because the models will silently give  "
    },
    {
      "start": 3522.48,
      "duration": 6,
      "text": "you all the same stuff.\nThey won't explore lots  "
    },
    {
      "start": 3528.48,
      "duration": 7.76,
      "text": "of different ways of answering a question.\nMaybe this diversity, not as many applications  "
    },
    {
      "start": 3536.24,
      "duration": 2.16,
      "text": "need it so the models don't have it.\nBut then it's a problem at  "
    },
    {
      "start": 3538.4,
      "duration": 3.28,
      "text": "synthetic data generation time, et cetera.\nSo we're shooting ourselves in the foot by not  "
    },
    {
      "start": 3541.68,
      "duration": 5.2,
      "text": "allowing this entropy to maintain in the model.\nPossibly the labs should try harder. "
    },
    {
      "start": 3546.88,
      "duration": 4.48,
      "text": "I think you hinted that it's a very \nfundamental problem, it won't be easy  "
    },
    {
      "start": 3551.36,
      "duration": 5.68,
      "text": "to solve. What's your intuition for that?\nI don't know if it's super fundamental. "
    },
    {
      "start": 3557.04,
      "duration": 6.4,
      "text": "I don't know if I intended to say that.\nI do think that I haven't done these experiments,  "
    },
    {
      "start": 3563.44,
      "duration": 3.44,
      "text": "but I do think that you could probably \nregularize the entropy to be higher. "
    },
    {
      "start": 3566.88,
      "duration": 4.48,
      "text": "So you're encouraging the model to give you more \nand more solutions, but you don't want it to  "
    },
    {
      "start": 3571.36,
      "duration": 3.12,
      "text": "start deviating too much from the training data.\nIt's going to start making up its own language. "
    },
    {
      "start": 3574.48,
      "duration": 4,
      "text": "It's going to start using words that are \nextremely rare, so it's going to drift too  "
    },
    {
      "start": 3578.48,
      "duration": 2.56,
      "text": "much from the distribution.\nSo I think controlling  "
    },
    {
      "start": 3581.04,
      "duration": 6.56,
      "text": "the distribution is just tricky.\nIt's probably not trivial in that sense. "
    },
    {
      "start": 3587.6,
      "duration": 6.48,
      "text": "How many bits should the optimal core \nof intelligence end up being if you  "
    },
    {
      "start": 3594.08,
      "duration": 2.48,
      "text": "just had to make a guess?\nThe thing we put on the  "
    },
    {
      "start": 3596.56,
      "duration": 5.2,
      "text": "von Neumann probes, how big does it have to be?\nIt's really interesting in the history of the  "
    },
    {
      "start": 3601.76,
      "duration": 4,
      "text": "field because at one point everything was \nvery scaling-pilled in terms of like, \"Oh,  "
    },
    {
      "start": 3605.76,
      "duration": 3.12,
      "text": "we're gonna make much bigger models, \ntrillions of parameter models.\" "
    },
    {
      "start": 3608.88,
      "duration": 3.12,
      "text": "What the models have done in size \nis they've gone up and now they've  "
    },
    {
      "start": 3614.64,
      "duration": 4.72,
      "text": "come down. State-of-the-art models are smaller. \nEven then, I think they memorized way too much. "
    },
    {
      "start": 3620.16,
      "duration": 4.48,
      "text": "So I had a prediction a while back that I almost \nfeel like we can get cognitive cores that are  "
    },
    {
      "start": 3624.64,
      "duration": 6.32,
      "text": "very good at even a billion parameters.\nIf you talk to a billion parameter model,  "
    },
    {
      "start": 3630.96,
      "duration": 3.28,
      "text": "I think in 20 years, you can have \na very productive conversation. "
    },
    {
      "start": 3634.24,
      "duration": 5.04,
      "text": "It thinks and it's a lot more like a human.\nBut if you ask it some factual question, it might  "
    },
    {
      "start": 3639.28,
      "duration": 2.96,
      "text": "have to look it up, but it knows that it doesn't \nknow and it might have to look it up and it will  "
    },
    {
      "start": 3642.24,
      "duration": 2.56,
      "text": "just do all the reasonable things.\nThat's surprising that you think  "
    },
    {
      "start": 3644.8,
      "duration": 2.72,
      "text": "it'll take a billion parameters.\nBecause already we have billion  "
    },
    {
      "start": 3647.52,
      "duration": 3.92,
      "text": "parameter models or a couple billion \nparameter models that are very intelligent. "
    },
    {
      "start": 3651.44,
      "duration": 2.24,
      "text": "Well, state-of-the-art models \nare like a trillion parameters. "
    },
    {
      "start": 3653.68,
      "duration": 5.04,
      "text": "But they remember so much stuff.\nYeah, but I'm surprised that in 10 years,  "
    },
    {
      "start": 3658.72,
      "duration": 8.32,
      "text": "given the pace… We have gpt-oss-20b.\nThat's way better than GPT-4 original,  "
    },
    {
      "start": 3667.04,
      "duration": 4.56,
      "text": "which was a trillion plus parameters.\nGiven that trend, I'm surprised you  "
    },
    {
      "start": 3671.6,
      "duration": 3.52,
      "text": "think in 10 years the cognitive \ncore is still a billion parameters. "
    },
    {
      "start": 3675.12,
      "duration": 4.88,
      "text": "I'm surprised you're not like, \"Oh it's \ngonna be like tens of millions or millions.\" "
    },
    {
      "start": 3682.16,
      "duration": 3.84,
      "text": "Here's the issue, the training data is \nthe internet, which is really terrible. "
    },
    {
      "start": 3686,
      "duration": 2.48,
      "text": "There's a huge amount of gains to be \nmade because the internet is terrible. "
    },
    {
      "start": 3689.52,
      "duration": 2.56,
      "text": "Even the internet, when you and I think of \nthe internet, you're thinking of like The  "
    },
    {
      "start": 3692.08,
      "duration": 4.16,
      "text": "Wall Street Journal. That's not what this \nis. When you're looking at a pre-training  "
    },
    {
      "start": 3696.24,
      "duration": 4.48,
      "text": "dataset in the frontier lab and you look at a \nrandom internet document, it's total garbage. "
    },
    {
      "start": 3700.72,
      "duration": 4.88,
      "text": "I don't even know how this works at all.\nIt's some like stock tickers, symbols,  "
    },
    {
      "start": 3706.56,
      "duration": 3.6,
      "text": "it's a huge amount of slop and garbage \nfrom like all the corners of the internet. "
    },
    {
      "start": 3710.16,
      "duration": 3.04,
      "text": "It's not like your Wall Street Journal \narticle, that's extremely rare. "
    },
    {
      "start": 3713.92,
      "duration": 6.48,
      "text": "So because the internet is so terrible, we have \nto build really big models to compress all that. "
    },
    {
      "start": 3720.4,
      "duration": 3.68,
      "text": "Most of that compression is memory \nwork instead of cognitive work. "
    },
    {
      "start": 3724.08,
      "duration": 2.88,
      "text": "But what we really want is the \ncognitive part, delete the memory. "
    },
    {
      "start": 3728.24,
      "duration": 4.24,
      "text": "I guess what I'm saying is that we need \nintelligent models to help us refine even  "
    },
    {
      "start": 3732.48,
      "duration": 3.44,
      "text": "the pre-training set to just narrow \nit down to the cognitive components. "
    },
    {
      "start": 3735.92,
      "duration": 2.48,
      "text": "Then I think you get away with a \nmuch smaller model because it's a  "
    },
    {
      "start": 3738.4,
      "duration": 3.84,
      "text": "much better dataset and you could train it on it.\nBut probably it's not trained directly on it, it's  "
    },
    {
      "start": 3742.24,
      "duration": 4.8,
      "text": "probably distilled from a much better model still.\nBut why is the distilled version still a billion? "
    },
    {
      "start": 3748.56,
      "duration": 1.92,
      "text": "I just feel like distillation \nworks extremely well. "
    },
    {
      "start": 3750.48,
      "duration": 4,
      "text": "So almost every small model, if you have a \nsmall model, it's almost certainly distilled. "
    },
    {
      "start": 3755.52,
      "duration": 4.16,
      "text": "Right, but why is the distillation in \n10 years not getting below 1 billion? "
    },
    {
      "start": 3759.68,
      "duration": 3.84,
      "text": "Oh, you think it should be smaller than a \nbillion? I mean, come on, right? I don't  "
    },
    {
      "start": 3763.52,
      "duration": 6,
      "text": "know. At some point it should take at least \na billion knobs to do something interesting. "
    },
    {
      "start": 3769.52,
      "duration": 3.68,
      "text": "You're thinking it should be even smaller?\nYeah. If you look at the trend over the last  "
    },
    {
      "start": 3773.2,
      "duration": 4.64,
      "text": "few years of just finding low-hanging fruit and \ngoing from trillion plus models to models that  "
    },
    {
      "start": 3777.84,
      "duration": 5.68,
      "text": "are literally two orders of magnitude smaller in a \nmatter of two years and having better performance,  "
    },
    {
      "start": 3783.52,
      "duration": 5.84,
      "text": "it makes me think the sort of core of \nintelligence might be even way, way smaller. "
    },
    {
      "start": 3789.36,
      "duration": 2.32,
      "text": "Plenty of room at the bottom, \nto paraphrase Feynman. "
    },
    {
      "start": 3791.68,
      "duration": 2.96,
      "text": "I feel like I'm already contrarian \nby talking about a billion parameter  "
    },
    {
      "start": 3794.64,
      "duration": 8.72,
      "text": "cognitive core and you're outdoing me.\nMaybe we could get a little bit smaller. "
    },
    {
      "start": 3803.36,
      "duration": 3.04,
      "text": "I do think that practically speaking, you \nwant the model to have some knowledge. "
    },
    {
      "start": 3806.4,
      "duration": 3.68,
      "text": "You don't want it to be looking up everything \nbecause then you can't think in your head. "
    },
    {
      "start": 3810.08,
      "duration": 1.44,
      "text": "You're looking up way too much stuff all the time. "
    },
    {
      "start": 3812.72,
      "duration": 5.04,
      "text": "Some basic curriculum needs to be there for \nknowledge, but it doesn't have esoteric knowledge. "
    },
    {
      "start": 3818.64,
      "duration": 2.72,
      "text": "We're discussing what plausibly \ncould be the cognitive core. "
    },
    {
      "start": 3821.36,
      "duration": 5.6,
      "text": "There's a separate question which is what \nwill be the size of frontier models over time? "
    },
    {
      "start": 3826.96,
      "duration": 4.88,
      "text": "I'm curious if you have predictions.\nWe had increasing scale up to maybe GPT 4.5 and  "
    },
    {
      "start": 3831.84,
      "duration": 4.88,
      "text": "now we're seeing decreasing or plateauing scale.\nThere are many reasons this could be going on. "
    },
    {
      "start": 3836.72,
      "duration": 4.16,
      "text": "Do you have a prediction going forward?\nWill the biggest models be bigger,  "
    },
    {
      "start": 3840.88,
      "duration": 5.2,
      "text": "will they be smaller, will they be the same?\nI don't have a super strong prediction. "
    },
    {
      "start": 3847.2,
      "duration": 3.76,
      "text": "The labs are just being practical.\nThey have a flops budget and a cost budget. "
    },
    {
      "start": 3850.96,
      "duration": 3.28,
      "text": "It just turns out that pre-training is not where \nyou want to put most of your flops or your cost. "
    },
    {
      "start": 3854.24,
      "duration": 3.6,
      "text": "That's why the models have gotten smaller.\nThey are a bit smaller, the pre-training  "
    },
    {
      "start": 3857.84,
      "duration": 2.64,
      "text": "stage is smaller, but they make \nit up in reinforcement learning,  "
    },
    {
      "start": 3861.2,
      "duration": 4.08,
      "text": "mid-training, and all this stuff that follows.\nThey're just being practical in terms of all the  "
    },
    {
      "start": 3865.28,
      "duration": 5.36,
      "text": "stages and how you get the most bang for the buck.\nForecasting that trend is quite hard. "
    },
    {
      "start": 3870.64,
      "duration": 5.04,
      "text": "I do still expect that there's so much \nlow-hanging fruit. That's my basic expectation.  "
    },
    {
      "start": 3878.24,
      "duration": 4.24,
      "text": "I have a very wide distribution here.\nDo you expect the low-hanging fruit to be  "
    },
    {
      "start": 3882.48,
      "duration": 5.92,
      "text": "similar in kind to the kinds of things that have \nbeen happening over the last two to five years? "
    },
    {
      "start": 3889.52,
      "duration": 4.56,
      "text": "If I look at nanochat versus nanoGPT \nand the architectural tweaks you made,  "
    },
    {
      "start": 3894.08,
      "duration": 2.8,
      "text": "is that the flavor of things you \nexpect to continue to keep happening? "
    },
    {
      "start": 3898,
      "duration": 3.2,
      "text": "You're not expecting any giant paradigm shifts.\nFor the most part, yeah. I expect the  "
    },
    {
      "start": 3901.2,
      "duration": 2.56,
      "text": "datasets to get much, much better.\nWhen you look at the average datasets,  "
    },
    {
      "start": 3903.76,
      "duration": 1.76,
      "text": "they're extremely terrible.\nThey’re so bad that I  "
    },
    {
      "start": 3905.52,
      "duration": 5.2,
      "text": "don't even know how anything works.\nLook at the average example in the training set:  "
    },
    {
      "start": 3910.72,
      "duration": 5.12,
      "text": "factual mistakes, errors, nonsensical things.\nSomehow when you do it at scale,  "
    },
    {
      "start": 3916.4,
      "duration": 4.72,
      "text": "the noise washes away and you're left with \nsome of the signal. Datasets will improve  "
    },
    {
      "start": 3921.12,
      "duration": 8.16,
      "text": "a ton. Everything gets better. Our hardware, \nall the kernels for running the hardware and  "
    },
    {
      "start": 3929.28,
      "duration": 4.32,
      "text": "maximizing what you get with the hardware.\nNvidia is slowly tuning the hardware itself,  "
    },
    {
      "start": 3933.6,
      "duration": 3.36,
      "text": "Tensor Cores, all that needs to \nhappen and will continue to happen. "
    },
    {
      "start": 3936.96,
      "duration": 2.88,
      "text": "All the kernels will get better and \nutilize the chip to the max extent. "
    },
    {
      "start": 3939.84,
      "duration": 5.28,
      "text": "All the algorithms will probably improve over \noptimization, architecture, and all the modeling  "
    },
    {
      "start": 3945.12,
      "duration": 3.76,
      "text": "components of how everything is done and what \nthe algorithms are that we're even training with. "
    },
    {
      "start": 3948.88,
      "duration": 10.4,
      "text": "I do expect that nothing dominates. Everything \nplus 20%. This is roughly what I've seen. "
    },
    {
      "start": 4033.92,
      "duration": 7.52,
      "text": "People have proposed different ways of charting \nhow much progress we've made towards full AGI. "
    },
    {
      "start": 4041.44,
      "duration": 3.84,
      "text": "If you can come up with some line, then you \ncan see where that line intersects with AGI  "
    },
    {
      "start": 4045.28,
      "duration": 4.64,
      "text": "and where that would happen on the x-axis.\nPeople have proposed it's the education level. "
    },
    {
      "start": 4049.92,
      "duration": 4.32,
      "text": "We had a high schooler, and then they went to \ncollege with RL, and they're going to get a Ph.D. "
    },
    {
      "start": 4054.24,
      "duration": 2.56,
      "text": "I don't like that one.\nOr they'll propose horizon  "
    },
    {
      "start": 4056.8,
      "duration": 4.96,
      "text": "length. Maybe they can do tasks that take \na minute, they can do those autonomously. "
    },
    {
      "start": 4061.76,
      "duration": 3.6,
      "text": "Then they can autonomously do tasks that take \nan hour, a human an hour, a human a week. "
    },
    {
      "start": 4066.32,
      "duration": 6.8,
      "text": "How do you think about the relevant y-axis here?\nHow should we think about how  "
    },
    {
      "start": 4073.12,
      "duration": 2.8,
      "text": "AI is making progress?\nI have two answers to that. "
    },
    {
      "start": 4075.92,
      "duration": 3.44,
      "text": "Number one, I'm almost tempted to \nreject the question entirely because  "
    },
    {
      "start": 4079.36,
      "duration": 3.52,
      "text": "I see this as an extension of computing.\nHave we talked about how to chart progress  "
    },
    {
      "start": 4082.88,
      "duration": 3.52,
      "text": "in computing, or how do you chart progress \nin computing since the 1970s or whatever?  "
    },
    {
      "start": 4086.4,
      "duration": 4.8,
      "text": "What is the y-axis? The whole question is \nfunny from that perspective a little bit. "
    },
    {
      "start": 4093.44,
      "duration": 5.52,
      "text": "When people talk about AI and the original AGI \nand how we spoke about it when OpenAI started,  "
    },
    {
      "start": 4098.96,
      "duration": 8.96,
      "text": "AGI was a system you could go to that can do any \neconomically valuable task at human performance  "
    },
    {
      "start": 4107.92,
      "duration": 4,
      "text": "or better. That was the definition. I \nwas pretty happy with that at the time. "
    },
    {
      "start": 4112.88,
      "duration": 3.6,
      "text": "I've stuck to that definition forever, and \nthen people have made up all kinds of other  "
    },
    {
      "start": 4116.48,
      "duration": 6.88,
      "text": "definitions. But I like that definition. The first \nconcession that people make all the time is they  "
    },
    {
      "start": 4123.36,
      "duration": 4.8,
      "text": "just take out all the physical stuff because \nwe're just talking about digital knowledge work. "
    },
    {
      "start": 4128.16,
      "duration": 3.92,
      "text": "That's a pretty major concession compared to \nthe original definition, which was any task  "
    },
    {
      "start": 4132.08,
      "duration": 5.6,
      "text": "a human can do. I can lift things, etc. AI \ncan't do that, obviously, but we'll take it. "
    },
    {
      "start": 4137.68,
      "duration": 5.04,
      "text": "What fraction of the economy are we taking away \nby saying, \"Oh, only knowledge work?\" I don't know  "
    },
    {
      "start": 4142.72,
      "duration": 7.2,
      "text": "the numbers. I feel about 10% to 20%, if I had to \nguess, is only knowledge work, someone could work  "
    },
    {
      "start": 4149.92,
      "duration": 4.96,
      "text": "from home and perform tasks, something like that.\nIt's still a really large market. "
    },
    {
      "start": 4156.64,
      "duration": 2.72,
      "text": "What is the size of the \neconomy, and what is 10% or 20%? "
    },
    {
      "start": 4159.36,
      "duration": 6.72,
      "text": "We're still talking about a few trillion \ndollars, even in the US, of market share or work. "
    },
    {
      "start": 4166.08,
      "duration": 4.08,
      "text": "So it's still a very massive bucket.\nGoing back to the definition,  "
    },
    {
      "start": 4170.16,
      "duration": 3.68,
      "text": "what I would be looking for is to \nwhat extent is that definition true? "
    },
    {
      "start": 4175.2,
      "duration": 5.2,
      "text": "Are there jobs or lots of tasks?\nIf we think of tasks as not jobs but tasks. "
    },
    {
      "start": 4180.4,
      "duration": 6,
      "text": "It's difficult because the problem is society will \nrefactor based on the tasks that make up jobs,  "
    },
    {
      "start": 4187.44,
      "duration": 4.72,
      "text": "based on what's automatable or not.\nToday, what jobs are replaceable by AI? "
    },
    {
      "start": 4192.16,
      "duration": 4.96,
      "text": "A good example recently was Geoff Hinton's \nprediction that radiologists would not be  "
    },
    {
      "start": 4197.12,
      "duration": 2.88,
      "text": "a job anymore, and this turned out \nto be very wrong in a bunch of ways. "
    },
    {
      "start": 4200.72,
      "duration": 3.52,
      "text": "Radiologists are alive and well and growing, \neven though computer vision is really,  "
    },
    {
      "start": 4204.24,
      "duration": 3.28,
      "text": "really good at recognizing all the different \nthings that they have to recognize in images. "
    },
    {
      "start": 4207.52,
      "duration": 4.08,
      "text": "It's just a messy, complicated job with a \nlot of surfaces and dealing with patients  "
    },
    {
      "start": 4211.6,
      "duration": 6.08,
      "text": "and all this stuff in the context of it.\nI don't know that by that definition  "
    },
    {
      "start": 4217.68,
      "duration": 5.12,
      "text": "AI has made a huge dent yet.\nSome of the jobs that I would  "
    },
    {
      "start": 4222.8,
      "duration": 4.32,
      "text": "be looking for have some features that make it \nvery amenable to automation earlier than later. "
    },
    {
      "start": 4227.12,
      "duration": 3.84,
      "text": "As an example, call center employees \noften come up, and I think rightly so. "
    },
    {
      "start": 4230.96,
      "duration": 4.8,
      "text": "Call center employees have a number of simplifying \nproperties with respect to what's automatable  "
    },
    {
      "start": 4235.76,
      "duration": 6.56,
      "text": "today. Their jobs are pretty simple. It's a \nsequence of tasks, and every task looks similar. "
    },
    {
      "start": 4242.32,
      "duration": 3.44,
      "text": "You take a phone call with a person, it's \n10 minutes of interaction or whatever it is,  "
    },
    {
      "start": 4245.76,
      "duration": 3.2,
      "text": "probably a bit longer.\nIn my experience, a lot longer. "
    },
    {
      "start": 4249.6,
      "duration": 3.68,
      "text": "You complete some task in some scheme, \nand you change some database entries  "
    },
    {
      "start": 4253.28,
      "duration": 2.32,
      "text": "around or something like that.\nSo you keep repeating something  "
    },
    {
      "start": 4255.6,
      "duration": 5.6,
      "text": "over and over again, and that's your job.\nYou do want to bring in the task horizon—how  "
    },
    {
      "start": 4261.2,
      "duration": 3.84,
      "text": "long it takes to perform a task—and \nthen you want to also remove context. "
    },
    {
      "start": 4265.04,
      "duration": 3.84,
      "text": "You're not dealing with different parts of \nservices of companies or other customers. "
    },
    {
      "start": 4268.88,
      "duration": 2.96,
      "text": "It's just the database, you, \nand a person you're serving. "
    },
    {
      "start": 4271.84,
      "duration": 3.68,
      "text": "It's more closed, it's more \nunderstandable, it's purely digital. "
    },
    {
      "start": 4275.52,
      "duration": 3.28,
      "text": "So I would be looking for those things.\nBut even there, I'm not looking  "
    },
    {
      "start": 4278.8,
      "duration": 2.96,
      "text": "at full automation yet.\nI'm looking for an autonomy slider. "
    },
    {
      "start": 4281.76,
      "duration": 3.84,
      "text": "I expect that we are not going \nto instantly replace people. "
    },
    {
      "start": 4285.6,
      "duration": 3.44,
      "text": "We're going to be swapping in \nAIs that do 80% of the volume. "
    },
    {
      "start": 4289.04,
      "duration": 4.32,
      "text": "They delegate 20% of the volume to humans, \nand humans are supervising teams of five AIs  "
    },
    {
      "start": 4293.36,
      "duration": 6.24,
      "text": "doing the call center work that's more rote.\nI would be looking for new interfaces or new  "
    },
    {
      "start": 4299.6,
      "duration": 4.8,
      "text": "companies that provide some \nlayer that allows you to manage  "
    },
    {
      "start": 4304.4,
      "duration": 4.48,
      "text": "some of these AIs that are not yet perfect.\nThen I would expect that across the economy. "
    },
    {
      "start": 4308.88,
      "duration": 2.64,
      "text": "A lot of jobs are a lot harder \nthan a call center employee. "
    },
    {
      "start": 4312.08,
      "duration": 4.24,
      "text": "With radiologists, I'm totally \nspeculating and I have no idea what  "
    },
    {
      "start": 4316.32,
      "duration": 7.6,
      "text": "the actual workflow of a radiologist involves.\nBut one analogy that might be applicable is when  "
    },
    {
      "start": 4323.92,
      "duration": 5.36,
      "text": "Waymos were first being rolled out, there'd be a \nperson sitting in the front seat, and you just had  "
    },
    {
      "start": 4329.28,
      "duration": 3.36,
      "text": "to have them there to make sure that if something \nwent really wrong, they're there to monitor. "
    },
    {
      "start": 4332.64,
      "duration": 2.8,
      "text": "Even today, people are still watching \nto make sure things are going well. "
    },
    {
      "start": 4335.44,
      "duration": 3.68,
      "text": "Robotaxi, which was just deployed, \nstill has a person inside it. "
    },
    {
      "start": 4339.12,
      "duration": 6.24,
      "text": "Now we could be in a similar situation where \nif you automate 99% of a job, that last 1%  "
    },
    {
      "start": 4345.36,
      "duration": 4.4,
      "text": "the human has to do is incredibly valuable \nbecause it's bottlenecking everything else. "
    },
    {
      "start": 4349.76,
      "duration": 5.28,
      "text": "If it were the case with radiologists, where \nthe person sitting in the front of Waymo has  "
    },
    {
      "start": 4355.04,
      "duration": 4.16,
      "text": "to be specially trained for years in order \nto provide the last 1%, their wages should  "
    },
    {
      "start": 4359.2,
      "duration": 4.48,
      "text": "go up tremendously because they're the \none thing bottlenecking wide deployment. "
    },
    {
      "start": 4363.68,
      "duration": 2.96,
      "text": "Radiologists, I think their wages have \ngone up for similar reasons, if you're  "
    },
    {
      "start": 4366.64,
      "duration": 5.36,
      "text": "the last bottleneck and you're not fungible.\nA Waymo driver might be fungible with others. "
    },
    {
      "start": 4373.52,
      "duration": 4.32,
      "text": "So you might see this thing where your wages \ngo up until you get to 99% and then fall just  "
    },
    {
      "start": 4377.84,
      "duration": 5.12,
      "text": "like that when the last 1% is gone.\nAnd I wonder if we're seeing similar  "
    },
    {
      "start": 4382.96,
      "duration": 3.6,
      "text": "things with radiology or salaries of call \ncenter workers or anything like that. "
    },
    {
      "start": 4387.44,
      "duration": 4.56,
      "text": "That's an interesting question. I don't think \nwe're currently seeing that with radiology. "
    },
    {
      "start": 4395.6,
      "duration": 3.52,
      "text": "I think radiology is not a good example.\nI don't know why Geoff Hinton picked  "
    },
    {
      "start": 4399.12,
      "duration": 5.28,
      "text": "on radiology because I think it's an \nextremely messy, complicated profession. "
    },
    {
      "start": 4405.04,
      "duration": 2.96,
      "text": "I would be a lot more interested in what's \nhappening with call center employees today,  "
    },
    {
      "start": 4408,
      "duration": 4.64,
      "text": "for example, because I would expect a lot \nof the rote stuff to be automatable today. "
    },
    {
      "start": 4412.64,
      "duration": 3.36,
      "text": "I don't have first-level access to it but \nI would be looking for trends of what's  "
    },
    {
      "start": 4416,
      "duration": 4.08,
      "text": "happening with the call center employees.\nSome of the things I would also expect  "
    },
    {
      "start": 4420.08,
      "duration": 4.96,
      "text": "is that maybe they are swapping in AI, but \nthen I would still wait for a year or two  "
    },
    {
      "start": 4425.04,
      "duration": 4.48,
      "text": "because I would potentially expect them to \npull back and rehire some of the people. "
    },
    {
      "start": 4429.52,
      "duration": 3.6,
      "text": "There's been evidence that that's already been \nhappening generally in companies that have been  "
    },
    {
      "start": 4433.12,
      "duration": 6.72,
      "text": "adopting AI, which I think is quite surprising.\nI also found what was really surprising. AGI,  "
    },
    {
      "start": 4439.84,
      "duration": 4.48,
      "text": "right? A thing which would do everything.\nWe'll take out physical work,  "
    },
    {
      "start": 4444.32,
      "duration": 4.72,
      "text": "but it should be able to do all knowledge work.\nWhat you would have naively anticipated is that  "
    },
    {
      "start": 4449.04,
      "duration": 5.2,
      "text": "the way this progression would happen is \nthat you take a little task that a consultant  "
    },
    {
      "start": 4454.24,
      "duration": 5.04,
      "text": "is doing, you take that out of the bucket.\nYou take a little task that an accountant is  "
    },
    {
      "start": 4459.28,
      "duration": 3.04,
      "text": "doing, you take that out of the bucket.\nThen you're just doing this  "
    },
    {
      "start": 4462.32,
      "duration": 2.96,
      "text": "across all knowledge work.\nBut instead, if we do believe we're  "
    },
    {
      "start": 4465.28,
      "duration": 3.6,
      "text": "on the path of AGI with the current paradigm, \nthe progression is very much not like that. "
    },
    {
      "start": 4470.48,
      "duration": 3.52,
      "text": "It does not seem like consultants and accountants \nare getting huge productivity improvements. "
    },
    {
      "start": 4474,
      "duration": 5.92,
      "text": "It's very much like programmers are getting \nmore and more chiseled away at their work. "
    },
    {
      "start": 4479.92,
      "duration": 6.88,
      "text": "If you look at the revenues of these companies, \ndiscounting normal chat revenue—which is similar  "
    },
    {
      "start": 4486.8,
      "duration": 4.88,
      "text": "to Google or something—just looking at \nAPI revenues, it's dominated by coding. "
    },
    {
      "start": 4491.68,
      "duration": 4.48,
      "text": "So this thing which is \"general\", which \nshould be able to do any knowledge work,  "
    },
    {
      "start": 4496.16,
      "duration": 4.08,
      "text": "is just overwhelmingly doing only coding.\nIt's a surprising way that you would  "
    },
    {
      "start": 4500.24,
      "duration": 4.48,
      "text": "expect the AGI to be deployed.\nThere's an interesting point  "
    },
    {
      "start": 4504.72,
      "duration": 7.68,
      "text": "here. I do believe coding is the perfect \nfirst thing for these LLMs and agents. "
    },
    {
      "start": 4512.4,
      "duration": 4.64,
      "text": "That’s because coding has always \nfundamentally worked around text. "
    },
    {
      "start": 4517.04,
      "duration": 3.52,
      "text": "It's computer terminals and text, \nand everything is based around text. "
    },
    {
      "start": 4520.56,
      "duration": 3.92,
      "text": "LLMs, the way they're trained \non the Internet, love text. "
    },
    {
      "start": 4524.48,
      "duration": 4.24,
      "text": "They're perfect text processors, and there's \nall this data out there. It's a perfect fit.  "
    },
    {
      "start": 4529.52,
      "duration": 4,
      "text": "We also have a lot of infrastructure \npre-built for handling code and text. "
    },
    {
      "start": 4533.52,
      "duration": 7.52,
      "text": "For example, we have Visual Studio Code \nor your favorite IDE showing you code,  "
    },
    {
      "start": 4541.04,
      "duration": 4.24,
      "text": "and an agent can plug into that.\nIf an agent has a diff where it made some change,  "
    },
    {
      "start": 4545.28,
      "duration": 5.76,
      "text": "we suddenly have all this code already that shows \nall the differences to a code base using a diff. "
    },
    {
      "start": 4551.04,
      "duration": 4.88,
      "text": "It's almost like we've pre-built a \nlot of the infrastructure for code. "
    },
    {
      "start": 4555.92,
      "duration": 2.96,
      "text": "Contrast that with some of the \nthings that don't enjoy that at all. "
    },
    {
      "start": 4558.88,
      "duration": 4.96,
      "text": "As an example, there are people trying to build \nautomation not for coding, but for slides. "
    },
    {
      "start": 4563.84,
      "duration": 3.52,
      "text": "I saw a company doing slides. That's \nmuch, much harder. The reason it's  "
    },
    {
      "start": 4567.36,
      "duration": 4.48,
      "text": "much harder is because slides are not text.\nSlides are little graphics, they're arranged  "
    },
    {
      "start": 4571.84,
      "duration": 7.04,
      "text": "spatially, and there's a visual component to it.\nSlides don't have this pre-built infrastructure. "
    },
    {
      "start": 4578.88,
      "duration": 5.44,
      "text": "For example, if an agent is to make a change to \nyour slides, how does a thing show you the diff? "
    },
    {
      "start": 4584.32,
      "duration": 2.64,
      "text": "How do you see the diff?\nThere's nothing that shows diffs  "
    },
    {
      "start": 4586.96,
      "duration": 7.44,
      "text": "for slides. Someone has to build it. Some of these \nthings are not amenable to AIs as they are, which  "
    },
    {
      "start": 4594.4,
      "duration": 5.84,
      "text": "are text processors, and code surprisingly is.\nI’m not sure that alone explains it. "
    },
    {
      "start": 4602.16,
      "duration": 7.12,
      "text": "I personally have tried to get LLMs to be useful \nin domains which are just pure language-in,  "
    },
    {
      "start": 4609.28,
      "duration": 6.08,
      "text": "language-out, like rewriting transcripts, \ncoming up with clips based on transcripts. "
    },
    {
      "start": 4617.6,
      "duration": 3.12,
      "text": "It's very plausible that I didn't do \nevery single possible thing I could do. "
    },
    {
      "start": 4620.72,
      "duration": 5.2,
      "text": "I put a bunch of good examples in context, but \nmaybe I should have done some kind of fine-tuning. "
    },
    {
      "start": 4626.56,
      "duration": 7.12,
      "text": "Our mutual friend, Andy Matuschak, told me that \nhe tried 50 billion things to try to get models  "
    },
    {
      "start": 4633.68,
      "duration": 5.76,
      "text": "to be good at writing spaced repetition prompts.\nAgain, very much language-in, language-out tasks,  "
    },
    {
      "start": 4639.44,
      "duration": 3.12,
      "text": "the kind of thing that should be dead \ncenter in the repertoire of these LLMs. "
    },
    {
      "start": 4642.56,
      "duration": 3.36,
      "text": "He tried in-context learning \nwith a few-shot examples. "
    },
    {
      "start": 4645.92,
      "duration": 9.6,
      "text": "He tried supervised fine-tuning and retrieval.\nHe could not get them to make  "
    },
    {
      "start": 4655.52,
      "duration": 4.24,
      "text": "cards to his satisfaction.\nSo I find it striking that even in language-out  "
    },
    {
      "start": 4659.76,
      "duration": 5.84,
      "text": "domains, it's very hard to get a lot of economic \nvalue out of these models separate from coding. "
    },
    {
      "start": 4665.6,
      "duration": 6.4,
      "text": "I don't know what explains it.\nThat makes sense. I'm not  "
    },
    {
      "start": 4672,
      "duration": 5.28,
      "text": "saying that anything text is trivial.\nI do think that code is pretty structured. "
    },
    {
      "start": 4678.72,
      "duration": 6.16,
      "text": "Text is maybe a lot more flowery, and there's \na lot more entropy in text, I would say. "
    },
    {
      "start": 4684.88,
      "duration": 5.52,
      "text": "I don't know how else to put it.\nAlso code is hard, and so people feel quite  "
    },
    {
      "start": 4690.4,
      "duration": 8.88,
      "text": "empowered by LLMs, even from simple knowledge.\nI don't know that I have a very good answer. "
    },
    {
      "start": 4699.28,
      "duration": 5.04,
      "text": "Obviously, text makes it much, much easier, \nbut it doesn't mean that all text is trivial. "
    },
    {
      "start": 4705.2,
      "duration": 4.56,
      "text": "How do you think about superintelligence?\nDo you expect it to feel qualitatively different  "
    },
    {
      "start": 4709.76,
      "duration": 7.28,
      "text": "from normal humans or human companies?\nI see it as a progression  "
    },
    {
      "start": 4717.04,
      "duration": 5.28,
      "text": "of automation in society.\nExtrapolating the trend of computing, there will  "
    },
    {
      "start": 4722.32,
      "duration": 4.32,
      "text": "be a gradual automation of a lot of things, and \nsuperintelligence will an extrapolation of that. "
    },
    {
      "start": 4727.6,
      "duration": 2.96,
      "text": "We expect more and more autonomous \nentities over time that are doing a lot  "
    },
    {
      "start": 4730.56,
      "duration": 5.52,
      "text": "of the digital work and then eventually even \nthe physical work some amount of time later. "
    },
    {
      "start": 4736.08,
      "duration": 4.48,
      "text": "Basically I see it as just \nautomation, roughly speaking. "
    },
    {
      "start": 4740.56,
      "duration": 2.56,
      "text": "But automation includes the things humans \ncan already do, and superintelligence  "
    },
    {
      "start": 4743.12,
      "duration": 2.8,
      "text": "implies things humans can’t do.\nBut one of the things that people  "
    },
    {
      "start": 4745.92,
      "duration": 4,
      "text": "do is invent new things, which I would just \nput into the automation if that makes sense. "
    },
    {
      "start": 4750.96,
      "duration": 7.44,
      "text": "But I guess, less abstractly and more \nqualitatively, do you expect something  "
    },
    {
      "start": 4758.4,
      "duration": 8.4,
      "text": "to feel like… Because this thing can either think \nso fast, or has so many copies, or the copies can  "
    },
    {
      "start": 4766.8,
      "duration": 8.4,
      "text": "merge back into themselves, or is much smarter, \nany number of advantages an AI might have, will  "
    },
    {
      "start": 4776,
      "duration": 3.92,
      "text": "the civilization in which these AIs exist\njust feel qualitatively different from humans? "
    },
    {
      "start": 4779.92,
      "duration": 5.04,
      "text": "I think it will. It is fundamentally automation, \nbut it will be extremely foreign. It will look  "
    },
    {
      "start": 4784.96,
      "duration": 6.72,
      "text": "really strange. Like you mentioned, we can run \nall of this on a computer cluster and much faster. "
    },
    {
      "start": 4793.04,
      "duration": 5.36,
      "text": "Some of the scenarios that I start to get \nnervous about when the world looks like  "
    },
    {
      "start": 4798.4,
      "duration": 3.36,
      "text": "that is this gradual loss of control \nand understanding of what's happening. "
    },
    {
      "start": 4801.76,
      "duration": 4.32,
      "text": "I think that's the most likely outcome, that \nthere will be a gradual loss of understanding. "
    },
    {
      "start": 4807.52,
      "duration": 2.88,
      "text": "We'll gradually layer all this stuff \neverywhere, and there will be fewer  "
    },
    {
      "start": 4810.4,
      "duration": 4.24,
      "text": "and fewer people who understand it.\nThen there will be a gradual loss of  "
    },
    {
      "start": 4814.64,
      "duration": 4.72,
      "text": "control and understanding of what's happening.\nThat to me seems the most likely outcome of how  "
    },
    {
      "start": 4819.36,
      "duration": 2.64,
      "text": "all this stuff will go down.\nLet me probe on that a bit. "
    },
    {
      "start": 4822,
      "duration": 5.52,
      "text": "It's not clear to me that loss of control and \nloss of understanding are the same things. "
    },
    {
      "start": 4827.52,
      "duration": 9.52,
      "text": "A board of directors at TSMC, Intel—name a random \ncompany—they're just prestigious 80-year-olds. "
    },
    {
      "start": 4837.04,
      "duration": 3.76,
      "text": "They have very little understanding, and maybe \nthey don't practically actually have control. "
    },
    {
      "start": 4843.2,
      "duration": 0.56,
      "text": "A better example  "
    },
    {
      "start": 4843.76,
      "duration": 3.84,
      "text": "is the President of the United States.\nThe President has a lot of fucking power. "
    },
    {
      "start": 4848.8,
      "duration": 4.24,
      "text": "I'm not trying to make a good statement \nabout the current operant, or maybe I am,  "
    },
    {
      "start": 4853.04,
      "duration": 3.44,
      "text": "but the actual level of understanding is \nvery different from the level of control. "
    },
    {
      "start": 4856.48,
      "duration": 8.72,
      "text": "I think that's fair. That's a good \npushback. I think I expect loss of both. "
    },
    {
      "start": 4865.2,
      "duration": 3.68,
      "text": "How come? Loss of understanding is \nobvious, but why loss of control? "
    },
    {
      "start": 4870.16,
      "duration": 4.24,
      "text": "We're really far into a territory where I \ndon't know what this looks like, but if I  "
    },
    {
      "start": 4874.4,
      "duration": 7.6,
      "text": "were to write sci-fi novels, they would look along \nthe lines of not even a single entity that takes  "
    },
    {
      "start": 4882,
      "duration": 4.48,
      "text": "over everything, but multiple competing entities \nthat gradually become more and more autonomous. "
    },
    {
      "start": 4887.36,
      "duration": 2.64,
      "text": "Some of them go rogue and \nthe others fight them off. "
    },
    {
      "start": 4891.04,
      "duration": 6.56,
      "text": "It's this hot pot of completely autonomous \nactivity that we've delegated to. "
    },
    {
      "start": 4897.6,
      "duration": 6,
      "text": "I feel it would have that flavor.\nIt is not the fact that they are smarter  "
    },
    {
      "start": 4903.6,
      "duration": 4.24,
      "text": "than us that is resulting in the loss of control.\nIt's the fact that they are competing with each  "
    },
    {
      "start": 4907.84,
      "duration": 6.56,
      "text": "other, and whatever arises out of that \ncompetition leads to the loss of control. "
    },
    {
      "start": 4918.96,
      "duration": 6,
      "text": "A lot of these things, they will be \ntools to people, they're acting on  "
    },
    {
      "start": 4924.96,
      "duration": 2.48,
      "text": "behalf of people or something like that.\nSo maybe those people are in control,  "
    },
    {
      "start": 4927.44,
      "duration": 4.8,
      "text": "but maybe it's a loss of control overall for \nsociety in the sense of outcomes we want. "
    },
    {
      "start": 4933.84,
      "duration": 5.52,
      "text": "You have entities acting on behalf of individuals \nthat are still roughly seen as out of control. "
    },
    {
      "start": 4940.08,
      "duration": 3.92,
      "text": "This is a question I should have asked earlier.\nWe were talking about how currently it feels like  "
    },
    {
      "start": 4944,
      "duration": 5.2,
      "text": "when you're doing AI engineering or AI research, \nthese models are more in the category of compiler  "
    },
    {
      "start": 4949.2,
      "duration": 5.44,
      "text": "rather than in the category of a replacement.\nAt some point, if you have AGI,  "
    },
    {
      "start": 4954.64,
      "duration": 4.08,
      "text": "it should be able to do what you do.\nDo you feel like having a million  "
    },
    {
      "start": 4958.72,
      "duration": 4.32,
      "text": "copies of you in parallel results in \nsome huge speed-up of AI progress? "
    },
    {
      "start": 4963.76,
      "duration": 5.596,
      "text": "If that does happen, do you expect to see an \nintelligence explosion once we have a true AGI? "
    },
    {
      "start": 4969.356,
      "duration": 6.724,
      "text": "I'm not talking about LLMs today.\nI do, but it's business as usual because  "
    },
    {
      "start": 4976.08,
      "duration": 2.96,
      "text": "we're in an intelligence explosion \nalready and have been for decades. "
    },
    {
      "start": 4980,
      "duration": 3.6,
      "text": "It's basically the GDP curve that is \nan exponential weighted sum over so  "
    },
    {
      "start": 4983.6,
      "duration": 2.56,
      "text": "many aspects of the industry.\nEverything is gradually being  "
    },
    {
      "start": 4986.16,
      "duration": 4.24,
      "text": "automated and has been for hundreds of years.\nThe Industrial Revolution is automation and  "
    },
    {
      "start": 4990.4,
      "duration": 2.64,
      "text": "some of the physical components and \ntool building and all this stuff. "
    },
    {
      "start": 4993.04,
      "duration": 3.68,
      "text": "Compilers are early software \nautomation, et cetera. "
    },
    {
      "start": 4996.72,
      "duration": 5.04,
      "text": "We've been recursively self-improving \nand exploding for a long time. "
    },
    {
      "start": 5001.76,
      "duration": 5.68,
      "text": "Another way to see it is that Earth was a pretty \nboring place if you don't look at the biomechanics  "
    },
    {
      "start": 5007.44,
      "duration": 6,
      "text": "and so on, and looked very similar.\nIf you look from space, we're in the  "
    },
    {
      "start": 5013.44,
      "duration": 4.08,
      "text": "middle of this firecracker event, \nbut we're seeing it in slow motion. "
    },
    {
      "start": 5018.32,
      "duration": 4.48,
      "text": "I definitely feel like this has \nalready happened for a very long time. "
    },
    {
      "start": 5022.8,
      "duration": 4.4,
      "text": "Again, I don't see AI as a distinct \ntechnology with respect to what has  "
    },
    {
      "start": 5027.2,
      "duration": 3.6,
      "text": "already been happening for a long time.\nYou think it's continuous with this  "
    },
    {
      "start": 5030.8,
      "duration": 3.04,
      "text": "hyper-exponential trend?\nYes. That's why this was  "
    },
    {
      "start": 5033.84,
      "duration": 3.68,
      "text": "very interesting to me, because I was \ntrying to find AI in the GDP for a while. "
    },
    {
      "start": 5037.52,
      "duration": 3.6,
      "text": "I thought that GDP should go up.\nBut then I looked at some of the  "
    },
    {
      "start": 5041.12,
      "duration": 2.96,
      "text": "other technologies that I thought \nwere very transformative, like  "
    },
    {
      "start": 5044.72,
      "duration": 3.76,
      "text": "computers or mobile phones or et cetera.\nYou can't find them in GDP. GDP is the same  "
    },
    {
      "start": 5048.48,
      "duration": 5.36,
      "text": "exponential. Even the early iPhone didn't have the \nApp Store, and it didn't have a lot of the bells  "
    },
    {
      "start": 5053.84,
      "duration": 4.16,
      "text": "and whistles that the modern iPhone has.\nSo even though we think of 2008,  "
    },
    {
      "start": 5058,
      "duration": 3.52,
      "text": "when the iPhone came out, as this major \nseismic change, it's actually not. "
    },
    {
      "start": 5061.52,
      "duration": 3.84,
      "text": "Everything is so spread out and it so \nslowly diffuses that everything ends up  "
    },
    {
      "start": 5065.36,
      "duration": 3.36,
      "text": "being averaged up into the same exponential.\nIt's the exact same thing with computers. "
    },
    {
      "start": 5068.72,
      "duration": 2.72,
      "text": "You can't find them in the GDP \nlike, \"Oh, we have computers now.\" "
    },
    {
      "start": 5071.44,
      "duration": 2.32,
      "text": "That's not what happened, because \nit's such slow progression. "
    },
    {
      "start": 5073.76,
      "duration": 3.84,
      "text": "With AI we're going to see the exact same thing. \nIt's just more automation. It allows us to write  "
    },
    {
      "start": 5077.6,
      "duration": 4.24,
      "text": "different kinds of programs that we couldn't write \nbefore, but AI is still fundamentally a program. "
    },
    {
      "start": 5082.56,
      "duration": 4.32,
      "text": "It's a new kind of computer and \na new kind of computing system. "
    },
    {
      "start": 5086.88,
      "duration": 2.96,
      "text": "But it has all these problems, \nit's going to diffuse over time,  "
    },
    {
      "start": 5089.84,
      "duration": 2.24,
      "text": "and it's still going to add \nup to the same exponential. "
    },
    {
      "start": 5092.08,
      "duration": 4.32,
      "text": "We're still going to have an exponential \nthat's going to get extremely vertical. "
    },
    {
      "start": 5096.4,
      "duration": 3.12,
      "text": "It's going to be very foreign to \nlive in that kind of an environment. "
    },
    {
      "start": 5099.52,
      "duration": 6.08,
      "text": "Are you saying that, if you look at the trend \nbefore the Industrial Revolution to now,  "
    },
    {
      "start": 5105.6,
      "duration": 5.44,
      "text": "you have a hyper-exponential where you go \nfrom 0% growth to then 10,000 years ago,  "
    },
    {
      "start": 5111.04,
      "duration": 4.24,
      "text": "0.02% growth, and to now when we're at 2% \ngrowth. That's a hyper-exponential. Are you  "
    },
    {
      "start": 5115.28,
      "duration": 4.88,
      "text": "saying if you're charting AI on there, then \nAI takes you to 20% growth or 200% growth? "
    },
    {
      "start": 5120.16,
      "duration": 3.44,
      "text": "Or are you saying that if you look at \nthe last 300 years, what you've been  "
    },
    {
      "start": 5123.6,
      "duration": 3.6,
      "text": "seeing is that you have technology after \ntechnology—computers, electrification,  "
    },
    {
      "start": 5127.2,
      "duration": 6.32,
      "text": "steam engines, railways, et cetera—but the \nrate of growth is the exact same, it's 2%. "
    },
    {
      "start": 5133.52,
      "duration": 5.36,
      "text": "Are you saying the rate of growth will go up?\nThe rate of growth has also stayed  "
    },
    {
      "start": 5138.88,
      "duration": 2.64,
      "text": "roughly constant, right?\nOnly over the last 200, 300 years. "
    },
    {
      "start": 5141.52,
      "duration": 2.8,
      "text": "But over the course of \nhuman history it's exploded. "
    },
    {
      "start": 5144.32,
      "duration": 5.52,
      "text": "It's gone from 0% to faster, faster, \nfaster. Industrial explosion, 2%. "
    },
    {
      "start": 5151.12,
      "duration": 3.36,
      "text": "For a while I tried to find AI \nor look for AI in the GDP curve,  "
    },
    {
      "start": 5154.48,
      "duration": 3.76,
      "text": "and I've convinced myself that this is false.\nEven when people talk about recursive  "
    },
    {
      "start": 5158.24,
      "duration": 3.44,
      "text": "self-improvement and labs and stuff \nlike that, this is business as usual. "
    },
    {
      "start": 5161.68,
      "duration": 4,
      "text": "Of course it's going to recursively self-improve, \nand it's been recursively self-improving. "
    },
    {
      "start": 5165.68,
      "duration": 5.44,
      "text": "LLMs allow the engineers to work much more \nefficiently to build the next round of LLM,  "
    },
    {
      "start": 5171.12,
      "duration": 3.76,
      "text": "and a lot more of the components are \nbeing automated and tuned and et cetera. "
    },
    {
      "start": 5174.88,
      "duration": 4.24,
      "text": "All the engineers having access \nto Google Search is part of it. "
    },
    {
      "start": 5179.12,
      "duration": 3.92,
      "text": "All the engineers having an IDE, all of them \nhaving autocomplete or having Claude code,  "
    },
    {
      "start": 5183.04,
      "duration": 7.84,
      "text": "et cetera, it's all just part of the same \nspeed-up of the whole thing. It's just so smooth. "
    },
    {
      "start": 5190.88,
      "duration": 2.96,
      "text": "Just to clarify, you're saying that \nthe rate of growth will not change. "
    },
    {
      "start": 5195.44,
      "duration": 4,
      "text": "The intelligence explosion will show up as \nit just enabled us to continue staying on the  "
    },
    {
      "start": 5199.44,
      "duration": 3.12,
      "text": "2% growth trajectory, just as the Internet \nhelped us stay on the 2% growth trajectory. "
    },
    {
      "start": 5202.56,
      "duration": 2.96,
      "text": "Yes, my expectation is that \nit stays in the same pattern. "
    },
    {
      "start": 5207.92,
      "duration": 7.84,
      "text": "Just to throw the opposite argument against you, \nmy expectation is that it blows up because I think  "
    },
    {
      "start": 5215.76,
      "duration": 5.04,
      "text": "true AGI—and I'm not talking about LLM coding \nbots, I'm talking about actual replacement of a  "
    },
    {
      "start": 5220.8,
      "duration": 6.48,
      "text": "human in a server—is qualitatively different \nfrom these other productivity-improving  "
    },
    {
      "start": 5227.28,
      "duration": 5.84,
      "text": "technologies because it's labor itself.\nI think we live in a very labor-constrained world. "
    },
    {
      "start": 5233.12,
      "duration": 4.08,
      "text": "If you talk to any startup founder or any person, \nyou can be like, what do you need more of? You  "
    },
    {
      "start": 5237.2,
      "duration": 5.6,
      "text": "need really talented people. And if you have \nbillions of extra people who are inventing stuff,  "
    },
    {
      "start": 5242.8,
      "duration": 5.6,
      "text": "integrating themselves, making companies bottom \nstart to finish, that feels qualitatively  "
    },
    {
      "start": 5248.4,
      "duration": 4,
      "text": "different from a single technology.\nIt's as if you get 10 billion  "
    },
    {
      "start": 5252.4,
      "duration": 5.36,
      "text": "extra people on the planet.\nMaybe a counterpoint. I'm pretty willing  "
    },
    {
      "start": 5257.76,
      "duration": 4.24,
      "text": "to be convinced one way or another on this point.\nBut I will say, for example, computing is labor.  "
    },
    {
      "start": 5262,
      "duration": 3.52,
      "text": "Computing was labor. Computers, a lot \nof jobs disappeared because computers  "
    },
    {
      "start": 5265.52,
      "duration": 4.72,
      "text": "are automating a bunch of digital information \nprocessing that you now don't need a human for. "
    },
    {
      "start": 5270.24,
      "duration": 7.6,
      "text": "So computers are labor, and that has played out.\nSelf-driving as an example is also computers doing  "
    },
    {
      "start": 5277.84,
      "duration": 4.24,
      "text": "labor. That's already been playing \nout. It's still business as usual. "
    },
    {
      "start": 5282.8,
      "duration": 5.2,
      "text": "You have a machine which is spitting out more \nthings like that at potentially faster pace. "
    },
    {
      "start": 5288,
      "duration": 3.12,
      "text": "Historically, we have examples \nof the growth regime changing  "
    },
    {
      "start": 5291.12,
      "duration": 7.28,
      "text": "where you went from 0.2% growth to 2% growth.\nIt seems very plausible to me that a machine which  "
    },
    {
      "start": 5298.4,
      "duration": 4.8,
      "text": "is then spitting out the next self-driving \ncar and the next Internet and whatever… "
    },
    {
      "start": 5303.2,
      "duration": 4.32,
      "text": "I see where it's coming from.\nAt the same time, I do feel like  "
    },
    {
      "start": 5307.52,
      "duration": 4.32,
      "text": "people make this assumption of, \"We \nhave God in a box, and now it can do  "
    },
    {
      "start": 5311.84,
      "duration": 4.4,
      "text": "everything,\" and it just won't look like that.\nIt's going to be able to do some of the things. "
    },
    {
      "start": 5316.24,
      "duration": 3.12,
      "text": "It's going to fail at some other things.\nIt's going to be gradually put into society,  "
    },
    {
      "start": 5319.36,
      "duration": 4.56,
      "text": "and we'll end up with the same pattern. That \nis my prediction. This assumption of suddenly  "
    },
    {
      "start": 5323.92,
      "duration": 5.28,
      "text": "having a completely intelligent, fully flexible, \nfully general human in a box, and we can dispense  "
    },
    {
      "start": 5329.2,
      "duration": 6.64,
      "text": "it at arbitrary problems in society, I don't \nthink that we will have this discrete change. "
    },
    {
      "start": 5337.6,
      "duration": 5.28,
      "text": "I think we'll arrive at the same kind of \ngradual diffusion of this across the industry. "
    },
    {
      "start": 5343.44,
      "duration": 4.64,
      "text": "It often ends up being misleading \nin these conversations. "
    },
    {
      "start": 5349.28,
      "duration": 3.68,
      "text": "I don't like to use the word intelligence in \nthis context because intelligence implies you  "
    },
    {
      "start": 5352.96,
      "duration": 5.92,
      "text": "think there'll be a single superintelligence \nsitting in a server and it'll divine how  "
    },
    {
      "start": 5358.88,
      "duration": 3.92,
      "text": "to come up with new technologies and \ninventions that cause this explosion. "
    },
    {
      "start": 5362.8,
      "duration": 3.04,
      "text": "That's not what I'm imagining \nwhen I'm imagining 20% growth. "
    },
    {
      "start": 5365.84,
      "duration": 7.44,
      "text": "I'm imagining that there are billions of \nvery smart human-like minds, potentially,  "
    },
    {
      "start": 5373.28,
      "duration": 3.2,
      "text": "or that's all that's required.\nBut the fact that there's hundreds  "
    },
    {
      "start": 5376.48,
      "duration": 5.28,
      "text": "of millions of them, billions of them, each \nindividually making new products, figuring  "
    },
    {
      "start": 5381.76,
      "duration": 4.88,
      "text": "out how to integrate themselves into the economy.\nIf a highly experienced smart immigrant came to  "
    },
    {
      "start": 5386.64,
      "duration": 2.72,
      "text": "the country, you wouldn't need to figure out how \nwe integrate them in the economy. They figure it  "
    },
    {
      "start": 5389.36,
      "duration": 6.08,
      "text": "out. They could start a company, they could make \ninventions, or increase productivity in the world. "
    },
    {
      "start": 5395.44,
      "duration": 6,
      "text": "We have examples, even in the current regime, \nof places that have had 10-20% economic growth. "
    },
    {
      "start": 5401.44,
      "duration": 4.4,
      "text": "If you just have a lot of people and \nless capital in comparison to the people,  "
    },
    {
      "start": 5405.84,
      "duration": 6.24,
      "text": "you can have Hong Kong or Shenzhen or \nwhatever with decades of 10% plus growth. "
    },
    {
      "start": 5413.12,
      "duration": 4.56,
      "text": "There's a lot of really smart people who are \nready to make use of the resources and do  "
    },
    {
      "start": 5417.68,
      "duration": 4.8,
      "text": "this period of catch-up because we've had this \ndiscontinuity, and I think AI might be similar. "
    },
    {
      "start": 5424.48,
      "duration": 3.6,
      "text": "I understand, but I still think that \nyou're presupposing some discrete jump. "
    },
    {
      "start": 5428.08,
      "duration": 3.36,
      "text": "There's some unlock that we're waiting to claim.\nAnd suddenly we're going to have  "
    },
    {
      "start": 5431.44,
      "duration": 3.52,
      "text": "geniuses in data centers.\nI still think you're presupposing  "
    },
    {
      "start": 5434.96,
      "duration": 4.64,
      "text": "some discrete jump that has no historical \nprecedent that I can't find in any of the  "
    },
    {
      "start": 5439.6,
      "duration": 4.08,
      "text": "statistics and that I think probably won't happen.\nI mean, the Industrial Revolution is such a jump. "
    },
    {
      "start": 5443.68,
      "duration": 5.76,
      "text": "You went from 0.2% growth to 2% growth.\nI'm just saying you'll see another jump like that. "
    },
    {
      "start": 5449.44,
      "duration": 4.32,
      "text": "I'm a little bit suspicious, \nI would have to take a look. "
    },
    {
      "start": 5453.76,
      "duration": 4.32,
      "text": "For example, some of the logs are not very \ngood from before the Industrial Revolution. "
    },
    {
      "start": 5459.92,
      "duration": 4.72,
      "text": "I'm a bit suspicious of it but \nI don't have strong opinions. "
    },
    {
      "start": 5464.64,
      "duration": 2.8,
      "text": "You're saying that this was a singular \nevent that was extremely magical. "
    },
    {
      "start": 5467.44,
      "duration": 1.84,
      "text": "You're saying that maybe there's going \nto be another event that's going to  "
    },
    {
      "start": 5469.28,
      "duration": 3.68,
      "text": "be just like that, extremely magical.\nIt will break the paradigm, and so on. "
    },
    {
      "start": 5472.96,
      "duration": 4.16,
      "text": "I actually don't think… The crucial thing with the \nIndustrial Revolution was that it was not magical. "
    },
    {
      "start": 5478.48,
      "duration": 9.2,
      "text": "If you just zoomed in, what you would see in 1770 \nor 1870 is not that there was some key invention. "
    },
    {
      "start": 5488.4,
      "duration": 4.56,
      "text": "But at the same time, you did move the \neconomy to a regime where the progress  "
    },
    {
      "start": 5492.96,
      "duration": 4.88,
      "text": "was much faster and the exponential 10x'd.\nI expect a similar thing from AI where it's  "
    },
    {
      "start": 5497.84,
      "duration": 4.63,
      "text": "not like there's going to be a single moment \nwhere we've made the crucial invention. "
    },
    {
      "start": 5502.47,
      "duration": 3.29,
      "text": "It’s an overhang that's being unlocked.\nLike maybe there's a new energy source. "
    },
    {
      "start": 5505.76,
      "duration": 4,
      "text": "There's some unlock—in this case, some kind of \na cognitive capacity—and there's an overhang of  "
    },
    {
      "start": 5509.76,
      "duration": 2.4,
      "text": "cognitive work to do.\nThat's right. "
    },
    {
      "start": 5512.16,
      "duration": 3.6,
      "text": "You're expecting that overhang to be filled by \nthis new technology when it crosses the threshold. "
    },
    {
      "start": 5516.64,
      "duration": 4.48,
      "text": "Maybe one way to think about it is \nthroughout history, a lot of growth  "
    },
    {
      "start": 5521.12,
      "duration": 5.28,
      "text": "comes because people come up with ideas, \nand then people are out there doing stuff to  "
    },
    {
      "start": 5526.4,
      "duration": 4.64,
      "text": "execute those ideas and make valuable output.\nThrough most of this time, the population has  "
    },
    {
      "start": 5531.04,
      "duration": 3.12,
      "text": "been exploding. That has been driving \ngrowth. For the last 50 years, people  "
    },
    {
      "start": 5534.16,
      "duration": 3.36,
      "text": "have argued that growth has stagnated.\nThe population in frontier countries  "
    },
    {
      "start": 5537.52,
      "duration": 2.32,
      "text": "has also stagnated.\nI think we go back to  "
    },
    {
      "start": 5539.84,
      "duration": 7.2,
      "text": "the exponential growth in population that \ncauses hyper-exponential growth in output. "
    },
    {
      "start": 5548.96,
      "duration": 3.2,
      "text": "It's really hard to tell. I \nunderstand that viewpoint. I  "
    },
    {
      "start": 5552.16,
      "duration": 68.4,
      "text": "don't intuitively feel that viewpoint.\nYou recommended Nick Lane's book to me. "
    },
    {
      "start": 5620.56,
      "duration": 3.84,
      "text": "On that basis, I also found it super \ninteresting and I interviewed him. "
    },
    {
      "start": 5625.2,
      "duration": 3.92,
      "text": "I have some questions about thinking about \nintelligence and evolutionary history. "
    },
    {
      "start": 5629.12,
      "duration": 5.28,
      "text": "Now that you, over the last 20 years of doing AI \nresearch, you maybe have a more tangible sense of  "
    },
    {
      "start": 5634.4,
      "duration": 7.28,
      "text": "what intelligence is, what it takes to develop it.\nAre you more or less surprised as a result that  "
    },
    {
      "start": 5641.68,
      "duration": 10.48,
      "text": "evolution just spontaneously stumbled upon it?\nI love Nick Lane's books. I was just listening  "
    },
    {
      "start": 5652.16,
      "duration": 3.12,
      "text": "to his podcast on the way up here.\nWith respect to intelligence and its  "
    },
    {
      "start": 5655.28,
      "duration": 6.96,
      "text": "evolution, it's very, very recent.\nI am surprised that it evolved. "
    },
    {
      "start": 5663.04,
      "duration": 2.08,
      "text": "I find it fascinating to think \nabout all the worlds out there. "
    },
    {
      "start": 5665.12,
      "duration": 2.32,
      "text": "Say there's a thousand planets \nlike Earth and what they look like. "
    },
    {
      "start": 5667.44,
      "duration": 2.16,
      "text": "I think Nick Lane was here talking \nabout some of the earliest parts. "
    },
    {
      "start": 5670.88,
      "duration": 3.2,
      "text": "He expects very similar life \nforms, roughly speaking,  "
    },
    {
      "start": 5674.08,
      "duration": 4.4,
      "text": "and bacteria-like things in most of them.\nThere are a few breaks in there. "
    },
    {
      "start": 5679.92,
      "duration": 4,
      "text": "The evolution of intelligence intuitively feels \nto me like it should be a fairly rare event. "
    },
    {
      "start": 5685.92,
      "duration": 3.36,
      "text": "Maybe you should base it on \nhow long something has existed. "
    },
    {
      "start": 5689.28,
      "duration": 3.6,
      "text": "If bacteria were around for 2 billion years \nand nothing happened, then going to eukaryote  "
    },
    {
      "start": 5692.88,
      "duration": 7.12,
      "text": "is probably pretty hard because bacteria came \nup quite early in Earth's evolution or history. "
    },
    {
      "start": 5702.24,
      "duration": 2.48,
      "text": "How long have we had animals?\nMaybe a couple hundred million years,  "
    },
    {
      "start": 5704.72,
      "duration": 3.6,
      "text": "multicellular animals that \nrun around, crawl, et cetera. "
    },
    {
      "start": 5708.32,
      "duration": 7.52,
      "text": "That’s maybe 10% of Earth's lifespan.\nMaybe on that timescale it's not too tricky. "
    },
    {
      "start": 5718,
      "duration": 2.48,
      "text": "It's still surprising to me, \nintuitively, that it developed. "
    },
    {
      "start": 5720.48,
      "duration": 4.16,
      "text": "I would maybe expect just a lot of animal-like \nlife forms doing animal-like things. "
    },
    {
      "start": 5724.64,
      "duration": 3.44,
      "text": "The fact that you can get something \nthat creates culture and knowledge  "
    },
    {
      "start": 5728.08,
      "duration": 7.36,
      "text": "and accumulates it is surprising to me.\nThere's a couple of interesting follow-ups. "
    },
    {
      "start": 5735.44,
      "duration": 6.48,
      "text": "If you buy the Sutton perspective that the \ncrux of intelligence is animal intelligence…  "
    },
    {
      "start": 5741.92,
      "duration": 3.2,
      "text": "The quote he said is \"If you got to the \nsquirrel, you'd be most of the way to AGI.\" "
    },
    {
      "start": 5746.56,
      "duration": 4.64,
      "text": "We got to squirrel intelligence right after \nthe Cambrian explosion 600 million years ago. "
    },
    {
      "start": 5751.2,
      "duration": 4.8,
      "text": "It seems like what instigated that was the \noxygenation event 600 million years ago. "
    },
    {
      "start": 5756,
      "duration": 5.6,
      "text": "But immediately the intelligence algorithm \nwas there to make the squirrel intelligence. "
    },
    {
      "start": 5762.48,
      "duration": 3.76,
      "text": "It's suggestive that animal \nintelligence was like that. "
    },
    {
      "start": 5766.24,
      "duration": 2.88,
      "text": "As soon as you had the oxygen in the \nenvironment, you had the eukaryote,  "
    },
    {
      "start": 5769.12,
      "duration": 5.28,
      "text": "you could just get the algorithm.\nMaybe it was an accident that  "
    },
    {
      "start": 5774.4,
      "duration": 3.04,
      "text": "evolution stumbled upon it so fast, \nbut I don't know if that suggests that  "
    },
    {
      "start": 5778.48,
      "duration": 4.72,
      "text": "at the end it's going to be quite simple.\nIt's so hard to tell with any of this stuff. "
    },
    {
      "start": 5783.2,
      "duration": 2.96,
      "text": "You can base it a bit on how long \nsomething has existed or how long  "
    },
    {
      "start": 5786.16,
      "duration": 4.08,
      "text": "it feels like something has been bottlenecked.\nNick Lane is very good about describing this very  "
    },
    {
      "start": 5790.24,
      "duration": 4,
      "text": "apparent bottleneck in bacteria and archaea.\nFor two billion years, nothing happened. "
    },
    {
      "start": 5794.24,
      "duration": 6.96,
      "text": "There’s extreme diversity of biochemistry, \nand yet nothing grows to become animals.  "
    },
    {
      "start": 5801.2,
      "duration": 5.2,
      "text": "Two billion years. I don't know that we've \nseen exactly that kind of an equivalent with  "
    },
    {
      "start": 5806.4,
      "duration": 4.8,
      "text": "animals and intelligence, to your point.\nWe could also look at it with respect  "
    },
    {
      "start": 5811.2,
      "duration": 4.4,
      "text": "to how many times we think certain \nintelligence has individually sprung up. "
    },
    {
      "start": 5815.6,
      "duration": 7.52,
      "text": "That's a really good thing to investigate.\nOne thought on that. There's hominid intelligence,  "
    },
    {
      "start": 5823.12,
      "duration": 4.72,
      "text": "and then there's bird intelligence.\nRavens, etc., are extremely clever,  "
    },
    {
      "start": 5827.84,
      "duration": 5.92,
      "text": "but their brain parts are quite distinct, \nand we don't have that much in common. "
    },
    {
      "start": 5833.76,
      "duration": 5.12,
      "text": "That's a slight indication of maybe \nintelligence springing up a few times. "
    },
    {
      "start": 5838.88,
      "duration": 7.68,
      "text": "In that case, you'd expect it more frequently.\nA former guest, Gwern, and Carl Shulman, they’ve  "
    },
    {
      "start": 5846.56,
      "duration": 6.16,
      "text": "made a really interesting point about that.\nTheir perspective is that the scalable algorithm  "
    },
    {
      "start": 5852.72,
      "duration": 6.88,
      "text": "which humans have and primates have, arose in \nbirds as well, and maybe other times as well. "
    },
    {
      "start": 5859.6,
      "duration": 6.48,
      "text": "But humans found an evolutionary niche which \nrewarded marginal increases in intelligence  "
    },
    {
      "start": 5867.28,
      "duration": 5.04,
      "text": "and also had a scalable brain algorithm that \ncould achieve those increases in intelligence. "
    },
    {
      "start": 5873.44,
      "duration": 3.76,
      "text": "For example, if a bird had a bigger brain, \nit would just collapse out of the air. "
    },
    {
      "start": 5877.2,
      "duration": 3.36,
      "text": "It's very smart for the size of \nits brain, but it's not in a niche  "
    },
    {
      "start": 5880.56,
      "duration": 5.52,
      "text": "which rewards the brain getting bigger.\nIt’s maybe similar to some really smart… "
    },
    {
      "start": 5888.88,
      "duration": 1.28,
      "text": "Like dolphins?\nExaclty, humans, we have hands that  "
    },
    {
      "start": 5890.16,
      "duration": 4.16,
      "text": "reward being able to learn how to do tool use.\nWe can externalize digestion, more energy to  "
    },
    {
      "start": 5894.32,
      "duration": 5.12,
      "text": "the brain, and that kicks off the flywheel.\nAlso stuff to work with. I'm guessing it would  "
    },
    {
      "start": 5899.44,
      "duration": 8.64,
      "text": "be harder if I were a dolphin. How do you have \nfire? The universe of things you can do in water,  "
    },
    {
      "start": 5908.88,
      "duration": 4.08,
      "text": "inside water, is probably lower than \nwhat you can do on land, just chemically. "
    },
    {
      "start": 5913.84,
      "duration": 4.8,
      "text": "I do agree with this viewpoint of these niches \nand what's being incentivized. I still find it  "
    },
    {
      "start": 5918.64,
      "duration": 7.28,
      "text": "miraculous. I would have expected things to \nget stuck on animals with bigger muscles. "
    },
    {
      "start": 5927.04,
      "duration": 4.48,
      "text": "Going through intelligence is a \nreally fascinating breaking point. "
    },
    {
      "start": 5931.52,
      "duration": 4.64,
      "text": "The way Gwern put it is the reason it was so hard \nis that it's a very tight line between being in  "
    },
    {
      "start": 5936.16,
      "duration": 7.04,
      "text": "a situation where something is so important \nto learn that it's not worth distilling the  "
    },
    {
      "start": 5943.2,
      "duration": 6.8,
      "text": "exact right circuits directly back into your DNA, \nversus it's not important enough to learn at all. "
    },
    {
      "start": 5950,
      "duration": 7.2,
      "text": "It has to be something that incentivizes \nbuilding the algorithm to learn in a lifetime. "
    },
    {
      "start": 5957.2,
      "duration": 4.64,
      "text": "You have to incentivize some kind of adaptability.\nYou want environments that are unpredictable  "
    },
    {
      "start": 5961.84,
      "duration": 2.8,
      "text": "so evolution can't bake your \nalgorithms into your weights. "
    },
    {
      "start": 5964.64,
      "duration": 5.36,
      "text": "A lot of animals are pre-baked in this sense.\nHumans have to figure it out at test  "
    },
    {
      "start": 5970,
      "duration": 5.36,
      "text": "time when they get born.\nYou want these environments  "
    },
    {
      "start": 5975.36,
      "duration": 4.72,
      "text": "that change really rapidly, where you \ncan't foresee what will work well. "
    },
    {
      "start": 5982,
      "duration": 2.08,
      "text": "You create intelligence to \nfigure it out at test time. "
    },
    {
      "start": 5985.2,
      "duration": 3.2,
      "text": "Quintin Pope had this interesting blog post \nwhere he's saying the reason he doesn't  "
    },
    {
      "start": 5988.4,
      "duration": 6.88,
      "text": "expect a sharp takeoff is that humans had the \nsharp takeoff where 60,000 years ago we seem  "
    },
    {
      "start": 5995.28,
      "duration": 4.48,
      "text": "to have had the cognitive architectures \nthat we have today. 10,000 years ago,  "
    },
    {
      "start": 5999.76,
      "duration": 4.8,
      "text": "agricultural revolution, modernity.\nWhat was happening in that 50,000 years? "
    },
    {
      "start": 6004.56,
      "duration": 6.4,
      "text": "You had to build this cultural scaffold where \nyou can accumulate knowledge over generations. "
    },
    {
      "start": 6011.52,
      "duration": 4.866,
      "text": "This is an ability that exists for \nfree in the way we do AI training. "
    },
    {
      "start": 6016.386,
      "duration": 5.134,
      "text": "In many cases they are literally distilled.\nIf you retrain a model, they can be trained  "
    },
    {
      "start": 6021.52,
      "duration": 3.28,
      "text": "on each other, they can be trained \non the same pre-training corpus,  "
    },
    {
      "start": 6025.36,
      "duration": 5.92,
      "text": "they don't literally have to start from scratch.\nThere's a sense in which it took humans a long  "
    },
    {
      "start": 6031.28,
      "duration": 4.24,
      "text": "time to get this cultural loop going, but it just \ncomes for free with the way we do LLM training. "
    },
    {
      "start": 6036.24,
      "duration": 3.2,
      "text": "Yes and no. Because LLMs don't really \nhave the equivalent of culture. "
    },
    {
      "start": 6039.44,
      "duration": 2.56,
      "text": "Maybe we're giving them way too \nmuch and incentivizing not to  "
    },
    {
      "start": 6042,
      "duration": 3.12,
      "text": "create it or something like that.\nBut the invention of culture and of  "
    },
    {
      "start": 6045.12,
      "duration": 3.68,
      "text": "written record and of passing down notes \nbetween each other, I don't think there's  "
    },
    {
      "start": 6048.8,
      "duration": 4.4,
      "text": "an equivalent of that with LLMs right now.\nLLMs don't really have culture right now and  "
    },
    {
      "start": 6053.2,
      "duration": 5.2,
      "text": "it's one of the impediments I would say.\nCan you give me some sense of what  "
    },
    {
      "start": 6058.4,
      "duration": 3.44,
      "text": "LLM culture might look like?\nIn the simplest case it would be a  "
    },
    {
      "start": 6061.84,
      "duration": 5.04,
      "text": "giant scratchpad that the LLM can edit and as it's \nreading stuff or as it's helping out with work,  "
    },
    {
      "start": 6066.88,
      "duration": 3.68,
      "text": "it's editing the scratchpad for itself.\nWhy can't an LLM write a book for the other  "
    },
    {
      "start": 6070.56,
      "duration": 5.68,
      "text": "LLMs? That would be cool. Why can't other \nLLMs read this LLM's book and be inspired  "
    },
    {
      "start": 6076.24,
      "duration": 4.16,
      "text": "by it or shocked by it or something like that?\nThere's no equivalence for any of this stuff. "
    },
    {
      "start": 6080.4,
      "duration": 2.88,
      "text": "Interesting. When would you expect \nthat kind of thing to start happening? "
    },
    {
      "start": 6084.64,
      "duration": 6.48,
      "text": "Also, multi-agent systems and a sort of \nindependent AI civilization and culture? "
    },
    {
      "start": 6091.12,
      "duration": 3.28,
      "text": "There are two powerful ideas in the \nrealm of multi-agent that have both  "
    },
    {
      "start": 6094.4,
      "duration": 4.24,
      "text": "not been really claimed or so on.\nThe first one I would say is culture  "
    },
    {
      "start": 6098.64,
      "duration": 5.68,
      "text": "and LLMs having a growing repertoire \nof knowledge for their own purposes. "
    },
    {
      "start": 6104.32,
      "duration": 3.2,
      "text": "The second one looks a lot more \nlike the powerful idea of self-play. "
    },
    {
      "start": 6107.52,
      "duration": 5.52,
      "text": "In my mind it’s extremely powerful.\nEvolution has a lot of competition  "
    },
    {
      "start": 6113.04,
      "duration": 6.56,
      "text": "driving intelligence and evolution.\nIn AlphaGo more algorithmically,  "
    },
    {
      "start": 6119.6,
      "duration": 4.16,
      "text": "AlphaGo is playing against itself and that's \nhow it learns to get really good at Go. "
    },
    {
      "start": 6123.76,
      "duration": 3.76,
      "text": "There's no equivalent of self-playing LLMs, \nbut I would expect that to also exist. "
    },
    {
      "start": 6127.52,
      "duration": 3.12,
      "text": "No one has done it yet.\nWhy can't an LLM for example, create a bunch  "
    },
    {
      "start": 6130.64,
      "duration": 5.68,
      "text": "of problems that another LLM is learning to solve?\nThen the LLM is always trying to serve more and  "
    },
    {
      "start": 6136.32,
      "duration": 5.2,
      "text": "more difficult problems, stuff like that.\nThere's a bunch of ways to organize it. "
    },
    {
      "start": 6142.16,
      "duration": 4.4,
      "text": "It's a realm of research, but I haven't \nseen anything that convincingly claims  "
    },
    {
      "start": 6146.56,
      "duration": 5.04,
      "text": "both of those multi-agent improvements.\nWe're mostly in the realm of a single  "
    },
    {
      "start": 6151.6,
      "duration": 5.68,
      "text": "individual agent, but that will change.\nIn the realm of culture also,  "
    },
    {
      "start": 6157.28,
      "duration": 3.92,
      "text": "I would also bucket organizations.\nWe haven't seen anything like that convincingly  "
    },
    {
      "start": 6161.2,
      "duration": 4.64,
      "text": "either. That's why we're still early.\nCan you identify the key bottleneck  "
    },
    {
      "start": 6165.84,
      "duration": 4.32,
      "text": "that's preventing this kind \nof collaboration between LLMs? "
    },
    {
      "start": 6170.16,
      "duration": 5.92,
      "text": "Maybe the way I would put it is, \nsome of these analogies work and  "
    },
    {
      "start": 6176.08,
      "duration": 5.04,
      "text": "they shouldn't, but somehow, remarkably, they do.\nA lot of the smaller models, or the dumber models,  "
    },
    {
      "start": 6181.12,
      "duration": 5.6,
      "text": "remarkably resemble a kindergarten student, or an \nelementary school student or high school student. "
    },
    {
      "start": 6187.52,
      "duration": 3.2,
      "text": "Somehow, we still haven't graduated \nenough where this stuff can take over. "
    },
    {
      "start": 6192,
      "duration": 5.04,
      "text": "My Claude Code or Codex, they still \nfeel like this elementary-grade student. "
    },
    {
      "start": 6197.04,
      "duration": 4.48,
      "text": "I know that they can take PhD quizzes, \nbut they still cognitively feel like a  "
    },
    {
      "start": 6201.52,
      "duration": 3.44,
      "text": "kindergarten or an elementary school student.\nI don't think they can create culture because  "
    },
    {
      "start": 6204.96,
      "duration": 7.36,
      "text": "they're still kids. They're savant kids. \nThey have perfect memory of all this stuff. "
    },
    {
      "start": 6213.2,
      "duration": 3.52,
      "text": "They can convincingly create all \nkinds of slop that looks really good. "
    },
    {
      "start": 6216.72,
      "duration": 2.08,
      "text": "But I still think they don't really know \nwhat they're doing and they don't really  "
    },
    {
      "start": 6218.8,
      "duration": 4.24,
      "text": "have the cognition across all these little \ncheckboxes that we still have to collect. "
    },
    {
      "start": 6223.76,
      "duration": 6.4,
      "text": "You've talked about how you were at Tesla \nleading self-driving from 2017 to 2022. "
    },
    {
      "start": 6230.16,
      "duration": 8.24,
      "text": "And you firsthand saw this progress from cool \ndemos to now thousands of cars out there actually  "
    },
    {
      "start": 6238.4,
      "duration": 2.88,
      "text": "autonomously doing drives.\nWhy did that take a decade? "
    },
    {
      "start": 6241.28,
      "duration": 5.28,
      "text": "What was happening through that time?\nOne thing I will almost instantly push  "
    },
    {
      "start": 6246.56,
      "duration": 5.6,
      "text": "back on is that this is not even near done, \nin a bunch of ways that I'm going to get to. "
    },
    {
      "start": 6253.28,
      "duration": 3.52,
      "text": "Self-driving is very interesting because \nit's definitely where I get a lot of my  "
    },
    {
      "start": 6256.8,
      "duration": 5.68,
      "text": "intuitions because I spent five years on it.\nIt has this entire history where the first demos  "
    },
    {
      "start": 6262.48,
      "duration": 5.68,
      "text": "of self-driving go all the way to the 1980s.\nYou can see a demo from CMU in 1986. "
    },
    {
      "start": 6268.16,
      "duration": 6,
      "text": "There's a truck that's driving itself on \nroads. Fast forward. When I was joining Tesla,  "
    },
    {
      "start": 6274.16,
      "duration": 6,
      "text": "I had a very early demo of Waymo.\nIt basically gave me a perfect drive  "
    },
    {
      "start": 6280.16,
      "duration": 6.08,
      "text": "in 2014 or something like that, so \na perfect Waymo drive a decade ago. "
    },
    {
      "start": 6286.24,
      "duration": 3.36,
      "text": "It took us around Palo Alto and so on \nbecause I had a friend who worked there. "
    },
    {
      "start": 6290.64,
      "duration": 2.8,
      "text": "I thought it was very close and \nthen it still took a long time. "
    },
    {
      "start": 6294.8,
      "duration": 7.28,
      "text": "For some kinds of tasks and jobs and so on, \nthere's a very large demo-to-product gap where the  "
    },
    {
      "start": 6302.08,
      "duration": 5.04,
      "text": "demo is very easy, but the product is very hard.\nIt's especially the case in cases like  "
    },
    {
      "start": 6307.12,
      "duration": 3.68,
      "text": "self-driving where the cost \nof failure is too high. "
    },
    {
      "start": 6311.76,
      "duration": 4.08,
      "text": "Many industries, tasks, and jobs maybe don't have \nthat property, but when you do have that property,  "
    },
    {
      "start": 6315.84,
      "duration": 4,
      "text": "that definitely increases the timelines.\nFor example, in software engineering,  "
    },
    {
      "start": 6319.84,
      "duration": 4.72,
      "text": "I do think that property does exist.\nFor a lot of vibe coding, it doesn't. "
    },
    {
      "start": 6324.56,
      "duration": 4.08,
      "text": "But if you're writing actual production-grade \ncode, that property should exist, because any  "
    },
    {
      "start": 6328.64,
      "duration": 3.44,
      "text": "kind of mistake leads to a security \nvulnerability or something like that. "
    },
    {
      "start": 6332.08,
      "duration": 4.08,
      "text": "Millions and hundreds of millions of \npeople's personal Social Security numbers  "
    },
    {
      "start": 6336.16,
      "duration": 5.44,
      "text": "get leaked or something like that.\nSo in software, people should be careful,  "
    },
    {
      "start": 6342.16,
      "duration": 3.84,
      "text": "kind of like in self-driving.\nIn self-driving, if things go wrong,  "
    },
    {
      "start": 6346,
      "duration": 6.96,
      "text": "you might get injured. There are worse \noutcomes. But in software, it's almost  "
    },
    {
      "start": 6352.96,
      "duration": 5.68,
      "text": "unbounded how terrible something could be.\nI do think that they share that property. "
    },
    {
      "start": 6359.92,
      "duration": 4.96,
      "text": "What takes the long amount of time and the way \nto think about it is that it's a march of nines. "
    },
    {
      "start": 6364.88,
      "duration": 5.44,
      "text": "Every single nine is a constant amount of work.\nEvery single nine is the same amount of work. "
    },
    {
      "start": 6370.32,
      "duration": 6.4,
      "text": "When you get a demo and something works 90% \nof the time, that's just the first nine. "
    },
    {
      "start": 6376.72,
      "duration": 2.16,
      "text": "Then you need the second nine, a third \nnine, a fourth nine, a fifth nine. "
    },
    {
      "start": 6378.88,
      "duration": 4.16,
      "text": "While I was at Tesla for five years or so, we \nwent through maybe three nines or two nines. "
    },
    {
      "start": 6383.04,
      "duration": 2.56,
      "text": "I don't know what it is, but \nmultiple nines of iteration. "
    },
    {
      "start": 6385.6,
      "duration": 3.92,
      "text": "There are still more nines to go.\nThat's why these things take so long. "
    },
    {
      "start": 6391.52,
      "duration": 3.84,
      "text": "It's definitely formative for me, seeing \nsomething that was a demo. I'm very  "
    },
    {
      "start": 6395.36,
      "duration": 5.52,
      "text": "unimpressed by demos. Whenever I see demos of \nanything, I'm extremely unimpressed by that. "
    },
    {
      "start": 6403.44,
      "duration": 2.48,
      "text": "If it's a demo that someone cooked \nup as a showing, it's worse. "
    },
    {
      "start": 6405.92,
      "duration": 1.68,
      "text": "If you can interact with it, it's a bit better. "
    },
    {
      "start": 6407.6,
      "duration": 3.36,
      "text": "But even then, you're not done. You need the \nactual product. It's going to face all these  "
    },
    {
      "start": 6410.96,
      "duration": 2.8,
      "text": "challenges when it comes in contact \nwith reality and all these different  "
    },
    {
      "start": 6413.76,
      "duration": 3.68,
      "text": "pockets of behavior that need patching.\nWe're going to see all this stuff play  "
    },
    {
      "start": 6417.44,
      "duration": 4.4,
      "text": "out. It's a march of nines. Each nine is \nconstant. Demos are encouraging. It’s still  "
    },
    {
      "start": 6421.84,
      "duration": 5.36,
      "text": "a huge amount of work to do.\nIt is a critical safety domain,  "
    },
    {
      "start": 6427.2,
      "duration": 3.68,
      "text": "unless you're doing vibe coding, \nwhich is all nice and fun and so on. "
    },
    {
      "start": 6431.84,
      "duration": 2.72,
      "text": "That's why this also enforced my \ntimelines from that perspective. "
    },
    {
      "start": 6436,
      "duration": 4.56,
      "text": "It's very interesting to hear you say that, that \nthe safety guarantees you need from software  "
    },
    {
      "start": 6440.56,
      "duration": 4,
      "text": "are not dissimilar to self-driving.\nWhat people will often say is that  "
    },
    {
      "start": 6444.56,
      "duration": 5.6,
      "text": "self-driving took so long because \nthe cost of failure is so high. "
    },
    {
      "start": 6450.16,
      "duration": 4.16,
      "text": "A human makes a mistake on average every \n400,000 miles or every seven years. "
    },
    {
      "start": 6454.32,
      "duration": 5.04,
      "text": "If you had to release a coding agent that \ncouldn't make a mistake for at least seven years,  "
    },
    {
      "start": 6459.36,
      "duration": 4.08,
      "text": "it would be much harder to deploy.\nBut your point is that if you made a  "
    },
    {
      "start": 6463.44,
      "duration": 4.4,
      "text": "catastrophic coding mistake, like breaking \nsome important system every seven years... "
    },
    {
      "start": 6467.84,
      "duration": 3.36,
      "text": "Very easy to do.\nIn fact, in terms of wall clock time,  "
    },
    {
      "start": 6471.2,
      "duration": 3.36,
      "text": "it would be much less than seven years because \nyou're constantly outputting code like that. "
    },
    {
      "start": 6477.28,
      "duration": 2.8,
      "text": "In terms of tokens, it would be seven years.\nBut in terms of wall clock time... "
    },
    {
      "start": 6480.08,
      "duration": 2.8,
      "text": "In some ways, it's a much harder problem.\nSelf-driving is just one of  "
    },
    {
      "start": 6482.88,
      "duration": 4.24,
      "text": "thousands of things that people do.\nIt's almost like a single vertical, I suppose. "
    },
    {
      "start": 6487.12,
      "duration": 1.84,
      "text": "Whereas when we're talking about \ngeneral software engineering,  "
    },
    {
      "start": 6488.96,
      "duration": 5.76,
      "text": "it's even more... There's more surface area.\nThere's another objection people make to that  "
    },
    {
      "start": 6494.72,
      "duration": 6.4,
      "text": "analogy, which is that with self-driving, what \ntook a big fraction of that time was solving  "
    },
    {
      "start": 6501.12,
      "duration": 5.92,
      "text": "the problem of having basic perception \nthat's robust, building representations,  "
    },
    {
      "start": 6507.04,
      "duration": 5.04,
      "text": "and having a model that has some common \nsense so it can generalize to when it sees  "
    },
    {
      "start": 6512.08,
      "duration": 4.96,
      "text": "something that's slightly out of distribution.\nIf somebody's waving down the road this way,  "
    },
    {
      "start": 6517.04,
      "duration": 3.6,
      "text": "you don't need to train for it.\nThe thing will have some understanding  "
    },
    {
      "start": 6520.64,
      "duration": 3.76,
      "text": "of how to respond to something like that.\nThese are things we're getting for free  "
    },
    {
      "start": 6524.4,
      "duration": 5.28,
      "text": "with LLMs or VLMs today, so we don't have to \nsolve these very basic representation problems. "
    },
    {
      "start": 6529.68,
      "duration": 4.64,
      "text": "So now deploying AIs across different domains \nwill sort of be like deploying a self-driving  "
    },
    {
      "start": 6534.32,
      "duration": 4.24,
      "text": "car with current models to a different city, \nwhich is hard but not like a 10-year-long task. "
    },
    {
      "start": 6539.2,
      "duration": 3.92,
      "text": "I'm not 100% sure if I fully agree with that.\nI don't know how much we're getting for free. "
    },
    {
      "start": 6543.84,
      "duration": 2.72,
      "text": "There's still a lot of gaps in \nunderstanding what we are getting. "
    },
    {
      "start": 6547.68,
      "duration": 2.56,
      "text": "We're definitely getting more \ngeneralizable intelligence in a  "
    },
    {
      "start": 6550.24,
      "duration": 4.24,
      "text": "single entity, whereas self-driving is a \nvery special-purpose task that requires. "
    },
    {
      "start": 6554.48,
      "duration": 4.08,
      "text": "In some sense building a special-purpose task \nis maybe even harder in a certain sense because  "
    },
    {
      "start": 6558.56,
      "duration": 3.68,
      "text": "it doesn't fall out from a more general thing \nthat you're doing at scale, if that makes sense. "
    },
    {
      "start": 6564.24,
      "duration": 6.16,
      "text": "But the analogy still doesn't fully \nresonate because the LLMs are still  "
    },
    {
      "start": 6570.4,
      "duration": 3.12,
      "text": "pretty fallible and they have a lot of \ngaps that still need to be filled in. "
    },
    {
      "start": 6573.52,
      "duration": 2.16,
      "text": "I don't think that we're \ngetting magical generalization  "
    },
    {
      "start": 6575.68,
      "duration": 6.64,
      "text": "completely out of the box, in a certain sense.\nThe other aspect that I wanted to return to is  "
    },
    {
      "start": 6582.96,
      "duration": 7.52,
      "text": "that self-driving cars are nowhere near done \nstill. The deployments are pretty minimal.  "
    },
    {
      "start": 6591.36,
      "duration": 3.12,
      "text": "Even Waymo and so on has very few cars.\nThey're doing that roughly speaking  "
    },
    {
      "start": 6594.48,
      "duration": 4.96,
      "text": "because they're not economical.\nThey've built something that lives in the future. "
    },
    {
      "start": 6600.32,
      "duration": 3.44,
      "text": "They've had to pull back the future, \nbut they had to make it uneconomical. "
    },
    {
      "start": 6605.84,
      "duration": 3.44,
      "text": "There are all these costs, not just \nmarginal costs for those cars and  "
    },
    {
      "start": 6609.28,
      "duration": 3.12,
      "text": "their operation and maintenance, but \nalso the capex of the entire thing. "
    },
    {
      "start": 6613.12,
      "duration": 3.04,
      "text": "Making it economical is still \ngoing to be a slog for them. "
    },
    {
      "start": 6617.92,
      "duration": 3.6,
      "text": "Also, when you look at these cars and \nthere's no one driving, I actually think  "
    },
    {
      "start": 6621.52,
      "duration": 4.96,
      "text": "it's a little bit deceiving because there \nare very elaborate teleoperation centers  "
    },
    {
      "start": 6626.48,
      "duration": 6.16,
      "text": "of people kind of in a loop with these cars.\nI don't have the full extent of it, but there's  "
    },
    {
      "start": 6632.64,
      "duration": 3.52,
      "text": "more human-in-the-loop than you might expect.\nThere are people somewhere out there  "
    },
    {
      "start": 6636.8,
      "duration": 3.12,
      "text": "beaming in from the sky.\nI don't know if they're  "
    },
    {
      "start": 6639.92,
      "duration": 2.88,
      "text": "fully in the loop with the driving.\nSome of the time they are, but they're  "
    },
    {
      "start": 6642.8,
      "duration": 2.8,
      "text": "certainly involved and there are people.\nIn some sense, we haven't actually removed  "
    },
    {
      "start": 6645.6,
      "duration": 2.96,
      "text": "the person, we've moved them to \nsomewhere where you can't see them. "
    },
    {
      "start": 6648.56,
      "duration": 3.12,
      "text": "I still think there will be some work, as you \nmentioned, going from environment to environment. "
    },
    {
      "start": 6652.8,
      "duration": 2.88,
      "text": "There are still challenges \nto make self-driving real. "
    },
    {
      "start": 6655.68,
      "duration": 3.92,
      "text": "But I do agree that it's definitely crossed \na threshold where it kind of feels real,  "
    },
    {
      "start": 6659.6,
      "duration": 3.68,
      "text": "unless it's really teleoperated.\nFor example, Waymo can't go to  "
    },
    {
      "start": 6663.28,
      "duration": 3.84,
      "text": "all the different parts of the city.\nMy suspicion is that it's parts of the city  "
    },
    {
      "start": 6667.12,
      "duration": 4.72,
      "text": "where you don't get good signal.\nAnyway, I don't know anything  "
    },
    {
      "start": 6671.84,
      "duration": 6.08,
      "text": "about the stack. I'm just making stuff up.\nYou led self-driving for five years at Tesla. "
    },
    {
      "start": 6677.92,
      "duration": 1.68,
      "text": "Sorry, I don't know anything \nabout the specifics of Waymo. "
    },
    {
      "start": 6681.12,
      "duration": 1.76,
      "text": "By the way, I love Waymo \nand I take it all the time. "
    },
    {
      "start": 6684.88,
      "duration": 4.24,
      "text": "I just think that people are sometimes a \nlittle bit too naive about some of the progress  "
    },
    {
      "start": 6689.12,
      "duration": 4.32,
      "text": "and there's still a huge amount of work.\nTesla took in my mind a much more scalable  "
    },
    {
      "start": 6693.44,
      "duration": 6,
      "text": "approach and the team is doing extremely well.\nI'm kind of on the record for predicting  "
    },
    {
      "start": 6699.44,
      "duration": 2.96,
      "text": "how this thing will go.\nWaymo had an early start  "
    },
    {
      "start": 6702.4,
      "duration": 3.52,
      "text": "because you can package up so many sensors.\nBut I do think Tesla is taking the more  "
    },
    {
      "start": 6705.92,
      "duration": 2.8,
      "text": "scalable strategy and it's going \nto look a lot more like that. "
    },
    {
      "start": 6708.72,
      "duration": 5.52,
      "text": "So this will still have to play out and hasn't.\nBut I don't want to talk about self-driving as  "
    },
    {
      "start": 6714.24,
      "duration": 5.6,
      "text": "something that took a decade because it \ndidn't take it yet, if that makes sense. "
    },
    {
      "start": 6719.84,
      "duration": 5.28,
      "text": "Because one, the start is at 1980 and not 10 \nyears ago, and then two, the end is not here yet. "
    },
    {
      "start": 6725.12,
      "duration": 3.28,
      "text": "The end is not near yet because when \nwe're talking about self-driving,  "
    },
    {
      "start": 6728.4,
      "duration": 5.36,
      "text": "usually in my mind it's self-driving at scale.\nPeople don't have to get a driver's license, etc. "
    },
    {
      "start": 6733.76,
      "duration": 5.36,
      "text": "I'm curious to bounce two other ways in \nwhich the analogy might be different. "
    },
    {
      "start": 6739.12,
      "duration": 5.36,
      "text": "The reason I'm especially curious about this is \nbecause the question of how fast AI is deployed,  "
    },
    {
      "start": 6744.48,
      "duration": 4.24,
      "text": "how valuable it is when it's early \non is potentially the most important  "
    },
    {
      "start": 6748.72,
      "duration": 2.4,
      "text": "question in the world right now.\nIf you're trying to model what the  "
    },
    {
      "start": 6751.12,
      "duration": 4.08,
      "text": "year 2030 looks like, this is the question \nyou ought to have some understanding of. "
    },
    {
      "start": 6756.16,
      "duration": 5.36,
      "text": "Another thing you might think is one, you have \nthis latency requirement with self-driving. "
    },
    {
      "start": 6762.24,
      "duration": 3.2,
      "text": "I have no idea what the actual models are, but I \nassume it’s like tens of millions of parameters  "
    },
    {
      "start": 6765.44,
      "duration": 5.76,
      "text": "or something, which is not the necessary \nconstraint for knowledge work with LLMs. "
    },
    {
      "start": 6771.2,
      "duration": 5.68,
      "text": "Maybe it might be with computer use and stuff.\nBut the other big one is, maybe more  "
    },
    {
      "start": 6776.88,
      "duration": 5.92,
      "text": "importantly, on this capex question.\nYes, there is additional cost to serving  "
    },
    {
      "start": 6782.8,
      "duration": 9.2,
      "text": "up an additional copy of a model, but the opex \nof a session is quite low and you can amortize  "
    },
    {
      "start": 6792,
      "duration": 5.28,
      "text": "the cost of AI into the training run itself, \ndepending on how inference scaling goes and stuff. "
    },
    {
      "start": 6797.28,
      "duration": 6.48,
      "text": "But it's certainly not as much as building a whole \nnew car to serve another instance of a model. "
    },
    {
      "start": 6803.76,
      "duration": 4.8,
      "text": "So the economics of deploying more \nwidely are much more favorable. "
    },
    {
      "start": 6808.56,
      "duration": 2.56,
      "text": "I think that's right. If you're \nsticking to the realm of bits,  "
    },
    {
      "start": 6811.12,
      "duration": 5.52,
      "text": "bits are a million times easier than anything \nthat touches the physical world. I definitely  "
    },
    {
      "start": 6816.64,
      "duration": 6.24,
      "text": "grant that. Bits are completely changeable, \narbitrarily reshuffleable at a very rapid speed. "
    },
    {
      "start": 6822.88,
      "duration": 7.44,
      "text": "You would expect a much faster adaptation also in \nthe industry and so on. What was the first one? "
    },
    {
      "start": 6830.32,
      "duration": 3.2,
      "text": "The latency requirements and \nits implications for model size? "
    },
    {
      "start": 6833.52,
      "duration": 2.56,
      "text": "I think that's roughly right. I also \nthink that if we are talking about  "
    },
    {
      "start": 6836.08,
      "duration": 4,
      "text": "knowledge work at scale, there will be some \nlatency requirements, practically speaking,  "
    },
    {
      "start": 6840.08,
      "duration": 6.08,
      "text": "because we're going to have to create a \nhuge amount of compute and serve that. "
    },
    {
      "start": 6846.16,
      "duration": 3.92,
      "text": "The last aspect that I very briefly want \nto also talk about is all the rest of it. "
    },
    {
      "start": 6853.28,
      "duration": 4.24,
      "text": "What does society think about it? What are \nthe legal ramifications? How is it working  "
    },
    {
      "start": 6857.52,
      "duration": 6.56,
      "text": "legally? How is it working insurance-wise? \nWhat are those layers of it and aspects of it? "
    },
    {
      "start": 6865.04,
      "duration": 2.56,
      "text": "What is the equivalent of people \nputting a cone on a Waymo? "
    },
    {
      "start": 6868.8,
      "duration": 5.68,
      "text": "There are going to be equivalents of all that.\nSo I feel like self-driving is a very nice  "
    },
    {
      "start": 6874.48,
      "duration": 3.52,
      "text": "analogy that you can borrow things from.\nWhat is the equivalent of a cone in the car? "
    },
    {
      "start": 6878,
      "duration": 7.28,
      "text": "What is the equivalent of a teleoperating worker \nwho's hidden away and all the aspects of it. "
    },
    {
      "start": 6885.28,
      "duration": 3.2,
      "text": "Do you have any opinions on what this \nimplies about the current AI buildout,  "
    },
    {
      "start": 6889.2,
      "duration": 6.96,
      "text": "which would 10x the amount of available compute \nin the world in a year or two and maybe more  "
    },
    {
      "start": 6896.16,
      "duration": 4.64,
      "text": "than 100x it by the end of the decade.\nIf the use of AI will be lower than  "
    },
    {
      "start": 6900.8,
      "duration": 3.76,
      "text": "some people naively predict, does \nthat mean that we're overbuilding  "
    },
    {
      "start": 6904.56,
      "duration": 4,
      "text": "compute or is that a separate question?\nKind of like what happened with railroads. "
    },
    {
      "start": 6908.56,
      "duration": 2.32,
      "text": "With what, sorry?\nWas it railroads or? "
    },
    {
      "start": 6910.88,
      "duration": 3.2,
      "text": "Yeah, it was.\nYeah. There's historical precedent.  "
    },
    {
      "start": 6914.08,
      "duration": 4.4,
      "text": "Or was it with the telecommunication industry?\nPre-paving the internet that only came a decade  "
    },
    {
      "start": 6918.48,
      "duration": 5.52,
      "text": "later and creating a whole bubble in the \ntelecommunications industry in the late '90s. "
    },
    {
      "start": 6928,
      "duration": 5.2,
      "text": "I understand I'm sounding very pessimistic \nhere. I'm actually optimistic. I think this  "
    },
    {
      "start": 6933.2,
      "duration": 3.28,
      "text": "will work. I think it's tractable. I'm \nonly sounding pessimistic because when  "
    },
    {
      "start": 6936.48,
      "duration": 3.92,
      "text": "I go on my Twitter timeline, I see all \nthis stuff that makes no sense to me. "
    },
    {
      "start": 6942.56,
      "duration": 4.8,
      "text": "There's a lot of reasons for why that exists.\nA lot of it is honestly just fundraising.  "
    },
    {
      "start": 6947.36,
      "duration": 3.12,
      "text": "It's just incentive structures. \nA lot of it may be fundraising. "
    },
    {
      "start": 6950.48,
      "duration": 4.96,
      "text": "A lot of it is just attention, converting \nattention to money on the internet,  "
    },
    {
      "start": 6955.44,
      "duration": 4.8,
      "text": "stuff like that.\nThere's a lot of  "
    },
    {
      "start": 6960.24,
      "duration": 5.68,
      "text": "that going on, and I'm only reacting to that.\nBut I'm still overall very bullish on technology. "
    },
    {
      "start": 6965.92,
      "duration": 4,
      "text": "We're going to work through all this stuff.\nThere's been a rapid amount of progress. "
    },
    {
      "start": 6969.92,
      "duration": 5.28,
      "text": "I don't know that there's overbuilding.\nI think we're going to be able to gobble up what,  "
    },
    {
      "start": 6975.2,
      "duration": 5.04,
      "text": "in my understanding, is being built.\nFor example, Claude Code or OpenAI Codex  "
    },
    {
      "start": 6980.24,
      "duration": 2.64,
      "text": "and stuff like that didn't even \nexist a year ago. Is that right?  "
    },
    {
      "start": 6984.96,
      "duration": 4.96,
      "text": "This is a miraculous technology that didn't exist.\nThere's going to be a huge amount of demand,  "
    },
    {
      "start": 6989.92,
      "duration": 6.08,
      "text": "as we see the demand in ChatGPT already and so on.\nSo I don't know that there's overbuilding. "
    },
    {
      "start": 6997.92,
      "duration": 4.4,
      "text": "I'm just reacting to some of the very fast \ntimelines that people continue to say incorrectly. "
    },
    {
      "start": 7002.32,
      "duration": 4.24,
      "text": "I've heard many, many times over the course \nof my 15 years in AI where very reputable  "
    },
    {
      "start": 7006.56,
      "duration": 6.88,
      "text": "people keep getting this wrong all the time.\nI want this to be properly calibrated, and some  "
    },
    {
      "start": 7013.44,
      "duration": 5.68,
      "text": "of this also has geopolitical ramifications and \nthings like that with some of these questions. "
    },
    {
      "start": 7019.68,
      "duration": 3.52,
      "text": "I don't want people to make \nmistakes in that sphere of things. "
    },
    {
      "start": 7024.08,
      "duration": 3.68,
      "text": "I do want us to be grounded in the \nreality of what technology is and isn't. "
    },
    {
      "start": 7028.88,
      "duration": 6.96,
      "text": "Let's talk about education and Eureka.\nOne thing you could do is start another AI  "
    },
    {
      "start": 7035.84,
      "duration": 5.28,
      "text": "lab and then try to solve those problems.\nI’m curious what you're up to now,  "
    },
    {
      "start": 7041.68,
      "duration": 4.64,
      "text": "and why not AI research itself?\nI guess the way I would put it  "
    },
    {
      "start": 7046.32,
      "duration": 6,
      "text": "is I feel some amount of determinism \naround the things that AI labs are doing. "
    },
    {
      "start": 7053.36,
      "duration": 7.84,
      "text": "I feel like I could help out there, but I \ndon't know that I would uniquely improve it. "
    },
    {
      "start": 7062.24,
      "duration": 4.24,
      "text": "My personal big fear is that a lot of this \nstuff happens on the side of humanity,  "
    },
    {
      "start": 7066.48,
      "duration": 6.32,
      "text": "and that humanity gets disempowered by it.\nI care not just about all the Dyson spheres  "
    },
    {
      "start": 7072.8,
      "duration": 2.64,
      "text": "that we're going to build and that AI is \ngoing to build in a fully autonomous way,  "
    },
    {
      "start": 7075.44,
      "duration": 5.04,
      "text": "I care about what happens to humans.\nI want humans to be well off in the future. "
    },
    {
      "start": 7080.48,
      "duration": 3.6,
      "text": "I feel like that's where I can a \nlot more uniquely add value than  "
    },
    {
      "start": 7084.08,
      "duration": 7.12,
      "text": "an incremental improvement in the frontier lab.\nI'm most afraid of something depicted in movies  "
    },
    {
      "start": 7091.2,
      "duration": 4.72,
      "text": "like WALL-E or Idiocracy or something like that, \nwhere humanity is on the side of this stuff. "
    },
    {
      "start": 7096.8,
      "duration": 3.12,
      "text": "I want humans to be much, \nmuch better in this future. "
    },
    {
      "start": 7101.12,
      "duration": 3.52,
      "text": "To me, this is through education \nthat you can achieve this. "
    },
    {
      "start": 7106,
      "duration": 4.56,
      "text": "So what are you working on there?\nThe easiest way I can describe it is  "
    },
    {
      "start": 7110.56,
      "duration": 4.24,
      "text": "we're trying to build the Starfleet Academy.\nI don’t know if you’ve watched Star Trek. "
    },
    {
      "start": 7114.8,
      "duration": 2.8,
      "text": "I haven’t.\nStarfleet Academy is  "
    },
    {
      "start": 7117.6,
      "duration": 6.24,
      "text": "this elite institution for frontier technology, \nbuilding spaceships, and graduating cadets to be  "
    },
    {
      "start": 7123.84,
      "duration": 4.08,
      "text": "the pilots of these spaceships and whatnot.\nSo I just imagine an elite institution for  "
    },
    {
      "start": 7127.92,
      "duration": 8.56,
      "text": "technical knowledge and a kind of school that's \nvery up-to-date and a premier institution. "
    },
    {
      "start": 7136.48,
      "duration": 7.04,
      "text": "A category of questions I have for you is \nexplaining how one teaches technical or  "
    },
    {
      "start": 7143.52,
      "duration": 4.48,
      "text": "scientific content well, because you \nare one of the world masters at it. "
    },
    {
      "start": 7148.64,
      "duration": 3.12,
      "text": "I'm curious both about how you think about \nit for content you've already put out there  "
    },
    {
      "start": 7151.76,
      "duration": 4.32,
      "text": "on YouTube, but also, to the extent it's any \ndifferent, how you think about it for Eureka. "
    },
    {
      "start": 7156.88,
      "duration": 3.68,
      "text": "With respect to Eureka, one thing that \nis very fascinating to me about education  "
    },
    {
      "start": 7160.56,
      "duration": 4.4,
      "text": "is that I do think education will pretty \nfundamentally change with AIs on the side. "
    },
    {
      "start": 7164.96,
      "duration": 5.44,
      "text": "It has to be rewired and changed to some extent.\nI still think that we're pretty early. "
    },
    {
      "start": 7170.4,
      "duration": 2.24,
      "text": "There's going to be a lot of people who \nare going to try to do the obvious things. "
    },
    {
      "start": 7173.44,
      "duration": 4.64,
      "text": "Have an LLM and ask it questions.\nDo all the basic things that you would  "
    },
    {
      "start": 7178.08,
      "duration": 2.08,
      "text": "do via prompting right now.\nIt's helpful,  "
    },
    {
      "start": 7180.16,
      "duration": 4.24,
      "text": "but it still feels to me a bit like slop.\nI'd like to do it properly, and I think the  "
    },
    {
      "start": 7184.4,
      "duration": 6.16,
      "text": "capability is not there for what I would want.\nWhat I'd want is an actual tutor experience. "
    },
    {
      "start": 7191.36,
      "duration": 5.76,
      "text": "A prominent example in my mind is I was \nrecently learning Korean, so language learning. "
    },
    {
      "start": 7197.12,
      "duration": 3.2,
      "text": "I went through a phase where I was \nlearning Korean by myself on the internet. "
    },
    {
      "start": 7200.32,
      "duration": 5.76,
      "text": "I went through a phase where I was part of a small \nclass in Korea taking Korean with a bunch of other  "
    },
    {
      "start": 7206.08,
      "duration": 2,
      "text": "people, which was really funny.\nWe had a teacher and 10  "
    },
    {
      "start": 7208.08,
      "duration": 4.4,
      "text": "people or so taking Korean.\nThen I switched to a one-on-one tutor. "
    },
    {
      "start": 7213.6,
      "duration": 6,
      "text": "I guess what was fascinating to me was, I think \nI had a really good tutor, but just thinking  "
    },
    {
      "start": 7219.6,
      "duration": 5.6,
      "text": "through what this tutor was doing for me and how \nincredible that experience was and how high the  "
    },
    {
      "start": 7225.2,
      "duration": 6.48,
      "text": "bar is for what I want to build eventually.\nInstantly from a very short  "
    },
    {
      "start": 7231.68,
      "duration": 4.08,
      "text": "conversation, she understood where I am \nas a student, what I know and don't know. "
    },
    {
      "start": 7235.76,
      "duration": 5.28,
      "text": "She was able to probe exactly the kinds of \nquestions or things to understand my world model. "
    },
    {
      "start": 7241.04,
      "duration": 3.28,
      "text": "No LLM will do that for you \n100% right now, not even close. "
    },
    {
      "start": 7244.32,
      "duration": 5.04,
      "text": "But a tutor will do that if they're good.\nOnce she understands, she really served  "
    },
    {
      "start": 7249.36,
      "duration": 3.44,
      "text": "me all the things that I needed at \nmy current sliver of capability. "
    },
    {
      "start": 7252.8,
      "duration": 4.08,
      "text": "I need to be always appropriately challenged.\nI can't be faced with something too hard or  "
    },
    {
      "start": 7256.88,
      "duration": 3.92,
      "text": "too trivial, and a tutor is really good \nat serving you just the right stuff. "
    },
    {
      "start": 7261.68,
      "duration": 5.92,
      "text": "I felt like I was the only constraint to learning.\nI was always given the perfect information. I'm  "
    },
    {
      "start": 7267.6,
      "duration": 3.52,
      "text": "the only constraint. I felt good because \nI'm the only impediment that exists. "
    },
    {
      "start": 7271.12,
      "duration": 3.12,
      "text": "It's not that I can't find knowledge or \nthat it's not properly explained or etc. "
    },
    {
      "start": 7274.24,
      "duration": 4.4,
      "text": "It's just my ability to memorize and so on.\nThis is what I want for people. "
    },
    {
      "start": 7278.64,
      "duration": 2.56,
      "text": "How do you automate that?\nVery good question. At  "
    },
    {
      "start": 7281.2,
      "duration": 5.76,
      "text": "the current capability, you don't.\nThat's why I think it's not actually the  "
    },
    {
      "start": 7286.96,
      "duration": 4.4,
      "text": "right time to build this kind of an AI tutor.\nI still think it's a useful product,  "
    },
    {
      "start": 7291.36,
      "duration": 7.12,
      "text": "and lots of people will build it, but the bar \nis so high and the capability is not there. "
    },
    {
      "start": 7300.4,
      "duration": 5.04,
      "text": "Even today, I would say ChatGPT is an \nextremely valuable educational product. "
    },
    {
      "start": 7305.44,
      "duration": 3.2,
      "text": "But for me, it was so fascinating \nto see how high the bar is. "
    },
    {
      "start": 7308.64,
      "duration": 4.4,
      "text": "When I was with her, I almost felt \nlike there's no way I can build this. "
    },
    {
      "start": 7313.04,
      "duration": 2.16,
      "text": "But you are building it, right?\nAnyone who's had a really good  "
    },
    {
      "start": 7315.2,
      "duration": 6.24,
      "text": "tutor is like, \"How are you going to build \nthis?\" I'm waiting for that capability. I  "
    },
    {
      "start": 7324.4,
      "duration": 4.64,
      "text": "did some AI consulting for computer vision.\nA lot of times, the value that I brought to  "
    },
    {
      "start": 7329.04,
      "duration": 4.48,
      "text": "the company was telling them not to use AI.\nI was the AI expert, and they described the  "
    },
    {
      "start": 7333.52,
      "duration": 4.16,
      "text": "problem, and I said, \"Don't use AI.\" \nThis is my value add. I feel like it's  "
    },
    {
      "start": 7337.68,
      "duration": 3.92,
      "text": "the same in education right now, where \nI feel like for what I have in mind,  "
    },
    {
      "start": 7341.6,
      "duration": 4.56,
      "text": "it's not yet the time, but the time will come.\nFor now, I'm building something that looks  "
    },
    {
      "start": 7346.16,
      "duration": 4.16,
      "text": "maybe a bit more conventional that has a \nphysical and digital component and so on. "
    },
    {
      "start": 7350.32,
      "duration": 4.72,
      "text": "But it's obvious how this \nshould look in the future. "
    },
    {
      "start": 7355.04,
      "duration": 2.72,
      "text": "To the extent you're willing to \nsay, what is the thing you hope  "
    },
    {
      "start": 7357.76,
      "duration": 5.52,
      "text": "will be released this year or next year?\nI'm building the first course. I want to  "
    },
    {
      "start": 7363.28,
      "duration": 4.72,
      "text": "have a really, really good course, the \nobvious state-of-the-art destination  "
    },
    {
      "start": 7368,
      "duration": 3.44,
      "text": "you go to to learn, AI in this case.\nThat's just what I'm familiar with, so it's  "
    },
    {
      "start": 7371.44,
      "duration": 4.64,
      "text": "a really good first product to get to be really \ngood at it. So that's what I'm building. Nanochat,  "
    },
    {
      "start": 7376.08,
      "duration": 4.64,
      "text": "which you briefly mentioned, is a capstone project \nof LLM101N, which is a class that I'm building. "
    },
    {
      "start": 7382.48,
      "duration": 2.24,
      "text": "That's a really big piece of it.\nBut now I have to build out a lot of  "
    },
    {
      "start": 7384.72,
      "duration": 6.24,
      "text": "the intermediates, and then I have to hire a small \nteam of TAs and so on and build the entire course. "
    },
    {
      "start": 7391.52,
      "duration": 3.92,
      "text": "One more thing that I would say is that many \ntimes, when people think about education,  "
    },
    {
      "start": 7395.44,
      "duration": 4.8,
      "text": "they think more about what I would say is \na softer component of diffusing knowledge. "
    },
    {
      "start": 7401.76,
      "duration": 5.2,
      "text": "I have something very hard and technical in mind.\nIn my mind, education is the very difficult  "
    },
    {
      "start": 7406.96,
      "duration": 6.48,
      "text": "technical process of building ramps to knowledge.\nIn my mind, nanochat is a ramp to  "
    },
    {
      "start": 7413.44,
      "duration": 4.56,
      "text": "knowledge because it's very simple.\nIt's the super simplified full-stack thing. "
    },
    {
      "start": 7418,
      "duration": 3.92,
      "text": "If you give this artifact to someone and they \nlook through it, they're learning a ton of stuff. "
    },
    {
      "start": 7423.76,
      "duration": 4.32,
      "text": "It's giving you a lot of what I call eurekas \nper second, which is understanding per second. "
    },
    {
      "start": 7428.08,
      "duration": 4.64,
      "text": "That's what I want, lots of eurekas per second.\nSo to me, this is a technical problem of  "
    },
    {
      "start": 7432.72,
      "duration": 5.92,
      "text": "how do we build these ramps to knowledge.\nSo I almost think of Eureka as maybe not that  "
    },
    {
      "start": 7438.64,
      "duration": 4.48,
      "text": "different from some of the frontier labs \nor some of the work that's going on there. "
    },
    {
      "start": 7443.12,
      "duration": 4.48,
      "text": "I want to figure out how to build these \nramps very efficiently so that people are  "
    },
    {
      "start": 7447.6,
      "duration": 5.92,
      "text": "never stuck and everything is always \nnot too hard or not too trivial, and  "
    },
    {
      "start": 7454.24,
      "duration": 4.72,
      "text": "you have just the right material to progress.\nYou're imagining in the short term that instead  "
    },
    {
      "start": 7458.96,
      "duration": 5.68,
      "text": "of a tutor being able to probe your understanding, \nif you have enough self-awareness to be able to  "
    },
    {
      "start": 7464.64,
      "duration": 5.04,
      "text": "probe yourself, you're never going to be stuck.\nYou can find the right answer between talking  "
    },
    {
      "start": 7469.68,
      "duration": 3.76,
      "text": "to the TA or talking to an LLM and \nlooking at the reference implementation. "
    },
    {
      "start": 7473.44,
      "duration": 5.44,
      "text": "It sounds like automation or \nAI is not a significant part. "
    },
    {
      "start": 7478.88,
      "duration": 7.6,
      "text": "So far, the big alpha here is your \nability to explain AI codified  "
    },
    {
      "start": 7486.48,
      "duration": 4.88,
      "text": "in the source material of the class.\nThat's fundamentally what the course is. "
    },
    {
      "start": 7491.36,
      "duration": 3.76,
      "text": "You always have to be calibrated to \nwhat capability exists in the industry. "
    },
    {
      "start": 7495.92,
      "duration": 3.36,
      "text": "A lot of people are going to \npursue just asking ChatGPT, etc. "
    },
    {
      "start": 7499.28,
      "duration": 4.56,
      "text": "But I think right now, for example, if you go to \nChatGPT and you say, teach me AI, there's no way. "
    },
    {
      "start": 7503.84,
      "duration": 5.28,
      "text": "It's going to give you some slop.\nAI is never going to write nanochat right now. "
    },
    {
      "start": 7509.12,
      "duration": 3.12,
      "text": "But nanochat is a really \nuseful intermediate point. "
    },
    {
      "start": 7513.52,
      "duration": 2.24,
      "text": "I'm collaborating with AI \nto create all this material,  "
    },
    {
      "start": 7515.76,
      "duration": 5.28,
      "text": "so AI is still fundamentally very helpful.\nEarlier on, I built CS231n at Stanford,  "
    },
    {
      "start": 7521.04,
      "duration": 5.28,
      "text": "which I think was the first deep learning \nclass at Stanford, which became very popular. "
    },
    {
      "start": 7527.68,
      "duration": 4.08,
      "text": "The difference in building out 231n \nthen and LLM101N now is quite stark. "
    },
    {
      "start": 7533.12,
      "duration": 4.72,
      "text": "I feel really empowered by the LLMs as they \nexist right now, but I'm very much in the loop. "
    },
    {
      "start": 7537.84,
      "duration": 3.12,
      "text": "They're helping me build the \nmaterials, I go much faster. "
    },
    {
      "start": 7540.96,
      "duration": 4.16,
      "text": "They're doing a lot of the boring stuff, etc.\nI feel like I'm developing the course much faster,  "
    },
    {
      "start": 7545.12,
      "duration": 5.04,
      "text": "and it's LLM-infused, but it's not yet at a \nplace where it can creatively create the content. "
    },
    {
      "start": 7550.16,
      "duration": 3.28,
      "text": "I'm still there to do that.\nThe trickiness is always  "
    },
    {
      "start": 7553.44,
      "duration": 3.6,
      "text": "calibrating yourself to what exists.\nWhen you imagine what is available  "
    },
    {
      "start": 7557.04,
      "duration": 5.36,
      "text": "through Eureka in a couple of years, it \nseems like the big bottleneck is going to be  "
    },
    {
      "start": 7562.4,
      "duration": 6,
      "text": "finding Karpathys in field after field who can \nconvert their understanding into these ramps. "
    },
    {
      "start": 7569.44,
      "duration": 5.36,
      "text": "It would change over time. Right now, \nit would be hiring faculty to help work  "
    },
    {
      "start": 7574.8,
      "duration": 5.6,
      "text": "hand-in-hand with AI and a team of people \nprobably to build state-of-the-art courses. "
    },
    {
      "start": 7581.44,
      "duration": 6.72,
      "text": "Over time maybe some of the TAs can become AIs.\nYou just take all the course materials and then  "
    },
    {
      "start": 7588.16,
      "duration": 4.96,
      "text": "I think you could serve a very good automated \nTA for the student when they have more basic  "
    },
    {
      "start": 7593.12,
      "duration": 2.96,
      "text": "questions or something like that.\nBut I think you'll need faculty  "
    },
    {
      "start": 7596.08,
      "duration": 4.24,
      "text": "for the overall architecture of a \ncourse and making sure that it fits. "
    },
    {
      "start": 7600.32,
      "duration": 4.72,
      "text": "So I see a progression of how this will evolve.\nMaybe at some future point I'm not even that  "
    },
    {
      "start": 7605.04,
      "duration": 2.72,
      "text": "useful and AI is doing most of the \ndesign much better than I could. "
    },
    {
      "start": 7607.76,
      "duration": 2.4,
      "text": "But I still think that's going \nto take some time to play out. "
    },
    {
      "start": 7610.88,
      "duration": 5.84,
      "text": "Are you imagining that people who have expertise \nin other fields are then contributing courses,  "
    },
    {
      "start": 7616.72,
      "duration": 4.4,
      "text": "or do you feel like it's quite \nessential to the vision that you,  "
    },
    {
      "start": 7621.12,
      "duration": 5.36,
      "text": "given your understanding of how you want to \nteach, are the one designing the content? "
    },
    {
      "start": 7627.12,
      "duration": 2.4,
      "text": "Sal Khan is narrating all \nthe videos on Khan Academy. "
    },
    {
      "start": 7629.52,
      "duration": 3.44,
      "text": "Are you imagining something like that?\nNo, I will hire faculty because there  "
    },
    {
      "start": 7632.96,
      "duration": 5.68,
      "text": "are domains in which I'm not an expert.\nThat's the only way to offer the state-of-the-art  "
    },
    {
      "start": 7638.64,
      "duration": 5.52,
      "text": "experience for the student ultimately.\nI do expect that I would hire faculty, but  "
    },
    {
      "start": 7644.16,
      "duration": 5.76,
      "text": "I will probably stick around in AI for some time.\nI do have something more conventional in mind for  "
    },
    {
      "start": 7649.92,
      "duration": 2.96,
      "text": "the current capability than what \npeople would probably anticipate. "
    },
    {
      "start": 7653.68,
      "duration": 4.96,
      "text": "When I'm building Starfleet Academy, I do probably \nimagine a physical institution, and maybe a tier  "
    },
    {
      "start": 7658.64,
      "duration": 5.52,
      "text": "below that a digital offering that is not the \nstate-of-the-art experience you would get when  "
    },
    {
      "start": 7664.16,
      "duration": 4.32,
      "text": "someone comes in physically full-time and we \nwork through material from start to end and  "
    },
    {
      "start": 7668.48,
      "duration": 5.2,
      "text": "make sure you understand it. That's the physical \noffering. The digital offering is a bunch of stuff  "
    },
    {
      "start": 7673.68,
      "duration": 4,
      "text": "on the internet and maybe some LLM assistant.\nIt's a bit more gimmicky in a tier below, but  "
    },
    {
      "start": 7677.68,
      "duration": 7.28,
      "text": "at least it's accessible to 8 billion people.\nI think you're basically inventing college  "
    },
    {
      "start": 7684.96,
      "duration": 6.72,
      "text": "from first principles for the tools that \nare available today and just selecting  "
    },
    {
      "start": 7691.68,
      "duration": 5.76,
      "text": "for people who have the motivation and the \ninterest of really engaging with material. "
    },
    {
      "start": 7698.08,
      "duration": 3.28,
      "text": "There's going to have to be a lot of not \njust education but also re-education. "
    },
    {
      "start": 7701.36,
      "duration": 4.64,
      "text": "I would love to help out there because \nthe jobs will probably change quite a bit. "
    },
    {
      "start": 7707.36,
      "duration": 2.48,
      "text": "For example, today a lot of people are \ntrying to upskill in AI specifically. "
    },
    {
      "start": 7709.84,
      "duration": 2.56,
      "text": "I think it's a really good \ncourse to teach in this respect. "
    },
    {
      "start": 7714.96,
      "duration": 6.16,
      "text": "Motivation-wise, before AGI motivation is very \nsimple to solve because people want to make money. "
    },
    {
      "start": 7721.12,
      "duration": 5.84,
      "text": "This is how you make money in the industry today.\nPost-AGI is a lot more interesting possibly  "
    },
    {
      "start": 7726.96,
      "duration": 2.8,
      "text": "because if everything is automated \nand there's nothing to do for anyone,  "
    },
    {
      "start": 7729.76,
      "duration": 7.92,
      "text": "why would anyone go to a school?\nI often say that pre-AGI education  "
    },
    {
      "start": 7737.68,
      "duration": 7.84,
      "text": "is useful. Post-AGI education is fun. In \na similar way, people go to the gym today. "
    },
    {
      "start": 7746.56,
      "duration": 3.84,
      "text": "We don't need their physical strength \nto manipulate heavy objects because we  "
    },
    {
      "start": 7750.4,
      "duration": 2,
      "text": "have machines that do that.\nThey still go to the gym. "
    },
    {
      "start": 7752.4,
      "duration": 2.72,
      "text": "Why do they go to the gym?\nBecause it's fun, it's healthy,  "
    },
    {
      "start": 7756.24,
      "duration": 7.76,
      "text": "and you look hot when you have a six-pack.\nIt's attractive for people to do that  "
    },
    {
      "start": 7764,
      "duration": 4.88,
      "text": "in a very deep, psychological, \nevolutionary sense for humanity. "
    },
    {
      "start": 7770.32,
      "duration": 3.44,
      "text": "Education will play out in the same way.\nYou'll go to school like you go to the gym. "
    },
    {
      "start": 7776,
      "duration": 4.72,
      "text": "Right now, not that many people learn \nbecause learning is hard. You bounce  "
    },
    {
      "start": 7780.72,
      "duration": 4,
      "text": "from material. Some people overcome that \nbarrier, but for most people, it's hard. "
    },
    {
      "start": 7786,
      "duration": 4.32,
      "text": "It's a technical problem to solve.\nIt's a technical problem to do what my tutor  "
    },
    {
      "start": 7790.32,
      "duration": 3.12,
      "text": "did for me when I was learning Korean.\nIt's tractable and buildable,  "
    },
    {
      "start": 7793.44,
      "duration": 2.32,
      "text": "and someone should build it.\nIt's going to make learning  "
    },
    {
      "start": 7795.76,
      "duration": 4.72,
      "text": "anything trivial and desirable, and people \nwill do it for fun because it's trivial. "
    },
    {
      "start": 7800.48,
      "duration": 4.96,
      "text": "If I had a tutor like that for any arbitrary piece \nof knowledge, it's going to be so much easier to  "
    },
    {
      "start": 7805.44,
      "duration": 2.16,
      "text": "learn anything, and people will do it.\nThey'll do it for the same  "
    },
    {
      "start": 7807.6,
      "duration": 6,
      "text": "reasons they go to the gym.\nThat sounds different from using…  "
    },
    {
      "start": 7814.16,
      "duration": 7.12,
      "text": "So post-AGI, you're using this as \nentertainment or as self-betterment. "
    },
    {
      "start": 7821.28,
      "duration": 3.92,
      "text": "But it sounded like you had a vision \nalso that this education is relevant to  "
    },
    {
      "start": 7825.2,
      "duration": 5.6,
      "text": "keeping humanity in control of AI. That sounds \ndifferent. Is it entertaining for some people,  "
    },
    {
      "start": 7830.8,
      "duration": 2,
      "text": "but then empowerment for some others?\nHow do you think about that? "
    },
    {
      "start": 7832.8,
      "duration": 7.28,
      "text": "I do think eventually it's a bit of \na losing game, if that makes sense. "
    },
    {
      "start": 7841.44,
      "duration": 2.72,
      "text": "It is in the long term.\nIn the long term, which  "
    },
    {
      "start": 7844.16,
      "duration": 3.36,
      "text": "is longer than maybe most people in the \nindustry think about, it's a losing game. "
    },
    {
      "start": 7847.52,
      "duration": 5.92,
      "text": "I do think people can go so far and we've barely \nscratched the surface of how much a person can go. "
    },
    {
      "start": 7853.44,
      "duration": 3.2,
      "text": "That's just because people are bouncing off \nof material that's too easy or too hard. "
    },
    {
      "start": 7859.28,
      "duration": 3.76,
      "text": "People will be able to go much further.\nAnyone will speak five languages because  "
    },
    {
      "start": 7863.04,
      "duration": 6.64,
      "text": "why not? Because it's so trivial. Anyone will know \nall the basic curriculum of undergrad, et cetera. "
    },
    {
      "start": 7869.68,
      "duration": 3.52,
      "text": "Now that I'm understanding the \nvision, that's very interesting. "
    },
    {
      "start": 7874.08,
      "duration": 3.92,
      "text": "It has a perfect analog in gym culture.\nI don't think 100 years  "
    },
    {
      "start": 7878,
      "duration": 4.72,
      "text": "ago anybody would be ripped.\nNobody would have been able to just spontaneously  "
    },
    {
      "start": 7882.72,
      "duration": 6.48,
      "text": "bench two plates or three plates or something.\nIt's very common now because of this idea of  "
    },
    {
      "start": 7889.2,
      "duration": 4.32,
      "text": "systematically training and lifting weights in \nthe gym, or systematically training to be able  "
    },
    {
      "start": 7893.52,
      "duration": 4.96,
      "text": "to run a marathon, which is a capability \nmost humans would not spontaneously have. "
    },
    {
      "start": 7898.48,
      "duration": 5.04,
      "text": "You're imagining similar things for \nlearning across many different domains,  "
    },
    {
      "start": 7903.52,
      "duration": 4.56,
      "text": "much more intensely, deeply, faster.\nExactly. I am betting a bit implicitly  "
    },
    {
      "start": 7908.08,
      "duration": 8.48,
      "text": "on some of the timelessness of human nature.\nIt will be desirable to do all these things,  "
    },
    {
      "start": 7918.24,
      "duration": 3.84,
      "text": "and I think people will look up \nto it as they have for millennia. "
    },
    {
      "start": 7923.6,
      "duration": 3.76,
      "text": "This will continue to be true.\nThere's some evidence of that historically. "
    },
    {
      "start": 7927.36,
      "duration": 3.92,
      "text": "If you look at, for example, aristocrats, or you \nlook at ancient Greece or something like that,  "
    },
    {
      "start": 7931.28,
      "duration": 4.08,
      "text": "whenever you had little pocket environments \nthat were post-AGI in a certain sense, people  "
    },
    {
      "start": 7935.36,
      "duration": 5.36,
      "text": "have spent a lot of their time flourishing in a \ncertain way, either physically or cognitively. "
    },
    {
      "start": 7942.16,
      "duration": 6,
      "text": "I feel okay about the prospects of that.\nIf this is false and I'm wrong and we end up in a  "
    },
    {
      "start": 7949.12,
      "duration": 5.92,
      "text": "WALL-E or Idiocracy future, then I don't even care \nif there are Dyson spheres. This is a terrible  "
    },
    {
      "start": 7955.04,
      "duration": 6.32,
      "text": "outcome. I really do care about humanity.\nEveryone has to just be  "
    },
    {
      "start": 7961.36,
      "duration": 5.2,
      "text": "superhuman in a certain sense.\nIt's still a world in which that is not enabling  "
    },
    {
      "start": 7966.56,
      "duration": 5.04,
      "text": "us to… It's like the culture world, right?\nYou're not fundamentally going to be able  "
    },
    {
      "start": 7971.6,
      "duration": 6.08,
      "text": "to transform the trajectory \nof technology or influence  "
    },
    {
      "start": 7977.68,
      "duration": 5.44,
      "text": "decisions by your own labor or cognition alone.\nMaybe you can influence decisions because the AI  "
    },
    {
      "start": 7983.12,
      "duration": 6.96,
      "text": "is asking for your approval, but it's not because \nI've invented something or I've come up with a new  "
    },
    {
      "start": 7990.08,
      "duration": 5.84,
      "text": "design that I'm really influencing the future.\nMaybe. I think there will be a transitional  "
    },
    {
      "start": 7995.92,
      "duration": 3.2,
      "text": "period where we are going to be \nable to be in the loop and advance  "
    },
    {
      "start": 7999.12,
      "duration": 4.32,
      "text": "things if we understand a lot of stuff.\nIn the long-term, that probably goes away. "
    },
    {
      "start": 8005.04,
      "duration": 3.28,
      "text": "It might even become a sport.\nRight now you have powerlifters  "
    },
    {
      "start": 8008.32,
      "duration": 5.2,
      "text": "who go extreme in this direction.\nWhat is powerlifting in a cognitive era? "
    },
    {
      "start": 8013.52,
      "duration": 3.28,
      "text": "Maybe it's people who are really trying \nto make Olympics out of knowing stuff. "
    },
    {
      "start": 8019.6,
      "duration": 4,
      "text": "If you have a perfect AI tutor, \nmaybe you can get extremely far. "
    },
    {
      "start": 8023.6,
      "duration": 4.88,
      "text": "I feel that the geniuses of \ntoday are barely scratching the  "
    },
    {
      "start": 8028.48,
      "duration": 7.36,
      "text": "surface of what a human mind can do, I think.\nI love this vision. I also feel like the person  "
    },
    {
      "start": 8035.84,
      "duration": 6.4,
      "text": "you have the most product-market fit with is me \nbecause my job involves having to learn different  "
    },
    {
      "start": 8042.24,
      "duration": 8.56,
      "text": "subjects every week, and I am very excited.\nI'm similar, for that matter. A lot of people,  "
    },
    {
      "start": 8050.8,
      "duration": 4.24,
      "text": "for example, hate school and want to get \nout of it. I really liked school. I loved  "
    },
    {
      "start": 8055.04,
      "duration": 1.76,
      "text": "learning things, et cetera.\nI wanted to stay in school. "
    },
    {
      "start": 8056.8,
      "duration": 2.64,
      "text": "I stayed all the way until Ph.D. and \nthen they wouldn't let me stay longer,  "
    },
    {
      "start": 8059.44,
      "duration": 5.6,
      "text": "so I went to the industry.\nRoughly speaking, I love learning,  "
    },
    {
      "start": 8065.04,
      "duration": 4.16,
      "text": "even for the sake of learning, but I also love \nlearning because it's a form of empowerment and  "
    },
    {
      "start": 8069.2,
      "duration": 2.8,
      "text": "being useful and productive.\nYou also made a point that  "
    },
    {
      "start": 8072.8,
      "duration": 3.84,
      "text": "was subtle and I want to spell it out.\nWith what’s happened so far with online  "
    },
    {
      "start": 8076.64,
      "duration": 7.52,
      "text": "courses, why haven't they already enabled us to \nenable every single human to know everything? "
    },
    {
      "start": 8084.72,
      "duration": 6.64,
      "text": "They're just so motivation-laden because there are \nno obvious on-ramps and it's so easy to get stuck. "
    },
    {
      "start": 8092.64,
      "duration": 7.36,
      "text": "If you had this thing instead—like a really \ngood human tutor—it would just be such an  "
    },
    {
      "start": 8100,
      "duration": 4.24,
      "text": "unlock from a motivation perspective.\nI think so. It feels bad to bounce from  "
    },
    {
      "start": 8104.24,
      "duration": 5.28,
      "text": "material. It feels bad. You get negative reward \nfrom sinking an amount of time in something and it  "
    },
    {
      "start": 8109.52,
      "duration": 4.56,
      "text": "doesn't pan out, or being completely bored because \nwhat you're getting is too easy or too hard. "
    },
    {
      "start": 8116.16,
      "duration": 4.48,
      "text": "When you do it properly, learning feels good.\nIt's a technical problem to get there. "
    },
    {
      "start": 8121.92,
      "duration": 4,
      "text": "For a while, it's going to be AI plus human \ncollab, and at some point, maybe it's just AI. "
    },
    {
      "start": 8127.12,
      "duration": 5.76,
      "text": "Can I ask some questions about teaching well?\nIf you had to give advice to another educator  "
    },
    {
      "start": 8132.88,
      "duration": 6.8,
      "text": "in another field that you're curious about to \nmake the kinds of YouTube tutorials you've made. "
    },
    {
      "start": 8140.4,
      "duration": 2.72,
      "text": "Maybe it might be especially interesting \nto talk about domains where you can't  "
    },
    {
      "start": 8143.92,
      "duration": 3.68,
      "text": "test someone's technical understanding by \nhaving them code something up or something. "
    },
    {
      "start": 8147.6,
      "duration": 6.88,
      "text": "What advice would you give them?\nThat's a pretty broad topic. There are 10–20 tips  "
    },
    {
      "start": 8154.48,
      "duration": 8.88,
      "text": "and tricks that I semi-consciously do probably.\nBut a lot of this comes  "
    },
    {
      "start": 8163.36,
      "duration": 2.72,
      "text": "from my physics background.\nI really, really did enjoy my physics background. "
    },
    {
      "start": 8166.08,
      "duration": 4.88,
      "text": "I have a whole rant on how everyone \nshould learn physics in early school  "
    },
    {
      "start": 8170.96,
      "duration": 4.88,
      "text": "education because early school education is \nnot about accumulating knowledge or memory  "
    },
    {
      "start": 8175.84,
      "duration": 3.12,
      "text": "for tasks later in the industry.\nIt's about booting up a brain. "
    },
    {
      "start": 8178.96,
      "duration": 4,
      "text": "Physics uniquely boots up the brain the \nbest because some of the things that they  "
    },
    {
      "start": 8182.96,
      "duration": 3.52,
      "text": "get you to do in your brain during \nphysics is extremely valuable later. "
    },
    {
      "start": 8186.48,
      "duration": 4.88,
      "text": "The idea of building models and abstractions \nand understanding that there's a first-order  "
    },
    {
      "start": 8191.36,
      "duration": 3.44,
      "text": "approximation that describes most of the system, \nbut then there're second-order, third-order,  "
    },
    {
      "start": 8194.8,
      "duration": 4.48,
      "text": "fourth-order terms that may or may not be present.\nThe idea that you're observing a very noisy  "
    },
    {
      "start": 8199.28,
      "duration": 4,
      "text": "system, but there are these fundamental \nfrequencies that you can abstract away. "
    },
    {
      "start": 8203.28,
      "duration": 4.479,
      "text": "When a physicist walks into the class and \nthey say, \"Assume there's a spherical cow,\"  "
    },
    {
      "start": 8208.72,
      "duration": 4.32,
      "text": "everyone laughs at that, but this is brilliant.\nIt's brilliant thinking that's very generalizable  "
    },
    {
      "start": 8213.04,
      "duration": 5.04,
      "text": "across the industry because a cow can be \napproximated as a sphere in a bunch of ways. "
    },
    {
      "start": 8218.8,
      "duration": 5.44,
      "text": "There's a really good book, for example, Scale.\nIt's from a physicist talking about biology. "
    },
    {
      "start": 8224.24,
      "duration": 1.76,
      "text": "Maybe this is also a book \nI would recommend reading. "
    },
    {
      "start": 8226.56,
      "duration": 4.799,
      "text": "You can get a lot of really interesting \napproximations and chart scaling laws of animals. "
    },
    {
      "start": 8231.359,
      "duration": 4.08,
      "text": "You can look at their heartbeats and \nthings like that, and they line up with  "
    },
    {
      "start": 8235.439,
      "duration": 3.521,
      "text": "the size of the animal and things like that.\nYou can talk about an animal as a volume. "
    },
    {
      "start": 8240.88,
      "duration": 4.88,
      "text": "You can talk about the heat dissipation of that, \nbecause your heat dissipation grows as the surface  "
    },
    {
      "start": 8245.76,
      "duration": 4.08,
      "text": "area, which is growing as a square.\nBut your heat creation or generation  "
    },
    {
      "start": 8249.84,
      "duration": 3.28,
      "text": "is growing as a cube.\nSo I just feel like physicists  "
    },
    {
      "start": 8253.12,
      "duration": 2.96,
      "text": "have all the right cognitive tools to \napproach problem solving in the world. "
    },
    {
      "start": 8256.08,
      "duration": 3.2,
      "text": "So because of that training, I \nalways try to find the first-order  "
    },
    {
      "start": 8259.28,
      "duration": 4.159,
      "text": "terms or the second-order terms of everything.\nWhen I'm observing a system or a thing, I have a  "
    },
    {
      "start": 8263.439,
      "duration": 5.521,
      "text": "tangle of a web of ideas or knowledge in my mind.\nI'm trying to find, what is the thing that  "
    },
    {
      "start": 8268.96,
      "duration": 3.6,
      "text": "matters? What is the first-order component? \nHow can I simplify it? How can I have a  "
    },
    {
      "start": 8272.56,
      "duration": 4.721,
      "text": "simplest thing that shows that thing, shows it in \naction, and then I can tack on the other terms? "
    },
    {
      "start": 8278.319,
      "duration": 4.721,
      "text": "Maybe an example from one of my repos that I \nthink illustrates it well is called micrograd. "
    },
    {
      "start": 8283.04,
      "duration": 3.68,
      "text": "I don't know if you're familiar with this.\nSo micrograd is 100 lines of code  "
    },
    {
      "start": 8286.72,
      "duration": 3.599,
      "text": "that shows backpropagation.\nYou can create neural networks  "
    },
    {
      "start": 8290.319,
      "duration": 4.561,
      "text": "out of simple operations like plus and times, et \ncetera. Lego blocks of neural networks. You build  "
    },
    {
      "start": 8294.88,
      "duration": 4.24,
      "text": "up a computational graph and you do a forward \npass and a backward pass to get the gradients. "
    },
    {
      "start": 8299.12,
      "duration": 2.64,
      "text": "Now, this is at the heart of \nall neural network learning. "
    },
    {
      "start": 8301.76,
      "duration": 3.599,
      "text": "So micrograd is a 100 lines of \npretty interpretable Python code,  "
    },
    {
      "start": 8305.359,
      "duration": 3.921,
      "text": "and it can do forward and backward arbitrary \nneural networks, but not efficiently. "
    },
    {
      "start": 8309.28,
      "duration": 3.439,
      "text": "So micrograd, these 100 lines of Python, \nare everything you need to understand how  "
    },
    {
      "start": 8312.72,
      "duration": 5.44,
      "text": "neural networks train. Everything else is just \nefficiency. Everything else is efficiency. There's  "
    },
    {
      "start": 8318.16,
      "duration": 3.199,
      "text": "a huge amount of work to get efficiency.\nYou need your tensors, you lay them out,  "
    },
    {
      "start": 8321.359,
      "duration": 2.16,
      "text": "you stride them, you make sure \nyour kernels, orchestrating  "
    },
    {
      "start": 8323.52,
      "duration": 3.6,
      "text": "memory movement correctly, et cetera.\nIt's all just efficiency, roughly speaking. "
    },
    {
      "start": 8327.12,
      "duration": 3.76,
      "text": "But the core intellectual piece of neural \nnetwork training is micrograd. It's 100 lines.  "
    },
    {
      "start": 8330.88,
      "duration": 4.479,
      "text": "You can easily understand it. It's a recursive \napplication of chain rule to derive the gradient,  "
    },
    {
      "start": 8335.359,
      "duration": 2.721,
      "text": "which allows you to optimize any \narbitrary differentiable function. "
    },
    {
      "start": 8338.08,
      "duration": 8.88,
      "text": "So I love finding these small-order terms and \nserving them on a platter and discovering them. "
    },
    {
      "start": 8346.96,
      "duration": 4.8,
      "text": "I feel like education is the most intellectually \ninteresting thing because you have a tangle  "
    },
    {
      "start": 8351.76,
      "duration": 5.12,
      "text": "of understanding and you're trying to lay \nit out in a way that creates a ramp where  "
    },
    {
      "start": 8356.88,
      "duration": 5.04,
      "text": "everything only depends on the thing before it.\nI find that this untangling of knowledge is just  "
    },
    {
      "start": 8361.92,
      "duration": 5.28,
      "text": "so intellectually interesting as a cognitive task.\nI love doing it personally, but I just  "
    },
    {
      "start": 8367.2,
      "duration": 4,
      "text": "have a fascination with trying to lay things \nout in a certain way. Maybe that helps me. "
    },
    {
      "start": 8371.2,
      "duration": 4.239,
      "text": "It also makes the learning \nexperience so much more motivated. "
    },
    {
      "start": 8375.439,
      "duration": 7.12,
      "text": "Your tutorial on the transformer begins \nwith bigrams, literally a lookup table from,  "
    },
    {
      "start": 8382.56,
      "duration": 4,
      "text": "\"Here's the word right now, or here's \nthe previous word, here's the next word.\" "
    },
    {
      "start": 8386.56,
      "duration": 2.4,
      "text": "It's literally just a lookup table.\nThat’s the essence of it, yeah. "
    },
    {
      "start": 8388.96,
      "duration": 4.64,
      "text": "It’s such a brilliant way, starting with a \nlookup table and then going to a transformer.  "
    },
    {
      "start": 8393.6,
      "duration": 3.44,
      "text": "Each piece is motivated. Why would you add \nthat? Why would you add the next thing? "
    },
    {
      "start": 8397.04,
      "duration": 4.399,
      "text": "You could memorize the attention formula, \nbut having an understanding of why every  "
    },
    {
      "start": 8401.439,
      "duration": 4.561,
      "text": "single piece is relevant, what problem it solves.\nYou're presenting the pain before you present a  "
    },
    {
      "start": 8406,
      "duration": 2.479,
      "text": "solution, and how clever is that?\nYou want to take the student  "
    },
    {
      "start": 8408.479,
      "duration": 3.36,
      "text": "through that progression.\nThere are a lot of other small  "
    },
    {
      "start": 8411.84,
      "duration": 5.76,
      "text": "things that make it nice and engaging and \ninteresting. Always prompting the student.  "
    },
    {
      "start": 8417.6,
      "duration": 4.4,
      "text": "There's a lot of small things like that are \nimportant and a lot of good educators will do  "
    },
    {
      "start": 8422,
      "duration": 5.68,
      "text": "this. How would you solve this? I'm not going to \npresent the solution before you guess. That would  "
    },
    {
      "start": 8427.68,
      "duration": 6.56,
      "text": "be wasteful. That's a little bit of a…I don’t \nwant to swear but it’s a dick move towards you  "
    },
    {
      "start": 8434.24,
      "duration": 3.909,
      "text": "to present you with the solution before I give \nyou a shot to try to come up with it yourself. "
    },
    {
      "start": 8438.149,
      "duration": 8.49,
      "text": "Because if you try to come up with it yourself, \nyou get a better understanding of what the action  "
    },
    {
      "start": 8446.64,
      "duration": 5.84,
      "text": "space is, what the objective is, and then \nwhy only this action fulfills that objective. "
    },
    {
      "start": 8453.439,
      "duration": 4.48,
      "text": "You have a chance to try it yourself, and you \nhave an appreciation when I give you the solution. "
    },
    {
      "start": 8458.64,
      "duration": 2.72,
      "text": "It maximizes the amount of \nknowledge per new fact added. "
    },
    {
      "start": 8462.88,
      "duration": 7.92,
      "text": "Why do you think, by default, people who are \ngenuine experts in their field are often bad  "
    },
    {
      "start": 8470.8,
      "duration": 5.599,
      "text": "at explaining it to somebody ramping up?\nIt's the curse of knowledge and expertise. "
    },
    {
      "start": 8476.399,
      "duration": 3.841,
      "text": "This is a real phenomenon, and I suffered \nfrom it myself as much as I try not to. "
    },
    {
      "start": 8481.04,
      "duration": 2.96,
      "text": "But you take certain things for granted, \nand you can't put yourself in the shoes  "
    },
    {
      "start": 8484,
      "duration": 4.08,
      "text": "of new people who are just starting out.\nThis is pervasive and happens to me as  "
    },
    {
      "start": 8488.08,
      "duration": 4.16,
      "text": "well. One thing that's extremely helpful. \nAs an example, someone was trying to show  "
    },
    {
      "start": 8492.24,
      "duration": 4.88,
      "text": "me a paper in biology recently, and I just \ninstantly had so many terrible questions. "
    },
    {
      "start": 8498.08,
      "duration": 4.64,
      "text": "What I did was I used ChatGPT to ask the \nquestions with the paper in the context window. "
    },
    {
      "start": 8503.92,
      "duration": 3.84,
      "text": "It worked through some of the simple things.\nThen I shared the thread to the person who  "
    },
    {
      "start": 8509.12,
      "duration": 5.68,
      "text": "wrote that paper or worked on that work.\nI felt like if they could see the dumb  "
    },
    {
      "start": 8514.8,
      "duration": 2.88,
      "text": "questions I had, it might help \nthem explain better in the future. "
    },
    {
      "start": 8520.16,
      "duration": 4.72,
      "text": "For my material, I would love it if people \nshared their dumb conversations with ChatGPT  "
    },
    {
      "start": 8524.88,
      "duration": 2.8,
      "text": "about the stuff that I've created \nbecause it really helps me put myself  "
    },
    {
      "start": 8527.68,
      "duration": 7.2,
      "text": "again in the shoes of someone who's starting out.\nAnother trick that just works astoundingly well. "
    },
    {
      "start": 8536.24,
      "duration": 8.96,
      "text": "If somebody writes a paper or a blog post or an \nannouncement, it is in 100% of cases that just  "
    },
    {
      "start": 8545.2,
      "duration": 7.92,
      "text": "the narration or the transcription of how they \nwould explain it to you over lunch is way more,  "
    },
    {
      "start": 8553.12,
      "duration": 6.16,
      "text": "not only understandable, but actually \nalso more accurate and scientific,  "
    },
    {
      "start": 8559.28,
      "duration": 4.8,
      "text": "in the sense that people have a bias \nto explain things in the most abstract,  "
    },
    {
      "start": 8564.88,
      "duration": 3.599,
      "text": "jargon-filled way possible and to clear \ntheir throat for four paragraphs before  "
    },
    {
      "start": 8568.479,
      "duration": 2.96,
      "text": "they explain the central idea.\nBut there's something about  "
    },
    {
      "start": 8571.439,
      "duration": 6.24,
      "text": "communicating one-on-one with a person \nwhich compels you to just say the thing. "
    },
    {
      "start": 8577.68,
      "duration": 2.96,
      "text": "Just say the thing. I saw that \ntweet, I thought it was really good. "
    },
    {
      "start": 8580.64,
      "duration": 4.48,
      "text": "I shared it with a bunch of people.\nI noticed this many, many times. "
    },
    {
      "start": 8586.16,
      "duration": 3.84,
      "text": "The most prominent example is that I \nremember back in my PhD days doing research. "
    },
    {
      "start": 8591.04,
      "duration": 3.52,
      "text": "You read someone's paper, and you \nwork to understand what it's doing. "
    },
    {
      "start": 8595.359,
      "duration": 3.28,
      "text": "Then you catch them, you're having beers \nat the conference later, and you ask them,  "
    },
    {
      "start": 8598.64,
      "duration": 4.56,
      "text": "\"So this paper, what were you doing? What is the \npaper about?\" They will just tell you these three  "
    },
    {
      "start": 8603.2,
      "duration": 3.52,
      "text": "sentences that perfectly captured the essence \nof that paper and totally give you the idea. "
    },
    {
      "start": 8606.72,
      "duration": 4,
      "text": "And you didn't have to read the paper.\nIt's only when you're sitting at the table  "
    },
    {
      "start": 8610.72,
      "duration": 2.639,
      "text": "with a beer or something, and they're \nlike, \"Oh yeah, the paper is just,  "
    },
    {
      "start": 8613.359,
      "duration": 3.841,
      "text": "you take this idea, you take that idea and try \nthis experiment and you try out this thing.\" "
    },
    {
      "start": 8617.2,
      "duration": 3.6,
      "text": "They have a way of just putting it \nconversationally just perfectly.  "
    },
    {
      "start": 8620.8,
      "duration": 6.24,
      "text": "Why isn't that the abstract?\nExactly. This is coming from the  "
    },
    {
      "start": 8627.04,
      "duration": 4.16,
      "text": "perspective of how somebody who's trying to \nexplain an idea should formulate it better. "
    },
    {
      "start": 8631.2,
      "duration": 5.84,
      "text": "What is your advice as a student to other \nstudents, if you don't have a Karpathy  "
    },
    {
      "start": 8637.04,
      "duration": 3.84,
      "text": "who is doing the exposition of an idea?\nIf you're reading a paper from somebody  "
    },
    {
      "start": 8640.88,
      "duration": 6.24,
      "text": "or reading a book, what strategies do \nyou employ to learn material you're  "
    },
    {
      "start": 8647.12,
      "duration": 6.8,
      "text": "interested in in fields you're not an expert at?\nI don't know that I have unique tips and tricks,  "
    },
    {
      "start": 8653.92,
      "duration": 11.12,
      "text": "to be honest. It's a painful process. One thing \nthat has always helped me quite a bit is—I  "
    },
    {
      "start": 8666.8,
      "duration": 4.88,
      "text": "had a small tweet about this—learning things \non demand is pretty nice. Learning depth-wise.  "
    },
    {
      "start": 8671.68,
      "duration": 3.44,
      "text": "I do feel you need a bit of alternation of \nlearning depth-wise, on demand—you're trying  "
    },
    {
      "start": 8675.12,
      "duration": 3.359,
      "text": "to achieve a certain project that you're going \nto get a reward from—and learning breadth-wise,  "
    },
    {
      "start": 8678.479,
      "duration": 3.84,
      "text": "which is just, \"Oh, let's do whatever 101, \nand here's all the things you might need.\" "
    },
    {
      "start": 8682.319,
      "duration": 3.04,
      "text": "Which is a lot of school—does breadth-wise \nlearning, like, \"Oh, trust me, you'll need  "
    },
    {
      "start": 8685.359,
      "duration": 5.601,
      "text": "this later,\" that kind of stuff. Okay, I trust \nyou. I'll learn it because I guess I need it. "
    },
    {
      "start": 8690.96,
      "duration": 2.88,
      "text": "But I love the kind of learning \nwhere you'll get a reward out of  "
    },
    {
      "start": 8693.84,
      "duration": 4.16,
      "text": "doing something, and you're learning on demand.\nThe other thing that I've found extremely helpful. "
    },
    {
      "start": 8699.84,
      "duration": 5.04,
      "text": "This is an aspect where education is a bit more \nselfless, but explaining things to people is a  "
    },
    {
      "start": 8704.88,
      "duration": 4.16,
      "text": "beautiful way to learn something more deeply.\nThis happens to me all the time. "
    },
    {
      "start": 8709.04,
      "duration": 4.319,
      "text": "It probably happens to other people too because \nI realize if I don't really understand something,  "
    },
    {
      "start": 8713.359,
      "duration": 3.841,
      "text": "I can't explain it.\nI'm trying and I'm like,  "
    },
    {
      "start": 8717.2,
      "duration": 4.08,
      "text": "\"Oh, I don't understand this.\"\nIt's so annoying to come to terms with that. "
    },
    {
      "start": 8721.28,
      "duration": 4.079,
      "text": "You can go back and make sure you understood it.\nIt fills these gaps of your understanding. "
    },
    {
      "start": 8725.359,
      "duration": 3.12,
      "text": "It forces you to come to terms \nwith them and to reconcile them. "
    },
    {
      "start": 8728.479,
      "duration": 4.801,
      "text": "I love to re-explain things and people \nshould be doing that more as well. "
    },
    {
      "start": 8733.28,
      "duration": 2.96,
      "text": "That forces you to manipulate the knowledge \nand make sure that you know what you're  "
    },
    {
      "start": 8736.24,
      "duration": 4.159,
      "text": "talking about when you're explaining it.\nThat's an excellent note to close on. Andrej,  "
    },
    {
      "start": 8740.399,
      "duration": 1.761,
      "text": "that was great.\nThank you."
    }
  ],
  "fullText": "Today I'm speaking with Andrej Karpathy.\nAndrej, why do you say that this will be   the decade of agents and not the year of agents?\nFirst of all, thank you for having me here. I'm   excited to be here. The quote you've just \nmentioned, \"It's the decade of agents,\"   is actually a reaction to a pre-existing quote.\nI'm not actually sure who said this but they were   alluding to this being the year of agents with \nrespect to LLMs and how they were going to evolve.  I was triggered by that because there's some \nover-prediction going on in the industry.  In my mind, this is more accurately \ndescribed as the decade of agents.  We have some very early agents that \nare extremely impressive and that I   use daily—Claude and Codex and so on—but I \nstill feel there's so much work to be done.  My reaction is we'll be working \nwith these things for a decade.  They're going to get better, \nand it's going to be wonderful.  I was just reacting to the \ntimelines of the implication.  What do you think will take a decade to \naccomplish? What are the bottlenecks?  Actually making it work. When you're talking \nabout an agent, or what the labs have in mind   and maybe what I have in mind as well, you \nshould think of it almost like an employee or   an intern that you would hire to work with you.\nFor example, you work with some employees here.  When would you prefer to have an agent like \nClaude or Codex do that work? Currently,   of course they can't. What would it \ntake for them to be able to do that?  Why don't you do it today?\nThe reason you don't do it   today is because they just don't work.\nThey don't have enough intelligence,   they're not multimodal enough, they \ncan't do computer use and all this stuff.  They don't do a lot of the things you've \nalluded to earlier. They don't have   continual learning. You can't just tell \nthem something and they'll remember it.  They're cognitively lacking \nand it's just not working.  It will take about a decade to \nwork through all of those issues.  Interesting. As a professional podcaster \nand a viewer of AI from afar, it's easy   for me to identify what's lacking: continual \nlearning is lacking, or multimodality is lacking.  But I don't really have a good way \nof trying to put a timeline on it.  If somebody asks how long continual learning \nwill take, I have no prior about whether   this is a project that should take 5 \nyears, 10 years, or 50 years. Why a   decade? Why not one year? Why not 50 years?\nThis is where you get into a bit of my own   intuition, and doing a bit of an extrapolation \nwith respect to my own experience in the field.  I've been in AI for almost two decades.\nIt's going to be 15 years or so, not that long.  You had Richard Sutton here, \nwho was around for much longer.  I do have about 15 years of experience of people \nmaking predictions, of seeing how they turned out.  Also I was in the industry for \na while, I was in research,   and I've worked in the industry for a while.  I have a general intuition \nthat I have left from that.  I feel like the problems are tractable, they're \nsurmountable, but they're still difficult.  If I just average it out, it \njust feels like a decade to me.  This is quite interesting. I want \nto hear not only the history,   but what people in the room felt was about to \nhappen at various different breakthrough moments.  What were the ways in which their feelings were \neither overly pessimistic or overly optimistic?  Should we just go through each of them one by one?\nThat's a giant question because you're talking   about 15 years of stuff that happened.\nAI is so wonderful because there have been   a number of seismic shifts where the entire \nfield has suddenly looked a different way.  I've maybe lived through two or three of those.\nI still think there will continue to be   some because they come with \nalmost surprising regularity.  When my career began, when I started to work on \ndeep learning, when I became interested in deep   learning, this was by chance of being right next \nto Geoff Hinton at the University of Toronto.  Geoff Hinton, of course, is \nthe godfather figure of AI.  He was training all these neural networks.\nI thought it was incredible and interesting.  This was not the main thing that \neveryone in AI was doing by far.  This was a niche little subject on the side.\nThat's maybe the first dramatic seismic shift   that came with the AlexNet and so on.\nAlexNet reoriented everyone, and everyone   started to train neural networks, but it \nwas still very per-task, per specific task.  Maybe I have an image classifier or I have a \nneural machine translator or something like that.  People became very slowly interested in agents.\nPeople started to think, \"Okay, maybe we have a   check mark next to the visual cortex or something \nlike that, but what about the other parts of the   brain, and how can we get a full agent or a \nfull entity that can interact in the world?\"  The Atari deep reinforcement learning shift \nin 2013 or so was part of that early effort   of agents, in my mind, because it was an \nattempt to try to get agents that not just   perceive the world, but also take actions and \ninteract and get rewards from environments.  At the time, this was Atari games.\nI feel that was a misstep.  It was a misstep that even the early OpenAI that \nI was a part of adopted because at that time,   the zeitgeist was reinforcement learning \nenvironments, games, game playing,   beat games, get lots of different types of \ngames, and OpenAI was doing a lot of that.  That was another prominent part of AI where maybe \nfor two or three or four years, everyone was doing   reinforcement learning on games.\nThat was all a bit of a misstep.  What I was trying to do at OpenAI is \nI was always a bit suspicious of games   as being this thing that would lead to AGI.\nBecause in my mind, you want something like   an accountant or something that's \ninteracting with the real world.  I just didn't see how games add up to it.\nMy project at OpenAI, for example, was within the   scope of the Universe project, on an agent that \nwas using keyboard and mouse to operate web pages.  I really wanted to have something that \ninteracts with the actual digital world   that can do knowledge work.\nIt just so turns out that this   was extremely early, way too early, so early \nthat we shouldn't have been working on that.  Because if you're just stumbling your way around \nand keyboard mashing and mouse clicking and trying   to get rewards in these environments, your \nreward is too sparse and you just won't learn.  You're going to burn a forest \ncomputing, and you're never   going to get something off the ground.\nWhat you're missing is this power of   representation in the neural network.\nFor example, today people are training   those computer-using agents, but they're \ndoing it on top of a large language model.  You have to get the language model first, \nyou have to get the representations first,   and you have to do that by all the \npre-training and all the LLM stuff.  I feel maybe loosely speaking, people \nkept trying to get the full thing too   early a few times, where people really try \nto go after agents too early, I would say.  That was Atari and Universe \nand even my own experience.  You actually have to do some things \nfirst before you get to those agents.  Now the agents are a lot more competent, but maybe \nwe're still missing some parts of that stack.  I would say those are the three major \nbuckets of what people were doing:   training neural nets per-tasks, \ntrying the first round of agents,   and then maybe the LLMs and seeking the \nrepresentation power of the neural networks   before you tack on everything else on top.\nInteresting. If I were to steelman the Sutton   perspective, it would be that humans \ncan just take on everything at once,   or even animals can take on everything at once.\nAnimals are maybe a better example because they   don't even have the scaffold of language.\nThey just get thrown out into the world,   and they just have to make sense \nof everything without any labels.  The vision for AGI then should just be \nsomething which looks at sensory data,   looks at the computer screen, and it just \nfigures out what's going on from scratch.  If a human were put in a similar situation and \nhad to be trained from scratch… This is like a   human growing up or an animal growing up.\nWhy shouldn't that be the vision for AI,   rather than this thing where we're \ndoing millions of years of training?  That's a really good question. Sutton was \non your podcast and I saw the podcast and I   had a write-up about that podcast that \ngets into a bit of how I see things.  I'm very careful to make analogies to \nanimals because they came about by a   very different optimization process.\nAnimals are evolved, and they come   with a huge amount of hardware that's built in.\nFor example, my example in the post was the zebra.  A zebra gets born, and a few minutes later \nit's running around and following its mother.  That's an extremely complicated thing to do. \nThat's not reinforcement learning. That's   something that's baked in. Evolution obviously \nhas some way of encoding the weights of our   neural nets in ATCGs, and I have no idea \nhow that works, but it apparently works.  Brains just came from a very different process, \nand I'm very hesitant to take inspiration from it   because we're not actually running that process.\nIn my post, I said we're not building animals.  We're building ghosts or spirits or \nwhatever people want to call it, because   we're not doing training by evolution.\nWe're doing training by imitation of humans   and the data that they've put on the Internet.\nYou end up with these ethereal spirit entities   because they're fully digital \nand they're mimicking humans.  It's a different kind of intelligence.\nIf you imagine a space of intelligences,   we're starting off at a different point almost. \nWe're not really building animals. But it's also   possible to make them a bit more animal-like \nover time, and I think we should be doing that.   One more point. I do feel Sutton has a very...\nHis framework is, \"We want to build animals.\"  I think that would be wonderful if we can \nget that to work. That would be amazing.   If there were a single algorithm that you \ncan just run on the Internet and it learns   everything, that would be incredible.\nI'm not sure that it exists and that's   certainly not what animals do, because \nanimals have this outer loop of evolution.  A lot of what looks like learning is \nmore like maturation of the brain.  I think there's very little \nreinforcement learning for animals.  A lot of the reinforcement learning is more \nlike motor tasks; it's not intelligence tasks.  So I actually kind of think humans \ndon’t really use RL, roughly speaking.  Can you repeat the last sentence?\nA lot of that intelligence is   not motor task…it's what, sorry?\nA lot of the reinforcement learning, in my   perspective, would be things that are a lot more \nmotor-like, simple tasks like throwing a hoop.  But I don't think that humans use reinforcement \nlearning for a lot of intelligence tasks   like problem-solving and so on.\nThat doesn't mean we shouldn't   do that for research, but I just feel \nlike that's what animals do or don't.  I'm going to take a second to digest that \nbecause there are a lot of different ideas.  Here’s one clarifying question I can \nask to understand the perspective.  You suggest that evolution is doing \nthe kind of thing that pre-training   does in the sense of building something \nwhich can then understand the world.  The difference is that evolution \nhas to be titrated in the case   of humans through three gigabytes of DNA.\nThat's very unlike the weights of a model.  Literally, the weights of the model are \na brain, which obviously does not exist   in the sperm and the egg.\nSo it has to be grown.  Also, the information for every single \nsynapse in the brain simply cannot exist   in the three gigabytes that exist in the DNA.\nEvolution seems closer to finding the algorithm   which then does the lifetime learning.\nNow, maybe the lifetime learning is   not analogous to RL, to your point.\nIs that compatible with the thing you   were saying, or would you disagree with that?\nI think so. I would agree with you that there's   some miraculous compression \ngoing on because obviously,   the weights of the neural net are not stored \nin ATCGs. There's some dramatic compression.   There are some learning algorithms encoded that \ntake over and do some of the learning online.  I definitely agree with you on that.\nI would say I'm a lot more practically minded.  I don't come at it from the \nperspective of, let's build animals.  I come from it from the perspective \nof, let's build useful things.  I have a hard hat on, and I'm just \nobserving that we're not going to do   evolution, because I don't know how to do that.\nBut it does turn out we can build these ghosts,   spirit-like entities, by imitating internet \ndocuments. This works. It's a way to bring you   up to something that has a lot of built-in \nknowledge and intelligence in some way,   similar to maybe what evolution has done.\nThat's why I call pre-training   this crappy evolution.\nIt's the practically possible version   with our technology and what we have available \nto us to get to a starting point where we can do   things like reinforcement learning and so on.\nJust to steelman the other perspective,   after doing this Sutton interview and thinking \nabout it a bit, he has an important point here.  Evolution does not give us the knowledge, really.\nIt gives us the algorithm to find the knowledge,   and that seems different from pre-training.\nPerhaps the perspective is that pre-training helps   build the kind of entity which can learn better.\nIt teaches meta-learning, and therefore   it is similar to finding an algorithm.\nBut if it's \"Evolution gives us knowledge,   pre-training gives us knowledge,\" \nthat analogy seems to break down.  It's subtle and I think you're right to \npush back on it, but basically the thing   that pre-training is doing, you're getting \nthe next-token predictor over the internet,   and you're training that into a neural net.\nIt's doing two things that are unrelated.  Number one, it's picking up all \nthis knowledge, as I call it.  Number two, it's actually becoming intelligent.\nBy observing the algorithmic patterns in the   internet, it boots up all these little circuits \nand algorithms inside the neural net to do things   like in-context learning and all this stuff.\nYou don't need or want the knowledge.  I think that's probably holding back the neural \nnetworks overall because it's getting them to rely   on the knowledge a little too much sometimes.\nFor example, I feel agents, one thing they're   not very good at, is going off the data \nmanifold of what exists on the internet.  If they had less knowledge or less \nmemory, maybe they would be better.  What I think we have to do going forward—and \nthis would be part of the research paradigms—is   figure out ways to remove some of the knowledge \nand to keep what I call this cognitive core.  It's this intelligent entity that is stripped from \nknowledge but contains the algorithms and contains   the magic of intelligence and problem-solving \nand the strategies of it and all this stuff.  There's so much interesting stuff there. Let's \nstart with in-context learning. This is an   obvious point, but I think it's worth just \nsaying it explicitly and meditating on it.  The situation in which these models seem the most \nintelligent—in which I talk to them and I'm like,   \"Wow, there's really something on the other end \nthat's responding to me thinking about things—is   if it makes a mistake it's like, \"Oh wait, that's \nthe wrong way to think about it. I'm backing up.\"   All that is happening in context.\nThat's where I feel like the real   intelligence is that you can visibly see.\nThat in-context learning process is   developed by gradient descent on pre-training.\nIt spontaneously meta-learns in-context learning,   but the in-context learning itself is not \ngradient descent, in the same way that our   lifetime intelligence as humans to be able \nto do things is conditioned by evolution   but our learning during our lifetime is \nhappening through some other process.  I don't fully agree with that, but \nyou should continue your thought.  Well, I'm very curious to understand \nhow that analogy breaks down.  I'm hesitant to say that in-context \nlearning is not doing gradient descent.  It's not doing explicit gradient descent.\nIn-context learning is pattern completion   within a token window.\nIt just turns out that there's   a huge amount of patterns on the internet.\nYou're right, the model learns to complete   the pattern, and that's inside the weights.\nThe weights of the neural network are trying   to discover patterns and complete the pattern.\nThere's some adaptation that happens inside   the neural network, which is magical \nand just falls out from the internet   just because there's a lot of patterns.\nI will say that there have been some papers   that I thought were interesting that look at \nthe mechanisms behind in-context learning.  I do think it's possible that in-context \nlearning runs a small gradient descent loop   internally in the layers of the neural network.\nI recall one paper in particular where they were   doing linear regression using in-context learning.\nYour inputs into the neural network are XY pairs,   XY, XY, XY that happen to be on the line.\nThen you do X and you expect Y.  The neural network, when you train it \nin this way, does linear regression.  Normally when you would run linear regression, you \nhave a small gradient descent optimizer that looks   at XY, looks at an error, calculates the gradient \nof the weights and does the update a few times.  It just turns out that when they looked at the \nweights of that in-context learning algorithm,   they found some analogies to \ngradient descent mechanics.  In fact, I think the paper was even stronger \nbecause they hardcoded the weights of a neural   network to do gradient descent through \nattention and all the internals of the   neural network. That's just my only pushback. \nWho knows how in-context learning works,   but I think that it's probably doing a bit of some \nfunky gradient descent internally. I think that   that's possible. I was only pushing back on your \nsaying that it's not doing in-context learning.  Who knows what it's doing, but it's probably maybe \ndoing something similar to it, but we don't know.  So then it's worth thinking okay, if \nin-context learning and pre-training   are both implementing something like gradient \ndescent, why does it feel like with in-context   learning we're getting to this continual \nlearning, real intelligence-like thing?  Whereas you don't get the analogous feeling just \nfrom pre-training. You could argue that. If it's   the same algorithm, what could be different?\nOne way you could think about it is,   how much information does the model store \nper information it receives from training?  If you look at pre-training, if \nyou look at Llama 3 for example,   I think it's trained on 15 trillion tokens.\nIf you look at the 70B model, that would   be the equivalent of 0.07 bits per \ntoken that it sees in pre-training,   in terms of the information in the weights \nof the model compared to the tokens it reads.  Whereas if you look at the KV cache \nand how it grows per additional token   in in-context learning, it's like 320 kilobytes.\nSo that's a 35 million-fold difference in how much   information per token is assimilated by the model.\nI wonder if that's relevant at all.  I kind of agree. The way I usually put this is \nthat anything that happens during the training of   the neural network, the knowledge is only a hazy \nrecollection of what happened in training time.  That's because the compression is dramatic.\nYou're taking 15 trillion tokens and you're   compressing it to just your final neural \nnetwork of a few billion parameters.  Obviously it's a massive \namount of compression going on.  So I refer to it as a hazy \nrecollection of the internet documents.  Whereas anything that happens in the \ncontext window of the neural network—you're   plugging in all the tokens and building up \nall those KV cache representations—is very   directly accessible to the neural net.\nSo I compare the KV cache and the stuff   that happens at test time to \nmore like a working memory.  All the stuff that's in the context window is \nvery directly accessible to the neural net.  There's always these almost surprising \nanalogies between LLMs and humans.  I find them surprising because we're not \ntrying to build a human brain directly.  We're just finding that this \nworks and we're doing it.  But I do think that anything \nthat's in the weights, it's a   hazy recollection of what you read a year ago.\nAnything that you give it as a context at test   time is directly in the working memory.\nThat's a very powerful analogy to   think through things.\nWhen you, for example,   go to an LLM and you ask it about some book \nand what happened in it, like Nick Lane's   book or something like that, the LLM will often \ngive you some stuff which is roughly correct.  But if you give it the full chapter and ask it \nquestions, you're going to get much better results   because it's now loaded in the \nworking memory of the model.  So a very long way of saying \nI agree and that's why.  Stepping back, what is the part \nabout human intelligence that we   have most failed to replicate with these models?  Just a lot of it. So maybe one way to think \nabout it, I don't know if this is the best way,   but I almost feel like — again, making these \nanalogies imperfect as they are — we've stumbled   by with the transformer neural network, \nwhich is extremely powerful, very general.  You can train transformers on audio, or \nvideo, or text, or whatever you want,   and it just learns patterns and they're \nvery powerful, and it works really well.  That to me almost indicates that this \nis some piece of cortical tissue.  It's something like that, because the \ncortex is famously very plastic as well.  You can rewire parts of brains.\nThere were the slightly gruesome experiments   with rewiring the visual cortex to the auditory \ncortex, and this animal learned fine, et cetera.  So I think that this is cortical tissue.\nI think when we're doing reasoning and   planning inside the neural networks, doing \nreasoning traces for thinking models,   that's kind of like the prefrontal cortex.\nMaybe those are like little checkmarks,   but I still think there are many brain \nparts and nuclei that are not explored.  For example, there's a basal ganglia doing a \nbit of reinforcement learning when we fine-tune   the models on reinforcement learning. But where's \nthe hippocampus? Not obvious what that would be.  Some parts are probably not important.\nMaybe the cerebellum is not important   to cognition, its thoughts, so \nmaybe we can skip some of it.  But I still think there's, for example, the \namygdala, all the emotions and instincts.  There's probably a bunch of other nuclei \nin the brain that are very ancient that   I don't think we've really replicated.\nI don't know that we should be pursuing the   building of an analog of a human brain.\nI'm an engineer mostly at heart.  Maybe another way to answer the question is that \nyou're not going to hire this thing as an intern.  It's missing a lot of it because it comes with \na lot of these cognitive deficits that we all   intuitively feel when we talk to the models.\nSo it's not fully there yet.  You can look at it as not all the \nbrain parts are checked off yet.  This is maybe relevant to the question of thinking \nabout how fast these issues will be solved.  Sometimes people will say \nabout continual learning,   \"Look, you could easily replicate this capability.\nJust as in-context learning emerged spontaneously   as a result of pre-training, continual \nlearning over longer horizons will emerge   spontaneously if the model is incentivized to \nrecollect information over longer horizons,   or horizons longer than one session.\"\nSo if there's some outer loop RL which has   many sessions within that outer loop, then this \ncontinual learning where it fine-tunes itself,   or it writes to an external memory or \nsomething, will just emerge spontaneously.  Do you think things like that are plausible?\nI just don't have a prior over   how plausible that is.\nHow likely is that to happen?  I don't know that I fully resonate with that.\nThese models, when you boot them up and they have   zero tokens in the window, they're always \nrestarting from scratch where they were.  So I don't know in that \nworldview what it looks like.  Maybe making some analogies to humans—just because \nI think it's roughly concrete and interesting to   think through—I feel like when I'm awake, I'm \nbuilding up a context window of stuff that's   happening during the day.\nBut when I go to sleep,   something magical happens where I don't \nthink that context window stays around.  There's some process of distillation \ninto the weights of my brain.  This happens during sleep and all this stuff.\nWe don't have an equivalent   of that in large language models.\nThat's to me more adjacent to when you talk   about continual learning and so on as absent.\nThese models don't really have a distillation   phase of taking what happened, analyzing it \nobsessively, thinking through it, doing some   synthetic data generation process and distilling \nit back into the weights, and maybe having   a specific neural net per person. Maybe it's \na LoRA. It's not a full-weight neural network.  It's just some small sparse subset \nof the weights that are changed.  But we do want to create ways of creating \nthese individuals that have very long context.  It's not only remaining in the context window \nbecause the context windows grow very, very long.  Maybe we have some very elaborate, \nsparse attention over it.  But I still think that humans obviously have some \nprocess for distilling some of that knowledge   into the weights. We're missing it. I do also \nthink that humans have some very elaborate,   sparse attention scheme, which I think \nwe're starting to see some early hints of.  DeepSeek v3.2 just came out and I saw that they \nhave sparse attention as an example, and this is   one way to have very, very long context windows.\nSo I feel like we are redoing a lot of the   cognitive tricks that evolution came up \nwith through a very different process.  But we're going to converge on a \nsimilar architecture cognitively.  In 10 years, do you think it'll still be something \nlike a transformer, but with much more modified   attention and more sparse MLPs and so forth?\nThe way I like to think about it is   translation invariance in time.\nSo 10 years ago, where were we? 2015.  In 2015, we had convolutional neural networks \nprimarily, residual networks just came out.  So remarkably similar, I guess, but quite a \nbit different still. The transformer was not   around. All these more modern tweaks \non the transformer were not around.  Maybe some of the things that we can bet on, I \nthink in 10 years by translational equivariance,   is that we're still training giant neural \nnetworks with a forward backward pass and   update through gradient descent, \nbut maybe it looks a bit different,   and it's just that everything is much bigger.\nRecently I went back all the way to 1989 which   was a fun exercise for me, a few years ago, \nbecause I was reproducing Yann LeCun's 1989   convolutional network, which was the first neural \nnetwork I'm aware of trained via gradient descent,   like modern neural network trained \ngradient descent on digit recognition.  I was just interested in \nhow I could modernize this.  How much of this is algorithms?\nHow much of this is data?  How much of this progress is compute and systems?\nI was able to very quickly halve the learning   just by time traveling by 33 years.\nSo if I time travel by algorithms 33 years,   I could adjust what Yann LeCun did \nin 1989, and I could halve the error.  But to get further gains, I \nhad to add a lot more data,   I had to 10x the training set, and then I \nhad to add more computational optimizations.  I had to train for much longer with dropout \nand other regularization techniques.  So all these things have \nto improve simultaneously.  We're probably going to have a lot more \ndata, we're probably going to have a lot   better hardware, probably going to have a lot \nbetter kernels and software, we're probably   going to have better algorithms.\nAll of those, it's almost like   no one of them is winning too much.\nAll of them are surprisingly equal.  This has been the trend for a while.\nSo to answer your question, I expect differences   algorithmically to what's happening today.\nBut I do also expect that some of the   things that have stuck around for a very \nlong time will probably still be there.  It's probably still a giant neural network trained \nwith gradient descent. That would be my guess.  It's surprising that all of those things \ntogether only halved the error, 30 years   of progress…. Maybe half is a lot. Because if \nyou halve the error, that actually means that…  Half is a lot. But I guess what was shocking to me \nis everything needs to improve across the board:   architecture, optimizer, loss function.\nIt also has improved across the board forever.  So I expect all those \nchanges to be alive and well.  Yeah. I was about to ask you a very \nsimilar question about nanochat.  Since you just coded it up recently, \nevery single step in the process of   building a chatbot is fresh in your RAM.\nI'm curious if you had similar thoughts about,   \"Oh, there was no one thing that was \nrelevant to going from GPT-2 to nanochat.\"  What are some surprising \ntakeaways from the experience?  Of building nanochat? So nanochat \nis a repository I released.  Was it yesterday or the day \nbefore? I can't remember.  We can see the sleep \ndeprivation that went into the…  It's trying to be the simplest complete \nrepository that covers the whole pipeline   end-to-end of building a ChatGPT clone.\nSo you have all of the steps, not just   any individual step, which is a bunch.\nI worked on all the individual steps   in the past and released small pieces \nof code that show you how that's done   in an algorithmic sense, in simple code.\nBut this handles the entire pipeline.  In terms of learning, I don't \nknow that I necessarily found   something that I learned from it.\nI already had in my mind how you build it.  This is just the process of mechanically building \nit and making it clean enough so that people can   learn from it and that they find it useful.\nWhat is the best way for somebody   to learn from it?\nIs it to just delete   all the code and try to reimplement from \nscratch, try to add modifications to it?  That's a great question. Basically \nit's about 8,000 lines of code   that takes you through the entire pipeline.\nI would probably put it on the right monitor.  If you have two monitors, you put it on the right.\nYou want to build it from scratch,   you build it from the start.\nYou're not allowed to copy-paste, you're allowed   to reference, you're not allowed to copy-paste.\nMaybe that's how I would do it.  But I also think the repository \nby itself is a pretty large beast.  When you write this code, you don't go from top \nto bottom, you go from chunks and you grow the   chunks, and that information is absent.\nYou wouldn't know where to start.  So it's not just a final repository that's \nneeded, it's the building of the repository,   which is a complicated chunk-growing process.\nSo that part is not there yet.  I would love to add that probably later this week.\nIt's probably a video or something like that.  Roughly speaking, that's what I would try to do.\nBuild the stuff yourself, but don't allow   yourself copy-paste.\nI do think that   there's two types of knowledge, almost.\nThere's the high-level surface knowledge, but when   you build something from scratch, you're forced to \ncome to terms with what you don't understand and   you don't know that you don't understand it.\nIt always leads to a deeper understanding.  It's the only way to build.\nIf I can't build it, I don't understand it.  That’s a Feynman quote, I believe.\nI 100% have always believed this very   strongly, because there are all these micro \nthings that are just not properly arranged   and you don't really have the knowledge.\nYou just think you have the knowledge.  So don't write blog posts, don't \ndo slides, don't do any of that.  Build the code, arrange it, get it to work.\nIt's the only way to go. Otherwise,   you're missing knowledge.\nYou tweeted out that coding models   were of very little help to you in assembling \nthis repository. I'm curious why that was.  I guess I built the repository over \na period of a bit more than a month.  I would say there are three major classes \nof how people interact with code right now.  Some people completely reject all of LLMs \nand they are just writing by scratch.  This is probably not the \nright thing to do anymore.  The intermediate part, which is where I am, is \nyou still write a lot of things from scratch,   but you use the autocomplete that's \navailable now from these models.  So when you start writing out a little \npiece of it, it will autocomplete for   you and you can just tap through.\nMost of the time it's correct,   sometimes it's not, and you edit it.\nBut you're still very much the   architect of what you're writing.\nThen there's the vibe coding: \"Hi,   please implement this or that,\" enter, and then \nlet the model do it. That's the agents. I do feel   like the agents work in very specific settings, \nand I would use them in specific settings.  But these are all tools available to you \nand you have to learn what they're good at,   what they're not good at, and when to use them.\nSo the agents are pretty good, for example,   if you're doing boilerplate stuff.\nBoilerplate code that's just   copy-paste stuff, they're very good at that.\nThey're very good at stuff that occurs very often   on the Internet because there are lots of examples \nof it in the training sets of these models.  There are features of things where \nthe models will do very well.  I would say nanochat is not an example of \nthose because it's a fairly unique repository.  There's not that much code in the way that \nI've structured it. It's not boilerplate code.   It's intellectually intense code almost, and \neverything has to be very precisely arranged.  The models have so many cognitive deficits.\nOne example, they kept misunderstanding the code   because they have too much memory from \nall the typical ways of doing things on   the Internet that I just wasn't adopting.\nThe models, for example—I don't know if I   want to get into the full details—but they kept \nthinking I'm writing normal code, and I'm not.  Maybe one example?\nYou have eight GPUs   that are all doing forward, backwards.\nThe way to synchronize gradients between   them is to use a Distributed Data Parallel \ncontainer of PyTorch, which automatically   as you're doing the backward, it will start \ncommunicating and synchronizing gradients.  I didn't use DDP because I didn't want \nto use it, because it's not necessary.  I threw it out and wrote my own synchronization \nroutine that's inside the step of the optimizer.  The models were trying to get me to \nuse the DDP container. They were very   concerned. This gets way too technical, \nbut I wasn't using that container because   I don't need it and I have a custom \nimplementation of something like it.  They just couldn't internalize \nthat you had your own.  They couldn't get past that. They kept trying to \nmess up the style. They're way too over-defensive.   They make all these try-catch statements.\nThey keep trying to make a production code base,   and I have a bunch of assumptions \nin my code, and it's okay.  I don't need all this extra stuff in there.\nSo I feel like they're bloating the code base,   bloating the complexity, they keep \nmisunderstanding, they're using   deprecated APIs a bunch of times. It's a total \nmess. It's just not net useful. I can go in,   I can clean it up, but it's not net useful.\nI also feel like it's annoying to have to   type out what I want in English \nbecause it's too much typing.  If I just navigate to the part of the code that I \nwant, and I go where I know the code has to appear   and I start typing out the first few letters, \nautocomplete gets it and just gives you the code.  This is a very high information \nbandwidth to specify what you want.  You point to the code where you want \nit, you type out the first few pieces,   and the model will complete it.\nSo what I mean is, these models   are good in certain parts of the stack.\nThere are two examples where I use the   models that I think are illustrative.\nOne was when I generated the report.  That's more boilerplate-y, so I \npartially vibe-coded some of that stuff.  That was fine because it's not \nmission-critical stuff, and it works fine.  The other part is when I was \nrewriting the tokenizer in Rust.  I'm not as good at Rust \nbecause I'm fairly new to Rust.  So there's a bit of vibe coding going on \nwhen I was writing some of the Rust code.  But I had a Python implementation that I \nfully understand, and I'm just making sure   I'm making a more efficient version of it, and \nI have tests so I feel safer doing that stuff.  They increase accessibility to languages or \nparadigms that you might not be as familiar with.  I think they're very helpful there as well.\nThere's a ton of Rust code out there,   the models are pretty good at it.\nI happen to not know that much about it,   so the models are very useful there.\nThe reason this question is so interesting   is because the main story people \nhave about AI exploding and getting   to superintelligence pretty rapidly is AI \nautomating AI engineering and AI research.  They'll look at the fact that you can have \nClaude Code and make entire applications,   CRUD applications, from scratch and think, \"If \nyou had this same capability inside of OpenAI   and DeepMind and everything, just imagine \na thousand of you or a million of you in   parallel, finding little architectural tweaks.\"\nIt's quite interesting to hear you say that this   is the thing they're asymmetrically worse at.\nIt's quite relevant to forecasting whether   the AI 2027-type explosion is \nlikely to happen anytime soon.  That's a good way of putting it, and you're \ngetting at why my timelines are a bit longer.   You're right. They're not very good at code \nthat has never been written before, maybe it's   one way to put it, which is what we're trying \nto achieve when we're building these models.  Very naive question, but the architectural \ntweaks that you're adding to nanochat,   they're in a paper somewhere, right?\nThey might even be in a repo somewhere.  Is it surprising that they aren't able to \nintegrate that into whenever you're like,   \"Add RoPE embeddings\" or something, \nthey do that in the wrong way?  It's tough. They know, but they don't fully know.\nThey don't know how to fully integrate it into   the repo and your style and your code and \nyour place, and some of the custom things   that you're doing and how it fits with \nall the assumptions of the repository.  They do have some knowledge, but they \nhaven't gotten to the place where they   can integrate it and make sense of it.\nA lot of the stuff continues to improve.  Currently, the state-of-the-art \nmodel that I go to is the GPT-5 Pro,   and that's a very powerful model.\nIf I have 20 minutes,   I will copy-paste my entire repo and I go to \nGPT-5 Pro, the oracle, for some questions.  Often it's not too bad and surprisingly \ngood compared to what existed a year ago.  Overall, the models are not there.\nI feel like the industry is making too   big of a jump and is trying to pretend like this \nis amazing, and it's not. It's slop. They're not   coming to terms with it, and maybe they're \ntrying to fundraise or something like that.  I'm not sure what's going on, but we're \nat this intermediate stage. The models are   amazing. They still need a lot of work.\nFor now, autocomplete is my sweet spot.  But sometimes, for some types of \ncode, I will go to an LLM agent.  Here's another reason this is really interesting.\nThrough the history of programming, there have   been many productivity improvements—compilers, \nlinting, better programming languages—which   have increased programmer productivity \nbut have not led to an explosion.  That sounds very much like the \nautocomplete tab, and this other   category is just automation of the programmer.\nIt's interesting you're seeing more in the   category of the historical analogies \nof better compilers or something.  Maybe this gets to one other thought.\nI have a hard time differentiating where   AI begins and stops because I see \nAI as fundamentally an extension of   computing in a pretty fundamental way.\nI see a continuum of this recursive   self-improvement or speeding up programmers \nall the way from the beginning: code editors,   syntax highlighting, or checking even of \nthe types, like data type checking—all   these tools that we've built for \neach other. Even search engines.   Why aren't search engines part of AI? Ranking \nis AI. At some point, Google, even early on,   was thinking of themselves as an AI company doing \nGoogle Search engine, which is totally fair.  I see it as a lot more of a continuum than other \npeople do, and it's hard for me to draw the line.  I feel like we're now getting a much \nbetter autocomplete, and now we're also   getting some agents which are these loopy \nthings, but they go off-rails sometimes.  What's going on is that the human is progressively \ndoing a bit less and less of the low-level stuff.  We're not writing the assembly \ncode because we have compilers.  Compilers will take my high-level \nlanguage in C and write the assembly code.  We're abstracting ourselves very, very slowly.\nThere's this what I call \"autonomy slider,\" where   more and more stuff is automated—of the stuff that \ncan be automated at any point in time—and we're   doing a bit less and less and raising ourselves \nin the layer of abstraction over the automation.  Let's talk about RL a bit.\nYou tweeted some very   interesting things about this.\nConceptually, how should we think about   the way that humans are able to build a rich world \nmodel just from interacting with our environment,   and in ways that seem almost irrespective of \nthe final reward at the end of the episode?  If somebody is starting a business, and at \nthe end of 10 years, she finds out whether   the business succeeded or failed, we say that \nshe's earned a bunch of wisdom and experience.  But it's not because the log probs of every \nsingle thing that happened over the last 10   years are up-weighted or down-weighted.\nSomething much more deliberate and   rich is happening.\nWhat is the ML analogy, and how does that   compare to what we're doing with LLMs right now?\nMaybe the way I would put it is that humans don't   use reinforcement learning, as I said.\nI think they do something different.  Reinforcement learning is a lot worse than I \nthink the average person thinks. Reinforcement   learning is terrible. It just so happens \nthat everything that we had before it is   much worse because previously we were just \nimitating people, so it has all these issues.  In reinforcement learning, say you're solving \na math problem, because it's very simple.  You're given a math problem and \nyou're trying to find the solution.  In reinforcement learning, you will \ntry lots of things in parallel first.  You're given a problem, you try hundreds of \ndifferent attempts. These attempts can be complex.   They can be like, \"Oh, let me try this, let me try \nthat, this didn't work, that didn't work,\" etc.  Then maybe you get an answer.\nNow you check the back of the book and you see,   \"Okay, the correct answer is this.\"\nYou can see that this one, this one,   and that one got the correct answer, \nbut these other 97 of them didn't.  Literally what reinforcement learning does is it \ngoes to the ones that worked really well and every   single thing you did along the way, every single \ntoken gets upweighted like, \"Do more of this.\"  The problem with that is people will say \nthat your estimator has high variance,   but it's just noisy. It's noisy. It almost assumes \nthat every single little piece of the solution   that you made that arrived at the right answer \nwas the correct thing to do, which is not true.  You may have gone down the wrong alleys \nuntil you arrived at the right solution.  Every single one of those incorrect things you \ndid, as long as you got to the correct solution,   will be upweighted as, \"Do more of this.\" \nIt's terrible. It's noise. You've done all   this work only to find, at the end, you get a \nsingle number of like, \"Oh, you did correct.\"  Based on that, you weigh that entire \ntrajectory as like, upweight or downweight.  The way I like to put it is you're \nsucking supervision through a straw.  You've done all this work that \ncould be a minute of rollout,   and you're sucking the bits of supervision of the \nfinal reward signal through a straw and you're   broadcasting that across the entire trajectory \nand using that to upweight or downweight that   trajectory. It's just stupid and \ncrazy. A human would never do this.  Number one, a human would \nnever do hundreds of rollouts.  Number two, when a person finds a solution, \nthey will have a pretty complicated process   of review of, \"Okay, I think these parts I \ndid well, these parts I did not do that well.  I should probably do this or that.\" \nThey think through things. There's   nothing in current LLMs that does this. \nThere's no equivalent of it. But I do see   papers popping out that are trying to do this \nbecause it's obvious to everyone in the field.  The first imitation learning, by the way, was \nextremely surprising and miraculous and amazing,   that we can fine-tune by imitation on humans. \nThat was incredible. Because in the beginning,   all we had was base models. Base models \nare autocomplete. It wasn't obvious to   me at the time, and I had to learn this.\nThe paper that blew my mind was InstructGPT,   because it pointed out that you can take \nthe pretrained model, which is autocomplete,   and if you just fine-tune it on text that looks \nlike conversations, the model will very rapidly   adapt to become very conversational, and it \nkeeps all the knowledge from pre-training.  This blew my mind because I didn't understand \nthat stylistically, it can adjust so quickly   and become an assistant to a user through just \na few loops of fine-tuning on that kind of data.  It was very miraculous to me that that worked. \nSo incredible. That was two to three years of   work. Now came RL. And RL allows you to do a bit \nbetter than just imitation learning because you   can have these reward functions and you \ncan hill-climb on the reward functions.  Some problems have just correct answers, you \ncan hill-climb on that without getting expert   trajectories to imitate. So that's amazing. The \nmodel can also discover solutions that a human   might never come up with. This is incredible. \nYet, it's still stupid. We need more. I saw a   paper from Google yesterday that tried to \nhave this reflect & review idea in mind.  Was it the memory bank paper or something? I don't \nknow. I've seen a few papers along these lines.  So I expect there to be some major update to how \nwe do algorithms for LLMs coming in that realm.  I think we need three or four or \nfive more, something like that.  You're so good at coming up with evocative \nphrases. \"Sucking supervision through a   straw.\" It's so good. You're saying the problem \nwith outcome-based reward is that you have this   huge trajectory, and then at the end, you're \ntrying to learn every single possible thing   about what you should do and what you should \nlearn about the world from that one final bit.  Given the fact that this is obvious, why hasn't \nprocess-based supervision as an alternative been   a successful way to make models more capable?\nWhat has been preventing us from using   this alternative paradigm?\nProcess-based supervision just   refers to the fact that we're not going to \nhave a reward function only at the very end.  After you've done 10 minutes of work, I'm not \ngoing to tell you you did well or not well.  I'm going to tell you at every single \nstep of the way how well you're doing.  The reason we don't have that is \nit's tricky how you do that properly.  You have partial solutions and you \ndon't know how to assign credit.  So when you get the right answer, it's just \nan equality match to the answer. It’s very   simple to implement. If you're doing process \nsupervision, how do you assign in an automatable   way, a partial credit assignment?\nIt's not obvious how you do it.  Lots of labs are trying to \ndo it with these LLM judges.  You get LLMs to try to do it.\nYou prompt an LLM, \"Hey,   look at a partial solution of a student.\nHow well do you think they're doing if the   answer is this?\" and they try to tune the prompt.\nThe reason that this is tricky is quite subtle.  It's the fact that anytime you use an LLM to \nassign a reward, those LLMs are giant things   with billions of parameters, and they're gameable.\nIf you're reinforcement learning with respect to   them, you will find adversarial examples \nfor your LLM judges, almost guaranteed.  So you can't do this for too long.\nYou do maybe 10 steps or 20 steps, and maybe   it will work, but you can't do 100 or 1,000.\nI understand it's not obvious, but basically   the model will find little cracks.\nIt will find all these spurious   things in the nooks and crannies of the \ngiant model and find a way to cheat it.  One example that's prominently in my mind, this \nwas probably public, if you're using an LLM judge   for a reward, you just give it a solution from a \nstudent and ask it if the student did well or not.  We were training with \nreinforcement learning against   that reward function, and it worked really well.\nThen, suddenly, the reward became extremely large.  It was a massive jump, and it did perfect.\nYou're looking at it like, \"Wow, this means   the student is perfect in all these problems. \nIt's fully solved math.\" But when you look at   the completions that you're getting from \nthe model, they are complete nonsense.  They start out okay, and then \nthey change to \"dhdhdhdh.\"  It's just like, \"Oh, okay, let's take two plus \nthree and we do this and this, and then dhdhdhdh.\"  You're looking at it, and \nit's like, this is crazy.  How is it getting a reward of one or 100%?\nYou look at the LLM judge, and it turns out   that \"dhdhdhdh\" is an adversarial example for \nthe model, and it assigns 100% probability to it.  It's just because this is an \nout-of-sample example to the LLM.  It's never seen it during training, \nand you're in pure generalization land.  It's never seen it during training, and in \nthe pure generalization land, you can find   these examples that break it.\nYou're basically training   the LLM to be a prompt injection model.\nNot even that. Prompt injection is way too fancy.  You're finding adversarial \nexamples, as they're called.  These are nonsensical solutions that are obviously \nwrong, but the model thinks they are amazing.  To the extent you think this is the \nbottleneck to making RL more functional,   then that will require making LLMs better judges, \nif you want to do this in an automated way.  Is it just going to be some sort \nof GAN-like approach where you   have to train models to be more robust?\nThe labs are probably doing all that.  The obvious thing is, \"dhdhdhdh\" \nshould not get 100% reward.  Okay, well, take \"dhdhdhdh,\" put it \nin the training set of the LLM judge,   and say this is not 100%, this is 0%.\nYou can do this, but every time you do   this, you get a new LLM, and it \nstill has adversarial examples.  There's an infinity of adversarial examples.\nProbably if you iterate this a few times, it'll   probably be harder and harder to find adversarial \nexamples, but I'm not 100% sure because this thing   has a trillion parameters or whatnot.\nI bet you the labs are trying.  I still think we need other ideas.\nInteresting. Do you have some shape   of what the other idea could be?\nThis idea of a review solution   encompassing synthetic examples such that \nwhen you train on them, you get better,   and meta-learn it in some way.\nI think there are some papers   that I'm starting to see pop out.\nI am only at a stage of reading abstracts   because a lot of these papers are just ideas.\nSomeone has to make it work on a frontier   LLM lab scale in full generality \nbecause when you see these papers,   they pop up, and it's just a bit noisy.\nThey're cool ideas, but I haven't seen   anyone convincingly show that this is possible.\nThat said, the LLM labs are fairly closed,   so who knows what they're doing now.\nI can conceptualize how you would be able   to train on synthetic examples or synthetic \nproblems that you have made for yourself.  But there seems to be another thing humans \ndo—maybe sleep is this, maybe daydreaming is   this—which is not necessarily to come up \nwith fake problems, but just to reflect.  I'm not sure what the ML analogy is for \ndaydreaming or sleeping, or just reflecting.  I haven't come up with a new problem.\nObviously, the very basic analogy would just   be fine-tuning on reflection bits, but I feel like \nin practice that probably wouldn't work that well.  Do you have some take on what \nthe analogy of this thing is?  I do think that we're missing some aspects there.\nAs an example, let’s take reading a book.  Currently when LLMs are reading a book, what that \nmeans is we stretch out the sequence of text,   and the model is predicting the next token, \nand it's getting some knowledge from that.  That's not really what humans do.\nWhen you're reading a book,   I don't even feel like the book is exposition \nI'm supposed to be attending to and training on.  The book is a set of prompts for \nme to do synthetic data generation,   or for you to get to a book club \nand talk about it with your friends.  It's by manipulating that information \nthat you actually gain that knowledge.  We have no equivalent of that with LLMs. They \ndon't really do that. I'd love to see during   pre-training some stage that thinks through \nthe material and tries to reconcile it with   what it already knows, and thinks through it \nfor some amount of time and gets that to work.  There's no equivalence of any of this. \nThis is all research. There are some   subtle—very subtle that I think are very hard \nto understand—reasons why it's not trivial.  If I can just describe one: why can't we \njust synthetically generate and train on it?  Because every synthetic example, if \nI just give synthetic generation of   the model thinking about a book, you look \nat it and you're like, \"This looks great.  Why can't I train on it?\"\nYou could try, but the model   will get much worse if you continue trying.\nThat's because all of the samples you get   from models are silently collapsed.\nSilently—it is not obvious if you look   at any individual example of it—they occupy \na very tiny manifold of the possible space of   thoughts about content.\nThe LLMs, when they come off,   they're what we call \"collapsed.\"\nThey have a collapsed data distribution.  One easy way to see it is to go to \nChatGPT and ask it, \"Tell me a joke.\"  It only has like three jokes.\nIt's not giving you the whole breadth   of possible jokes. It knows like three jokes. \nThey're silently collapsed. You're not getting   the richness and the diversity and the entropy \nfrom these models as you would get from humans.  Humans are a lot noisier, but \nat least they're not biased,   in a statistical sense. They're not silently \ncollapsed. They maintain a huge amount of entropy.  So how do you get synthetic data generation to \nwork despite the collapse and while maintaining   the entropy? That’s a research problem.\nJust to make sure I understood, the reason   that the collapse is relevant to synthetic data \ngeneration is because you want to be able to   come up with synthetic problems or reflections \nwhich are not already in your data distribution?  I guess what I'm saying is, say we have a chapter \nof a book and I ask an LLM to think about it,   it will give you something \nthat looks very reasonable.  But if I ask it 10 times, you'll \nnotice that all of them are the same.  You can't just keep scaling \"reflection\" \non the same amount of prompt information   and then get returns from that.\nAny individual sample will look okay,   but the distribution of it is quite terrible.\nIt's quite terrible in such a way that if   you continue training on too much of \nyour own stuff, you actually collapse.  I think that there's possibly \nno fundamental solution to this.  I also think humans collapse over time. \nThese analogies are surprisingly good.   Humans collapse during the course of their lives.\nThis is why children, they haven't overfit yet.  They will say stuff that will shock you \nbecause you can see where they're coming from,   but it's just not the thing people say, \nbecause they're not yet collapsed. But we're   collapsed. We end up revisiting the same thoughts.\nWe end up saying more and more of the same stuff,   and the learning rates go down, and \nthe collapse continues to get worse,   and then everything deteriorates.\nHave you seen this super interesting   paper that dreaming is a way of preventing \nthis kind of overfitting and collapse?  The reason dreaming is evolutionary adaptive \nis to put you in weird situations that are   very unlike your day-to-day reality, so \nas to prevent this kind of overfitting.  It's an interesting idea. I do think \nthat when you're generating things   in your head and then you're attending to \nit, you're training on your own samples,   you're training on your synthetic data.\nIf you do it for too long,   you go off-rails and you collapse way too much.\nYou always have to seek entropy in your life.  Talking to other people is a great \nsource of entropy, and things like that.  So maybe the brain has also built some internal \nmechanisms for increasing the amount of entropy   in that process. That's an interesting idea.\nThis is a very ill-formed thought so I’ll   just put it out and let you react to it.\nThe best learners that we are aware of,   which are children, are extremely \nbad at recollecting information.  In fact, at the very earliest stages of \nchildhood, you will forget everything.  You're just an amnesiac about everything \nthat happens before a certain year date.  But you're extremely good at picking up \nnew languages and learning from the world.  Maybe there's some element of being \nable to see the forest for the trees.  Whereas if you compare it to the opposite end \nof the spectrum, you have LLM pre-training,   where these models will literally be \nable to regurgitate word-for-word what   is the next thing in a Wikipedia page.\nBut their ability to learn abstract   concepts really quickly, the way \na child can, is much more limited.  Then adults are somewhere in between, where \nthey don't have the flexibility of childhood   learning, but they can memorize facts and \ninformation in a way that is harder for kids.  I don't know if there's something \ninteresting about that spectrum.  I think there's something very \ninteresting about that, 100%.  I do think that humans have a lot more of \nan element, compared to LLMs, of seeing   the forest for the trees.\nWe're not actually that good   at memorization, which is actually a feature.\nBecause we're not that good at memorization, we're   forced to find patterns in a more general sense.\nLLMs in comparison are extremely good   at memorization.\nThey will recite   passages from all these training sources.\nYou can give them completely nonsensical data.  You can hash some amount of text or something \nlike that, you get a completely random sequence.  If you train on it, even just for a single \niteration or two, it can suddenly regurgitate   the entire thing. It will memorize it. \nThere's no way a person can read a single   sequence of random numbers and recite it to you.\nThat's a feature, not a bug, because it forces   you to only learn the generalizable components.\nWhereas LLMs are distracted by all the memory   that they have of the pre-training \ndocuments, and it's probably very   distracting to them in a certain sense.\nSo that's why when I talk about the   cognitive core, I want to remove the \nmemory, which is what we talked about.  I'd love to have them have less memory \nso that they have to look things up,   and they only maintain the algorithms for \nthought, and the idea of an experiment,   and all this cognitive glue of acting.\nAnd this is also relevant to preventing   model collapse?\nLet me think. I'm   not sure. It's almost like a separate axis.\nThe models are way too good at memorization,   and somehow we should remove that.\nPeople are much worse, but it's a good thing.  What is a solution to model collapse?\nThere are very naive things you could attempt.  The distribution over logits \nshould be wider or something.  There are many naive things you could try.\nWhat ends up being the problem   with the naive approaches?\nThat's a great question. You can imagine having   a regularization for entropy and things like that.\nI guess they just don't work as well empirically   because right now the models are collapsed.\nBut I will say most of the tasks that we   want from them don't actually demand diversity.\nThat’s probably the answer to what's going on.  The frontier labs are trying \nto make the models useful.  I feel like the diversity of \nthe outputs is not so much...  Number one, it's much harder to work with and \nevaluate and all this stuff, but maybe it's not   what's capturing most of the value.\nIn fact, it's actively penalized.   If you're super creative in RL, it's not good.\nYeah. Or maybe if you're doing a lot of writing,   help from LLMs and stuff like that, it's probably \nbad because the models will silently give   you all the same stuff.\nThey won't explore lots   of different ways of answering a question.\nMaybe this diversity, not as many applications   need it so the models don't have it.\nBut then it's a problem at   synthetic data generation time, et cetera.\nSo we're shooting ourselves in the foot by not   allowing this entropy to maintain in the model.\nPossibly the labs should try harder.  I think you hinted that it's a very \nfundamental problem, it won't be easy   to solve. What's your intuition for that?\nI don't know if it's super fundamental.  I don't know if I intended to say that.\nI do think that I haven't done these experiments,   but I do think that you could probably \nregularize the entropy to be higher.  So you're encouraging the model to give you more \nand more solutions, but you don't want it to   start deviating too much from the training data.\nIt's going to start making up its own language.  It's going to start using words that are \nextremely rare, so it's going to drift too   much from the distribution.\nSo I think controlling   the distribution is just tricky.\nIt's probably not trivial in that sense.  How many bits should the optimal core \nof intelligence end up being if you   just had to make a guess?\nThe thing we put on the   von Neumann probes, how big does it have to be?\nIt's really interesting in the history of the   field because at one point everything was \nvery scaling-pilled in terms of like, \"Oh,   we're gonna make much bigger models, \ntrillions of parameter models.\"  What the models have done in size \nis they've gone up and now they've   come down. State-of-the-art models are smaller. \nEven then, I think they memorized way too much.  So I had a prediction a while back that I almost \nfeel like we can get cognitive cores that are   very good at even a billion parameters.\nIf you talk to a billion parameter model,   I think in 20 years, you can have \na very productive conversation.  It thinks and it's a lot more like a human.\nBut if you ask it some factual question, it might   have to look it up, but it knows that it doesn't \nknow and it might have to look it up and it will   just do all the reasonable things.\nThat's surprising that you think   it'll take a billion parameters.\nBecause already we have billion   parameter models or a couple billion \nparameter models that are very intelligent.  Well, state-of-the-art models \nare like a trillion parameters.  But they remember so much stuff.\nYeah, but I'm surprised that in 10 years,   given the pace… We have gpt-oss-20b.\nThat's way better than GPT-4 original,   which was a trillion plus parameters.\nGiven that trend, I'm surprised you   think in 10 years the cognitive \ncore is still a billion parameters.  I'm surprised you're not like, \"Oh it's \ngonna be like tens of millions or millions.\"  Here's the issue, the training data is \nthe internet, which is really terrible.  There's a huge amount of gains to be \nmade because the internet is terrible.  Even the internet, when you and I think of \nthe internet, you're thinking of like The   Wall Street Journal. That's not what this \nis. When you're looking at a pre-training   dataset in the frontier lab and you look at a \nrandom internet document, it's total garbage.  I don't even know how this works at all.\nIt's some like stock tickers, symbols,   it's a huge amount of slop and garbage \nfrom like all the corners of the internet.  It's not like your Wall Street Journal \narticle, that's extremely rare.  So because the internet is so terrible, we have \nto build really big models to compress all that.  Most of that compression is memory \nwork instead of cognitive work.  But what we really want is the \ncognitive part, delete the memory.  I guess what I'm saying is that we need \nintelligent models to help us refine even   the pre-training set to just narrow \nit down to the cognitive components.  Then I think you get away with a \nmuch smaller model because it's a   much better dataset and you could train it on it.\nBut probably it's not trained directly on it, it's   probably distilled from a much better model still.\nBut why is the distilled version still a billion?  I just feel like distillation \nworks extremely well.  So almost every small model, if you have a \nsmall model, it's almost certainly distilled.  Right, but why is the distillation in \n10 years not getting below 1 billion?  Oh, you think it should be smaller than a \nbillion? I mean, come on, right? I don't   know. At some point it should take at least \na billion knobs to do something interesting.  You're thinking it should be even smaller?\nYeah. If you look at the trend over the last   few years of just finding low-hanging fruit and \ngoing from trillion plus models to models that   are literally two orders of magnitude smaller in a \nmatter of two years and having better performance,   it makes me think the sort of core of \nintelligence might be even way, way smaller.  Plenty of room at the bottom, \nto paraphrase Feynman.  I feel like I'm already contrarian \nby talking about a billion parameter   cognitive core and you're outdoing me.\nMaybe we could get a little bit smaller.  I do think that practically speaking, you \nwant the model to have some knowledge.  You don't want it to be looking up everything \nbecause then you can't think in your head.  You're looking up way too much stuff all the time.  Some basic curriculum needs to be there for \nknowledge, but it doesn't have esoteric knowledge.  We're discussing what plausibly \ncould be the cognitive core.  There's a separate question which is what \nwill be the size of frontier models over time?  I'm curious if you have predictions.\nWe had increasing scale up to maybe GPT 4.5 and   now we're seeing decreasing or plateauing scale.\nThere are many reasons this could be going on.  Do you have a prediction going forward?\nWill the biggest models be bigger,   will they be smaller, will they be the same?\nI don't have a super strong prediction.  The labs are just being practical.\nThey have a flops budget and a cost budget.  It just turns out that pre-training is not where \nyou want to put most of your flops or your cost.  That's why the models have gotten smaller.\nThey are a bit smaller, the pre-training   stage is smaller, but they make \nit up in reinforcement learning,   mid-training, and all this stuff that follows.\nThey're just being practical in terms of all the   stages and how you get the most bang for the buck.\nForecasting that trend is quite hard.  I do still expect that there's so much \nlow-hanging fruit. That's my basic expectation.   I have a very wide distribution here.\nDo you expect the low-hanging fruit to be   similar in kind to the kinds of things that have \nbeen happening over the last two to five years?  If I look at nanochat versus nanoGPT \nand the architectural tweaks you made,   is that the flavor of things you \nexpect to continue to keep happening?  You're not expecting any giant paradigm shifts.\nFor the most part, yeah. I expect the   datasets to get much, much better.\nWhen you look at the average datasets,   they're extremely terrible.\nThey’re so bad that I   don't even know how anything works.\nLook at the average example in the training set:   factual mistakes, errors, nonsensical things.\nSomehow when you do it at scale,   the noise washes away and you're left with \nsome of the signal. Datasets will improve   a ton. Everything gets better. Our hardware, \nall the kernels for running the hardware and   maximizing what you get with the hardware.\nNvidia is slowly tuning the hardware itself,   Tensor Cores, all that needs to \nhappen and will continue to happen.  All the kernels will get better and \nutilize the chip to the max extent.  All the algorithms will probably improve over \noptimization, architecture, and all the modeling   components of how everything is done and what \nthe algorithms are that we're even training with.  I do expect that nothing dominates. Everything \nplus 20%. This is roughly what I've seen.  People have proposed different ways of charting \nhow much progress we've made towards full AGI.  If you can come up with some line, then you \ncan see where that line intersects with AGI   and where that would happen on the x-axis.\nPeople have proposed it's the education level.  We had a high schooler, and then they went to \ncollege with RL, and they're going to get a Ph.D.  I don't like that one.\nOr they'll propose horizon   length. Maybe they can do tasks that take \na minute, they can do those autonomously.  Then they can autonomously do tasks that take \nan hour, a human an hour, a human a week.  How do you think about the relevant y-axis here?\nHow should we think about how   AI is making progress?\nI have two answers to that.  Number one, I'm almost tempted to \nreject the question entirely because   I see this as an extension of computing.\nHave we talked about how to chart progress   in computing, or how do you chart progress \nin computing since the 1970s or whatever?   What is the y-axis? The whole question is \nfunny from that perspective a little bit.  When people talk about AI and the original AGI \nand how we spoke about it when OpenAI started,   AGI was a system you could go to that can do any \neconomically valuable task at human performance   or better. That was the definition. I \nwas pretty happy with that at the time.  I've stuck to that definition forever, and \nthen people have made up all kinds of other   definitions. But I like that definition. The first \nconcession that people make all the time is they   just take out all the physical stuff because \nwe're just talking about digital knowledge work.  That's a pretty major concession compared to \nthe original definition, which was any task   a human can do. I can lift things, etc. AI \ncan't do that, obviously, but we'll take it.  What fraction of the economy are we taking away \nby saying, \"Oh, only knowledge work?\" I don't know   the numbers. I feel about 10% to 20%, if I had to \nguess, is only knowledge work, someone could work   from home and perform tasks, something like that.\nIt's still a really large market.  What is the size of the \neconomy, and what is 10% or 20%?  We're still talking about a few trillion \ndollars, even in the US, of market share or work.  So it's still a very massive bucket.\nGoing back to the definition,   what I would be looking for is to \nwhat extent is that definition true?  Are there jobs or lots of tasks?\nIf we think of tasks as not jobs but tasks.  It's difficult because the problem is society will \nrefactor based on the tasks that make up jobs,   based on what's automatable or not.\nToday, what jobs are replaceable by AI?  A good example recently was Geoff Hinton's \nprediction that radiologists would not be   a job anymore, and this turned out \nto be very wrong in a bunch of ways.  Radiologists are alive and well and growing, \neven though computer vision is really,   really good at recognizing all the different \nthings that they have to recognize in images.  It's just a messy, complicated job with a \nlot of surfaces and dealing with patients   and all this stuff in the context of it.\nI don't know that by that definition   AI has made a huge dent yet.\nSome of the jobs that I would   be looking for have some features that make it \nvery amenable to automation earlier than later.  As an example, call center employees \noften come up, and I think rightly so.  Call center employees have a number of simplifying \nproperties with respect to what's automatable   today. Their jobs are pretty simple. It's a \nsequence of tasks, and every task looks similar.  You take a phone call with a person, it's \n10 minutes of interaction or whatever it is,   probably a bit longer.\nIn my experience, a lot longer.  You complete some task in some scheme, \nand you change some database entries   around or something like that.\nSo you keep repeating something   over and over again, and that's your job.\nYou do want to bring in the task horizon—how   long it takes to perform a task—and \nthen you want to also remove context.  You're not dealing with different parts of \nservices of companies or other customers.  It's just the database, you, \nand a person you're serving.  It's more closed, it's more \nunderstandable, it's purely digital.  So I would be looking for those things.\nBut even there, I'm not looking   at full automation yet.\nI'm looking for an autonomy slider.  I expect that we are not going \nto instantly replace people.  We're going to be swapping in \nAIs that do 80% of the volume.  They delegate 20% of the volume to humans, \nand humans are supervising teams of five AIs   doing the call center work that's more rote.\nI would be looking for new interfaces or new   companies that provide some \nlayer that allows you to manage   some of these AIs that are not yet perfect.\nThen I would expect that across the economy.  A lot of jobs are a lot harder \nthan a call center employee.  With radiologists, I'm totally \nspeculating and I have no idea what   the actual workflow of a radiologist involves.\nBut one analogy that might be applicable is when   Waymos were first being rolled out, there'd be a \nperson sitting in the front seat, and you just had   to have them there to make sure that if something \nwent really wrong, they're there to monitor.  Even today, people are still watching \nto make sure things are going well.  Robotaxi, which was just deployed, \nstill has a person inside it.  Now we could be in a similar situation where \nif you automate 99% of a job, that last 1%   the human has to do is incredibly valuable \nbecause it's bottlenecking everything else.  If it were the case with radiologists, where \nthe person sitting in the front of Waymo has   to be specially trained for years in order \nto provide the last 1%, their wages should   go up tremendously because they're the \none thing bottlenecking wide deployment.  Radiologists, I think their wages have \ngone up for similar reasons, if you're   the last bottleneck and you're not fungible.\nA Waymo driver might be fungible with others.  So you might see this thing where your wages \ngo up until you get to 99% and then fall just   like that when the last 1% is gone.\nAnd I wonder if we're seeing similar   things with radiology or salaries of call \ncenter workers or anything like that.  That's an interesting question. I don't think \nwe're currently seeing that with radiology.  I think radiology is not a good example.\nI don't know why Geoff Hinton picked   on radiology because I think it's an \nextremely messy, complicated profession.  I would be a lot more interested in what's \nhappening with call center employees today,   for example, because I would expect a lot \nof the rote stuff to be automatable today.  I don't have first-level access to it but \nI would be looking for trends of what's   happening with the call center employees.\nSome of the things I would also expect   is that maybe they are swapping in AI, but \nthen I would still wait for a year or two   because I would potentially expect them to \npull back and rehire some of the people.  There's been evidence that that's already been \nhappening generally in companies that have been   adopting AI, which I think is quite surprising.\nI also found what was really surprising. AGI,   right? A thing which would do everything.\nWe'll take out physical work,   but it should be able to do all knowledge work.\nWhat you would have naively anticipated is that   the way this progression would happen is \nthat you take a little task that a consultant   is doing, you take that out of the bucket.\nYou take a little task that an accountant is   doing, you take that out of the bucket.\nThen you're just doing this   across all knowledge work.\nBut instead, if we do believe we're   on the path of AGI with the current paradigm, \nthe progression is very much not like that.  It does not seem like consultants and accountants \nare getting huge productivity improvements.  It's very much like programmers are getting \nmore and more chiseled away at their work.  If you look at the revenues of these companies, \ndiscounting normal chat revenue—which is similar   to Google or something—just looking at \nAPI revenues, it's dominated by coding.  So this thing which is \"general\", which \nshould be able to do any knowledge work,   is just overwhelmingly doing only coding.\nIt's a surprising way that you would   expect the AGI to be deployed.\nThere's an interesting point   here. I do believe coding is the perfect \nfirst thing for these LLMs and agents.  That’s because coding has always \nfundamentally worked around text.  It's computer terminals and text, \nand everything is based around text.  LLMs, the way they're trained \non the Internet, love text.  They're perfect text processors, and there's \nall this data out there. It's a perfect fit.   We also have a lot of infrastructure \npre-built for handling code and text.  For example, we have Visual Studio Code \nor your favorite IDE showing you code,   and an agent can plug into that.\nIf an agent has a diff where it made some change,   we suddenly have all this code already that shows \nall the differences to a code base using a diff.  It's almost like we've pre-built a \nlot of the infrastructure for code.  Contrast that with some of the \nthings that don't enjoy that at all.  As an example, there are people trying to build \nautomation not for coding, but for slides.  I saw a company doing slides. That's \nmuch, much harder. The reason it's   much harder is because slides are not text.\nSlides are little graphics, they're arranged   spatially, and there's a visual component to it.\nSlides don't have this pre-built infrastructure.  For example, if an agent is to make a change to \nyour slides, how does a thing show you the diff?  How do you see the diff?\nThere's nothing that shows diffs   for slides. Someone has to build it. Some of these \nthings are not amenable to AIs as they are, which   are text processors, and code surprisingly is.\nI’m not sure that alone explains it.  I personally have tried to get LLMs to be useful \nin domains which are just pure language-in,   language-out, like rewriting transcripts, \ncoming up with clips based on transcripts.  It's very plausible that I didn't do \nevery single possible thing I could do.  I put a bunch of good examples in context, but \nmaybe I should have done some kind of fine-tuning.  Our mutual friend, Andy Matuschak, told me that \nhe tried 50 billion things to try to get models   to be good at writing spaced repetition prompts.\nAgain, very much language-in, language-out tasks,   the kind of thing that should be dead \ncenter in the repertoire of these LLMs.  He tried in-context learning \nwith a few-shot examples.  He tried supervised fine-tuning and retrieval.\nHe could not get them to make   cards to his satisfaction.\nSo I find it striking that even in language-out   domains, it's very hard to get a lot of economic \nvalue out of these models separate from coding.  I don't know what explains it.\nThat makes sense. I'm not   saying that anything text is trivial.\nI do think that code is pretty structured.  Text is maybe a lot more flowery, and there's \na lot more entropy in text, I would say.  I don't know how else to put it.\nAlso code is hard, and so people feel quite   empowered by LLMs, even from simple knowledge.\nI don't know that I have a very good answer.  Obviously, text makes it much, much easier, \nbut it doesn't mean that all text is trivial.  How do you think about superintelligence?\nDo you expect it to feel qualitatively different   from normal humans or human companies?\nI see it as a progression   of automation in society.\nExtrapolating the trend of computing, there will   be a gradual automation of a lot of things, and \nsuperintelligence will an extrapolation of that.  We expect more and more autonomous \nentities over time that are doing a lot   of the digital work and then eventually even \nthe physical work some amount of time later.  Basically I see it as just \nautomation, roughly speaking.  But automation includes the things humans \ncan already do, and superintelligence   implies things humans can’t do.\nBut one of the things that people   do is invent new things, which I would just \nput into the automation if that makes sense.  But I guess, less abstractly and more \nqualitatively, do you expect something   to feel like… Because this thing can either think \nso fast, or has so many copies, or the copies can   merge back into themselves, or is much smarter, \nany number of advantages an AI might have, will   the civilization in which these AIs exist\njust feel qualitatively different from humans?  I think it will. It is fundamentally automation, \nbut it will be extremely foreign. It will look   really strange. Like you mentioned, we can run \nall of this on a computer cluster and much faster.  Some of the scenarios that I start to get \nnervous about when the world looks like   that is this gradual loss of control \nand understanding of what's happening.  I think that's the most likely outcome, that \nthere will be a gradual loss of understanding.  We'll gradually layer all this stuff \neverywhere, and there will be fewer   and fewer people who understand it.\nThen there will be a gradual loss of   control and understanding of what's happening.\nThat to me seems the most likely outcome of how   all this stuff will go down.\nLet me probe on that a bit.  It's not clear to me that loss of control and \nloss of understanding are the same things.  A board of directors at TSMC, Intel—name a random \ncompany—they're just prestigious 80-year-olds.  They have very little understanding, and maybe \nthey don't practically actually have control.  A better example   is the President of the United States.\nThe President has a lot of fucking power.  I'm not trying to make a good statement \nabout the current operant, or maybe I am,   but the actual level of understanding is \nvery different from the level of control.  I think that's fair. That's a good \npushback. I think I expect loss of both.  How come? Loss of understanding is \nobvious, but why loss of control?  We're really far into a territory where I \ndon't know what this looks like, but if I   were to write sci-fi novels, they would look along \nthe lines of not even a single entity that takes   over everything, but multiple competing entities \nthat gradually become more and more autonomous.  Some of them go rogue and \nthe others fight them off.  It's this hot pot of completely autonomous \nactivity that we've delegated to.  I feel it would have that flavor.\nIt is not the fact that they are smarter   than us that is resulting in the loss of control.\nIt's the fact that they are competing with each   other, and whatever arises out of that \ncompetition leads to the loss of control.  A lot of these things, they will be \ntools to people, they're acting on   behalf of people or something like that.\nSo maybe those people are in control,   but maybe it's a loss of control overall for \nsociety in the sense of outcomes we want.  You have entities acting on behalf of individuals \nthat are still roughly seen as out of control.  This is a question I should have asked earlier.\nWe were talking about how currently it feels like   when you're doing AI engineering or AI research, \nthese models are more in the category of compiler   rather than in the category of a replacement.\nAt some point, if you have AGI,   it should be able to do what you do.\nDo you feel like having a million   copies of you in parallel results in \nsome huge speed-up of AI progress?  If that does happen, do you expect to see an \nintelligence explosion once we have a true AGI?  I'm not talking about LLMs today.\nI do, but it's business as usual because   we're in an intelligence explosion \nalready and have been for decades.  It's basically the GDP curve that is \nan exponential weighted sum over so   many aspects of the industry.\nEverything is gradually being   automated and has been for hundreds of years.\nThe Industrial Revolution is automation and   some of the physical components and \ntool building and all this stuff.  Compilers are early software \nautomation, et cetera.  We've been recursively self-improving \nand exploding for a long time.  Another way to see it is that Earth was a pretty \nboring place if you don't look at the biomechanics   and so on, and looked very similar.\nIf you look from space, we're in the   middle of this firecracker event, \nbut we're seeing it in slow motion.  I definitely feel like this has \nalready happened for a very long time.  Again, I don't see AI as a distinct \ntechnology with respect to what has   already been happening for a long time.\nYou think it's continuous with this   hyper-exponential trend?\nYes. That's why this was   very interesting to me, because I was \ntrying to find AI in the GDP for a while.  I thought that GDP should go up.\nBut then I looked at some of the   other technologies that I thought \nwere very transformative, like   computers or mobile phones or et cetera.\nYou can't find them in GDP. GDP is the same   exponential. Even the early iPhone didn't have the \nApp Store, and it didn't have a lot of the bells   and whistles that the modern iPhone has.\nSo even though we think of 2008,   when the iPhone came out, as this major \nseismic change, it's actually not.  Everything is so spread out and it so \nslowly diffuses that everything ends up   being averaged up into the same exponential.\nIt's the exact same thing with computers.  You can't find them in the GDP \nlike, \"Oh, we have computers now.\"  That's not what happened, because \nit's such slow progression.  With AI we're going to see the exact same thing. \nIt's just more automation. It allows us to write   different kinds of programs that we couldn't write \nbefore, but AI is still fundamentally a program.  It's a new kind of computer and \na new kind of computing system.  But it has all these problems, \nit's going to diffuse over time,   and it's still going to add \nup to the same exponential.  We're still going to have an exponential \nthat's going to get extremely vertical.  It's going to be very foreign to \nlive in that kind of an environment.  Are you saying that, if you look at the trend \nbefore the Industrial Revolution to now,   you have a hyper-exponential where you go \nfrom 0% growth to then 10,000 years ago,   0.02% growth, and to now when we're at 2% \ngrowth. That's a hyper-exponential. Are you   saying if you're charting AI on there, then \nAI takes you to 20% growth or 200% growth?  Or are you saying that if you look at \nthe last 300 years, what you've been   seeing is that you have technology after \ntechnology—computers, electrification,   steam engines, railways, et cetera—but the \nrate of growth is the exact same, it's 2%.  Are you saying the rate of growth will go up?\nThe rate of growth has also stayed   roughly constant, right?\nOnly over the last 200, 300 years.  But over the course of \nhuman history it's exploded.  It's gone from 0% to faster, faster, \nfaster. Industrial explosion, 2%.  For a while I tried to find AI \nor look for AI in the GDP curve,   and I've convinced myself that this is false.\nEven when people talk about recursive   self-improvement and labs and stuff \nlike that, this is business as usual.  Of course it's going to recursively self-improve, \nand it's been recursively self-improving.  LLMs allow the engineers to work much more \nefficiently to build the next round of LLM,   and a lot more of the components are \nbeing automated and tuned and et cetera.  All the engineers having access \nto Google Search is part of it.  All the engineers having an IDE, all of them \nhaving autocomplete or having Claude code,   et cetera, it's all just part of the same \nspeed-up of the whole thing. It's just so smooth.  Just to clarify, you're saying that \nthe rate of growth will not change.  The intelligence explosion will show up as \nit just enabled us to continue staying on the   2% growth trajectory, just as the Internet \nhelped us stay on the 2% growth trajectory.  Yes, my expectation is that \nit stays in the same pattern.  Just to throw the opposite argument against you, \nmy expectation is that it blows up because I think   true AGI—and I'm not talking about LLM coding \nbots, I'm talking about actual replacement of a   human in a server—is qualitatively different \nfrom these other productivity-improving   technologies because it's labor itself.\nI think we live in a very labor-constrained world.  If you talk to any startup founder or any person, \nyou can be like, what do you need more of? You   need really talented people. And if you have \nbillions of extra people who are inventing stuff,   integrating themselves, making companies bottom \nstart to finish, that feels qualitatively   different from a single technology.\nIt's as if you get 10 billion   extra people on the planet.\nMaybe a counterpoint. I'm pretty willing   to be convinced one way or another on this point.\nBut I will say, for example, computing is labor.   Computing was labor. Computers, a lot \nof jobs disappeared because computers   are automating a bunch of digital information \nprocessing that you now don't need a human for.  So computers are labor, and that has played out.\nSelf-driving as an example is also computers doing   labor. That's already been playing \nout. It's still business as usual.  You have a machine which is spitting out more \nthings like that at potentially faster pace.  Historically, we have examples \nof the growth regime changing   where you went from 0.2% growth to 2% growth.\nIt seems very plausible to me that a machine which   is then spitting out the next self-driving \ncar and the next Internet and whatever…  I see where it's coming from.\nAt the same time, I do feel like   people make this assumption of, \"We \nhave God in a box, and now it can do   everything,\" and it just won't look like that.\nIt's going to be able to do some of the things.  It's going to fail at some other things.\nIt's going to be gradually put into society,   and we'll end up with the same pattern. That \nis my prediction. This assumption of suddenly   having a completely intelligent, fully flexible, \nfully general human in a box, and we can dispense   it at arbitrary problems in society, I don't \nthink that we will have this discrete change.  I think we'll arrive at the same kind of \ngradual diffusion of this across the industry.  It often ends up being misleading \nin these conversations.  I don't like to use the word intelligence in \nthis context because intelligence implies you   think there'll be a single superintelligence \nsitting in a server and it'll divine how   to come up with new technologies and \ninventions that cause this explosion.  That's not what I'm imagining \nwhen I'm imagining 20% growth.  I'm imagining that there are billions of \nvery smart human-like minds, potentially,   or that's all that's required.\nBut the fact that there's hundreds   of millions of them, billions of them, each \nindividually making new products, figuring   out how to integrate themselves into the economy.\nIf a highly experienced smart immigrant came to   the country, you wouldn't need to figure out how \nwe integrate them in the economy. They figure it   out. They could start a company, they could make \ninventions, or increase productivity in the world.  We have examples, even in the current regime, \nof places that have had 10-20% economic growth.  If you just have a lot of people and \nless capital in comparison to the people,   you can have Hong Kong or Shenzhen or \nwhatever with decades of 10% plus growth.  There's a lot of really smart people who are \nready to make use of the resources and do   this period of catch-up because we've had this \ndiscontinuity, and I think AI might be similar.  I understand, but I still think that \nyou're presupposing some discrete jump.  There's some unlock that we're waiting to claim.\nAnd suddenly we're going to have   geniuses in data centers.\nI still think you're presupposing   some discrete jump that has no historical \nprecedent that I can't find in any of the   statistics and that I think probably won't happen.\nI mean, the Industrial Revolution is such a jump.  You went from 0.2% growth to 2% growth.\nI'm just saying you'll see another jump like that.  I'm a little bit suspicious, \nI would have to take a look.  For example, some of the logs are not very \ngood from before the Industrial Revolution.  I'm a bit suspicious of it but \nI don't have strong opinions.  You're saying that this was a singular \nevent that was extremely magical.  You're saying that maybe there's going \nto be another event that's going to   be just like that, extremely magical.\nIt will break the paradigm, and so on.  I actually don't think… The crucial thing with the \nIndustrial Revolution was that it was not magical.  If you just zoomed in, what you would see in 1770 \nor 1870 is not that there was some key invention.  But at the same time, you did move the \neconomy to a regime where the progress   was much faster and the exponential 10x'd.\nI expect a similar thing from AI where it's   not like there's going to be a single moment \nwhere we've made the crucial invention.  It’s an overhang that's being unlocked.\nLike maybe there's a new energy source.  There's some unlock—in this case, some kind of \na cognitive capacity—and there's an overhang of   cognitive work to do.\nThat's right.  You're expecting that overhang to be filled by \nthis new technology when it crosses the threshold.  Maybe one way to think about it is \nthroughout history, a lot of growth   comes because people come up with ideas, \nand then people are out there doing stuff to   execute those ideas and make valuable output.\nThrough most of this time, the population has   been exploding. That has been driving \ngrowth. For the last 50 years, people   have argued that growth has stagnated.\nThe population in frontier countries   has also stagnated.\nI think we go back to   the exponential growth in population that \ncauses hyper-exponential growth in output.  It's really hard to tell. I \nunderstand that viewpoint. I   don't intuitively feel that viewpoint.\nYou recommended Nick Lane's book to me.  On that basis, I also found it super \ninteresting and I interviewed him.  I have some questions about thinking about \nintelligence and evolutionary history.  Now that you, over the last 20 years of doing AI \nresearch, you maybe have a more tangible sense of   what intelligence is, what it takes to develop it.\nAre you more or less surprised as a result that   evolution just spontaneously stumbled upon it?\nI love Nick Lane's books. I was just listening   to his podcast on the way up here.\nWith respect to intelligence and its   evolution, it's very, very recent.\nI am surprised that it evolved.  I find it fascinating to think \nabout all the worlds out there.  Say there's a thousand planets \nlike Earth and what they look like.  I think Nick Lane was here talking \nabout some of the earliest parts.  He expects very similar life \nforms, roughly speaking,   and bacteria-like things in most of them.\nThere are a few breaks in there.  The evolution of intelligence intuitively feels \nto me like it should be a fairly rare event.  Maybe you should base it on \nhow long something has existed.  If bacteria were around for 2 billion years \nand nothing happened, then going to eukaryote   is probably pretty hard because bacteria came \nup quite early in Earth's evolution or history.  How long have we had animals?\nMaybe a couple hundred million years,   multicellular animals that \nrun around, crawl, et cetera.  That’s maybe 10% of Earth's lifespan.\nMaybe on that timescale it's not too tricky.  It's still surprising to me, \nintuitively, that it developed.  I would maybe expect just a lot of animal-like \nlife forms doing animal-like things.  The fact that you can get something \nthat creates culture and knowledge   and accumulates it is surprising to me.\nThere's a couple of interesting follow-ups.  If you buy the Sutton perspective that the \ncrux of intelligence is animal intelligence…   The quote he said is \"If you got to the \nsquirrel, you'd be most of the way to AGI.\"  We got to squirrel intelligence right after \nthe Cambrian explosion 600 million years ago.  It seems like what instigated that was the \noxygenation event 600 million years ago.  But immediately the intelligence algorithm \nwas there to make the squirrel intelligence.  It's suggestive that animal \nintelligence was like that.  As soon as you had the oxygen in the \nenvironment, you had the eukaryote,   you could just get the algorithm.\nMaybe it was an accident that   evolution stumbled upon it so fast, \nbut I don't know if that suggests that   at the end it's going to be quite simple.\nIt's so hard to tell with any of this stuff.  You can base it a bit on how long \nsomething has existed or how long   it feels like something has been bottlenecked.\nNick Lane is very good about describing this very   apparent bottleneck in bacteria and archaea.\nFor two billion years, nothing happened.  There’s extreme diversity of biochemistry, \nand yet nothing grows to become animals.   Two billion years. I don't know that we've \nseen exactly that kind of an equivalent with   animals and intelligence, to your point.\nWe could also look at it with respect   to how many times we think certain \nintelligence has individually sprung up.  That's a really good thing to investigate.\nOne thought on that. There's hominid intelligence,   and then there's bird intelligence.\nRavens, etc., are extremely clever,   but their brain parts are quite distinct, \nand we don't have that much in common.  That's a slight indication of maybe \nintelligence springing up a few times.  In that case, you'd expect it more frequently.\nA former guest, Gwern, and Carl Shulman, they’ve   made a really interesting point about that.\nTheir perspective is that the scalable algorithm   which humans have and primates have, arose in \nbirds as well, and maybe other times as well.  But humans found an evolutionary niche which \nrewarded marginal increases in intelligence   and also had a scalable brain algorithm that \ncould achieve those increases in intelligence.  For example, if a bird had a bigger brain, \nit would just collapse out of the air.  It's very smart for the size of \nits brain, but it's not in a niche   which rewards the brain getting bigger.\nIt’s maybe similar to some really smart…  Like dolphins?\nExaclty, humans, we have hands that   reward being able to learn how to do tool use.\nWe can externalize digestion, more energy to   the brain, and that kicks off the flywheel.\nAlso stuff to work with. I'm guessing it would   be harder if I were a dolphin. How do you have \nfire? The universe of things you can do in water,   inside water, is probably lower than \nwhat you can do on land, just chemically.  I do agree with this viewpoint of these niches \nand what's being incentivized. I still find it   miraculous. I would have expected things to \nget stuck on animals with bigger muscles.  Going through intelligence is a \nreally fascinating breaking point.  The way Gwern put it is the reason it was so hard \nis that it's a very tight line between being in   a situation where something is so important \nto learn that it's not worth distilling the   exact right circuits directly back into your DNA, \nversus it's not important enough to learn at all.  It has to be something that incentivizes \nbuilding the algorithm to learn in a lifetime.  You have to incentivize some kind of adaptability.\nYou want environments that are unpredictable   so evolution can't bake your \nalgorithms into your weights.  A lot of animals are pre-baked in this sense.\nHumans have to figure it out at test   time when they get born.\nYou want these environments   that change really rapidly, where you \ncan't foresee what will work well.  You create intelligence to \nfigure it out at test time.  Quintin Pope had this interesting blog post \nwhere he's saying the reason he doesn't   expect a sharp takeoff is that humans had the \nsharp takeoff where 60,000 years ago we seem   to have had the cognitive architectures \nthat we have today. 10,000 years ago,   agricultural revolution, modernity.\nWhat was happening in that 50,000 years?  You had to build this cultural scaffold where \nyou can accumulate knowledge over generations.  This is an ability that exists for \nfree in the way we do AI training.  In many cases they are literally distilled.\nIf you retrain a model, they can be trained   on each other, they can be trained \non the same pre-training corpus,   they don't literally have to start from scratch.\nThere's a sense in which it took humans a long   time to get this cultural loop going, but it just \ncomes for free with the way we do LLM training.  Yes and no. Because LLMs don't really \nhave the equivalent of culture.  Maybe we're giving them way too \nmuch and incentivizing not to   create it or something like that.\nBut the invention of culture and of   written record and of passing down notes \nbetween each other, I don't think there's   an equivalent of that with LLMs right now.\nLLMs don't really have culture right now and   it's one of the impediments I would say.\nCan you give me some sense of what   LLM culture might look like?\nIn the simplest case it would be a   giant scratchpad that the LLM can edit and as it's \nreading stuff or as it's helping out with work,   it's editing the scratchpad for itself.\nWhy can't an LLM write a book for the other   LLMs? That would be cool. Why can't other \nLLMs read this LLM's book and be inspired   by it or shocked by it or something like that?\nThere's no equivalence for any of this stuff.  Interesting. When would you expect \nthat kind of thing to start happening?  Also, multi-agent systems and a sort of \nindependent AI civilization and culture?  There are two powerful ideas in the \nrealm of multi-agent that have both   not been really claimed or so on.\nThe first one I would say is culture   and LLMs having a growing repertoire \nof knowledge for their own purposes.  The second one looks a lot more \nlike the powerful idea of self-play.  In my mind it’s extremely powerful.\nEvolution has a lot of competition   driving intelligence and evolution.\nIn AlphaGo more algorithmically,   AlphaGo is playing against itself and that's \nhow it learns to get really good at Go.  There's no equivalent of self-playing LLMs, \nbut I would expect that to also exist.  No one has done it yet.\nWhy can't an LLM for example, create a bunch   of problems that another LLM is learning to solve?\nThen the LLM is always trying to serve more and   more difficult problems, stuff like that.\nThere's a bunch of ways to organize it.  It's a realm of research, but I haven't \nseen anything that convincingly claims   both of those multi-agent improvements.\nWe're mostly in the realm of a single   individual agent, but that will change.\nIn the realm of culture also,   I would also bucket organizations.\nWe haven't seen anything like that convincingly   either. That's why we're still early.\nCan you identify the key bottleneck   that's preventing this kind \nof collaboration between LLMs?  Maybe the way I would put it is, \nsome of these analogies work and   they shouldn't, but somehow, remarkably, they do.\nA lot of the smaller models, or the dumber models,   remarkably resemble a kindergarten student, or an \nelementary school student or high school student.  Somehow, we still haven't graduated \nenough where this stuff can take over.  My Claude Code or Codex, they still \nfeel like this elementary-grade student.  I know that they can take PhD quizzes, \nbut they still cognitively feel like a   kindergarten or an elementary school student.\nI don't think they can create culture because   they're still kids. They're savant kids. \nThey have perfect memory of all this stuff.  They can convincingly create all \nkinds of slop that looks really good.  But I still think they don't really know \nwhat they're doing and they don't really   have the cognition across all these little \ncheckboxes that we still have to collect.  You've talked about how you were at Tesla \nleading self-driving from 2017 to 2022.  And you firsthand saw this progress from cool \ndemos to now thousands of cars out there actually   autonomously doing drives.\nWhy did that take a decade?  What was happening through that time?\nOne thing I will almost instantly push   back on is that this is not even near done, \nin a bunch of ways that I'm going to get to.  Self-driving is very interesting because \nit's definitely where I get a lot of my   intuitions because I spent five years on it.\nIt has this entire history where the first demos   of self-driving go all the way to the 1980s.\nYou can see a demo from CMU in 1986.  There's a truck that's driving itself on \nroads. Fast forward. When I was joining Tesla,   I had a very early demo of Waymo.\nIt basically gave me a perfect drive   in 2014 or something like that, so \na perfect Waymo drive a decade ago.  It took us around Palo Alto and so on \nbecause I had a friend who worked there.  I thought it was very close and \nthen it still took a long time.  For some kinds of tasks and jobs and so on, \nthere's a very large demo-to-product gap where the   demo is very easy, but the product is very hard.\nIt's especially the case in cases like   self-driving where the cost \nof failure is too high.  Many industries, tasks, and jobs maybe don't have \nthat property, but when you do have that property,   that definitely increases the timelines.\nFor example, in software engineering,   I do think that property does exist.\nFor a lot of vibe coding, it doesn't.  But if you're writing actual production-grade \ncode, that property should exist, because any   kind of mistake leads to a security \nvulnerability or something like that.  Millions and hundreds of millions of \npeople's personal Social Security numbers   get leaked or something like that.\nSo in software, people should be careful,   kind of like in self-driving.\nIn self-driving, if things go wrong,   you might get injured. There are worse \noutcomes. But in software, it's almost   unbounded how terrible something could be.\nI do think that they share that property.  What takes the long amount of time and the way \nto think about it is that it's a march of nines.  Every single nine is a constant amount of work.\nEvery single nine is the same amount of work.  When you get a demo and something works 90% \nof the time, that's just the first nine.  Then you need the second nine, a third \nnine, a fourth nine, a fifth nine.  While I was at Tesla for five years or so, we \nwent through maybe three nines or two nines.  I don't know what it is, but \nmultiple nines of iteration.  There are still more nines to go.\nThat's why these things take so long.  It's definitely formative for me, seeing \nsomething that was a demo. I'm very   unimpressed by demos. Whenever I see demos of \nanything, I'm extremely unimpressed by that.  If it's a demo that someone cooked \nup as a showing, it's worse.  If you can interact with it, it's a bit better.  But even then, you're not done. You need the \nactual product. It's going to face all these   challenges when it comes in contact \nwith reality and all these different   pockets of behavior that need patching.\nWe're going to see all this stuff play   out. It's a march of nines. Each nine is \nconstant. Demos are encouraging. It’s still   a huge amount of work to do.\nIt is a critical safety domain,   unless you're doing vibe coding, \nwhich is all nice and fun and so on.  That's why this also enforced my \ntimelines from that perspective.  It's very interesting to hear you say that, that \nthe safety guarantees you need from software   are not dissimilar to self-driving.\nWhat people will often say is that   self-driving took so long because \nthe cost of failure is so high.  A human makes a mistake on average every \n400,000 miles or every seven years.  If you had to release a coding agent that \ncouldn't make a mistake for at least seven years,   it would be much harder to deploy.\nBut your point is that if you made a   catastrophic coding mistake, like breaking \nsome important system every seven years...  Very easy to do.\nIn fact, in terms of wall clock time,   it would be much less than seven years because \nyou're constantly outputting code like that.  In terms of tokens, it would be seven years.\nBut in terms of wall clock time...  In some ways, it's a much harder problem.\nSelf-driving is just one of   thousands of things that people do.\nIt's almost like a single vertical, I suppose.  Whereas when we're talking about \ngeneral software engineering,   it's even more... There's more surface area.\nThere's another objection people make to that   analogy, which is that with self-driving, what \ntook a big fraction of that time was solving   the problem of having basic perception \nthat's robust, building representations,   and having a model that has some common \nsense so it can generalize to when it sees   something that's slightly out of distribution.\nIf somebody's waving down the road this way,   you don't need to train for it.\nThe thing will have some understanding   of how to respond to something like that.\nThese are things we're getting for free   with LLMs or VLMs today, so we don't have to \nsolve these very basic representation problems.  So now deploying AIs across different domains \nwill sort of be like deploying a self-driving   car with current models to a different city, \nwhich is hard but not like a 10-year-long task.  I'm not 100% sure if I fully agree with that.\nI don't know how much we're getting for free.  There's still a lot of gaps in \nunderstanding what we are getting.  We're definitely getting more \ngeneralizable intelligence in a   single entity, whereas self-driving is a \nvery special-purpose task that requires.  In some sense building a special-purpose task \nis maybe even harder in a certain sense because   it doesn't fall out from a more general thing \nthat you're doing at scale, if that makes sense.  But the analogy still doesn't fully \nresonate because the LLMs are still   pretty fallible and they have a lot of \ngaps that still need to be filled in.  I don't think that we're \ngetting magical generalization   completely out of the box, in a certain sense.\nThe other aspect that I wanted to return to is   that self-driving cars are nowhere near done \nstill. The deployments are pretty minimal.   Even Waymo and so on has very few cars.\nThey're doing that roughly speaking   because they're not economical.\nThey've built something that lives in the future.  They've had to pull back the future, \nbut they had to make it uneconomical.  There are all these costs, not just \nmarginal costs for those cars and   their operation and maintenance, but \nalso the capex of the entire thing.  Making it economical is still \ngoing to be a slog for them.  Also, when you look at these cars and \nthere's no one driving, I actually think   it's a little bit deceiving because there \nare very elaborate teleoperation centers   of people kind of in a loop with these cars.\nI don't have the full extent of it, but there's   more human-in-the-loop than you might expect.\nThere are people somewhere out there   beaming in from the sky.\nI don't know if they're   fully in the loop with the driving.\nSome of the time they are, but they're   certainly involved and there are people.\nIn some sense, we haven't actually removed   the person, we've moved them to \nsomewhere where you can't see them.  I still think there will be some work, as you \nmentioned, going from environment to environment.  There are still challenges \nto make self-driving real.  But I do agree that it's definitely crossed \na threshold where it kind of feels real,   unless it's really teleoperated.\nFor example, Waymo can't go to   all the different parts of the city.\nMy suspicion is that it's parts of the city   where you don't get good signal.\nAnyway, I don't know anything   about the stack. I'm just making stuff up.\nYou led self-driving for five years at Tesla.  Sorry, I don't know anything \nabout the specifics of Waymo.  By the way, I love Waymo \nand I take it all the time.  I just think that people are sometimes a \nlittle bit too naive about some of the progress   and there's still a huge amount of work.\nTesla took in my mind a much more scalable   approach and the team is doing extremely well.\nI'm kind of on the record for predicting   how this thing will go.\nWaymo had an early start   because you can package up so many sensors.\nBut I do think Tesla is taking the more   scalable strategy and it's going \nto look a lot more like that.  So this will still have to play out and hasn't.\nBut I don't want to talk about self-driving as   something that took a decade because it \ndidn't take it yet, if that makes sense.  Because one, the start is at 1980 and not 10 \nyears ago, and then two, the end is not here yet.  The end is not near yet because when \nwe're talking about self-driving,   usually in my mind it's self-driving at scale.\nPeople don't have to get a driver's license, etc.  I'm curious to bounce two other ways in \nwhich the analogy might be different.  The reason I'm especially curious about this is \nbecause the question of how fast AI is deployed,   how valuable it is when it's early \non is potentially the most important   question in the world right now.\nIf you're trying to model what the   year 2030 looks like, this is the question \nyou ought to have some understanding of.  Another thing you might think is one, you have \nthis latency requirement with self-driving.  I have no idea what the actual models are, but I \nassume it’s like tens of millions of parameters   or something, which is not the necessary \nconstraint for knowledge work with LLMs.  Maybe it might be with computer use and stuff.\nBut the other big one is, maybe more   importantly, on this capex question.\nYes, there is additional cost to serving   up an additional copy of a model, but the opex \nof a session is quite low and you can amortize   the cost of AI into the training run itself, \ndepending on how inference scaling goes and stuff.  But it's certainly not as much as building a whole \nnew car to serve another instance of a model.  So the economics of deploying more \nwidely are much more favorable.  I think that's right. If you're \nsticking to the realm of bits,   bits are a million times easier than anything \nthat touches the physical world. I definitely   grant that. Bits are completely changeable, \narbitrarily reshuffleable at a very rapid speed.  You would expect a much faster adaptation also in \nthe industry and so on. What was the first one?  The latency requirements and \nits implications for model size?  I think that's roughly right. I also \nthink that if we are talking about   knowledge work at scale, there will be some \nlatency requirements, practically speaking,   because we're going to have to create a \nhuge amount of compute and serve that.  The last aspect that I very briefly want \nto also talk about is all the rest of it.  What does society think about it? What are \nthe legal ramifications? How is it working   legally? How is it working insurance-wise? \nWhat are those layers of it and aspects of it?  What is the equivalent of people \nputting a cone on a Waymo?  There are going to be equivalents of all that.\nSo I feel like self-driving is a very nice   analogy that you can borrow things from.\nWhat is the equivalent of a cone in the car?  What is the equivalent of a teleoperating worker \nwho's hidden away and all the aspects of it.  Do you have any opinions on what this \nimplies about the current AI buildout,   which would 10x the amount of available compute \nin the world in a year or two and maybe more   than 100x it by the end of the decade.\nIf the use of AI will be lower than   some people naively predict, does \nthat mean that we're overbuilding   compute or is that a separate question?\nKind of like what happened with railroads.  With what, sorry?\nWas it railroads or?  Yeah, it was.\nYeah. There's historical precedent.   Or was it with the telecommunication industry?\nPre-paving the internet that only came a decade   later and creating a whole bubble in the \ntelecommunications industry in the late '90s.  I understand I'm sounding very pessimistic \nhere. I'm actually optimistic. I think this   will work. I think it's tractable. I'm \nonly sounding pessimistic because when   I go on my Twitter timeline, I see all \nthis stuff that makes no sense to me.  There's a lot of reasons for why that exists.\nA lot of it is honestly just fundraising.   It's just incentive structures. \nA lot of it may be fundraising.  A lot of it is just attention, converting \nattention to money on the internet,   stuff like that.\nThere's a lot of   that going on, and I'm only reacting to that.\nBut I'm still overall very bullish on technology.  We're going to work through all this stuff.\nThere's been a rapid amount of progress.  I don't know that there's overbuilding.\nI think we're going to be able to gobble up what,   in my understanding, is being built.\nFor example, Claude Code or OpenAI Codex   and stuff like that didn't even \nexist a year ago. Is that right?   This is a miraculous technology that didn't exist.\nThere's going to be a huge amount of demand,   as we see the demand in ChatGPT already and so on.\nSo I don't know that there's overbuilding.  I'm just reacting to some of the very fast \ntimelines that people continue to say incorrectly.  I've heard many, many times over the course \nof my 15 years in AI where very reputable   people keep getting this wrong all the time.\nI want this to be properly calibrated, and some   of this also has geopolitical ramifications and \nthings like that with some of these questions.  I don't want people to make \nmistakes in that sphere of things.  I do want us to be grounded in the \nreality of what technology is and isn't.  Let's talk about education and Eureka.\nOne thing you could do is start another AI   lab and then try to solve those problems.\nI’m curious what you're up to now,   and why not AI research itself?\nI guess the way I would put it   is I feel some amount of determinism \naround the things that AI labs are doing.  I feel like I could help out there, but I \ndon't know that I would uniquely improve it.  My personal big fear is that a lot of this \nstuff happens on the side of humanity,   and that humanity gets disempowered by it.\nI care not just about all the Dyson spheres   that we're going to build and that AI is \ngoing to build in a fully autonomous way,   I care about what happens to humans.\nI want humans to be well off in the future.  I feel like that's where I can a \nlot more uniquely add value than   an incremental improvement in the frontier lab.\nI'm most afraid of something depicted in movies   like WALL-E or Idiocracy or something like that, \nwhere humanity is on the side of this stuff.  I want humans to be much, \nmuch better in this future.  To me, this is through education \nthat you can achieve this.  So what are you working on there?\nThe easiest way I can describe it is   we're trying to build the Starfleet Academy.\nI don’t know if you’ve watched Star Trek.  I haven’t.\nStarfleet Academy is   this elite institution for frontier technology, \nbuilding spaceships, and graduating cadets to be   the pilots of these spaceships and whatnot.\nSo I just imagine an elite institution for   technical knowledge and a kind of school that's \nvery up-to-date and a premier institution.  A category of questions I have for you is \nexplaining how one teaches technical or   scientific content well, because you \nare one of the world masters at it.  I'm curious both about how you think about \nit for content you've already put out there   on YouTube, but also, to the extent it's any \ndifferent, how you think about it for Eureka.  With respect to Eureka, one thing that \nis very fascinating to me about education   is that I do think education will pretty \nfundamentally change with AIs on the side.  It has to be rewired and changed to some extent.\nI still think that we're pretty early.  There's going to be a lot of people who \nare going to try to do the obvious things.  Have an LLM and ask it questions.\nDo all the basic things that you would   do via prompting right now.\nIt's helpful,   but it still feels to me a bit like slop.\nI'd like to do it properly, and I think the   capability is not there for what I would want.\nWhat I'd want is an actual tutor experience.  A prominent example in my mind is I was \nrecently learning Korean, so language learning.  I went through a phase where I was \nlearning Korean by myself on the internet.  I went through a phase where I was part of a small \nclass in Korea taking Korean with a bunch of other   people, which was really funny.\nWe had a teacher and 10   people or so taking Korean.\nThen I switched to a one-on-one tutor.  I guess what was fascinating to me was, I think \nI had a really good tutor, but just thinking   through what this tutor was doing for me and how \nincredible that experience was and how high the   bar is for what I want to build eventually.\nInstantly from a very short   conversation, she understood where I am \nas a student, what I know and don't know.  She was able to probe exactly the kinds of \nquestions or things to understand my world model.  No LLM will do that for you \n100% right now, not even close.  But a tutor will do that if they're good.\nOnce she understands, she really served   me all the things that I needed at \nmy current sliver of capability.  I need to be always appropriately challenged.\nI can't be faced with something too hard or   too trivial, and a tutor is really good \nat serving you just the right stuff.  I felt like I was the only constraint to learning.\nI was always given the perfect information. I'm   the only constraint. I felt good because \nI'm the only impediment that exists.  It's not that I can't find knowledge or \nthat it's not properly explained or etc.  It's just my ability to memorize and so on.\nThis is what I want for people.  How do you automate that?\nVery good question. At   the current capability, you don't.\nThat's why I think it's not actually the   right time to build this kind of an AI tutor.\nI still think it's a useful product,   and lots of people will build it, but the bar \nis so high and the capability is not there.  Even today, I would say ChatGPT is an \nextremely valuable educational product.  But for me, it was so fascinating \nto see how high the bar is.  When I was with her, I almost felt \nlike there's no way I can build this.  But you are building it, right?\nAnyone who's had a really good   tutor is like, \"How are you going to build \nthis?\" I'm waiting for that capability. I   did some AI consulting for computer vision.\nA lot of times, the value that I brought to   the company was telling them not to use AI.\nI was the AI expert, and they described the   problem, and I said, \"Don't use AI.\" \nThis is my value add. I feel like it's   the same in education right now, where \nI feel like for what I have in mind,   it's not yet the time, but the time will come.\nFor now, I'm building something that looks   maybe a bit more conventional that has a \nphysical and digital component and so on.  But it's obvious how this \nshould look in the future.  To the extent you're willing to \nsay, what is the thing you hope   will be released this year or next year?\nI'm building the first course. I want to   have a really, really good course, the \nobvious state-of-the-art destination   you go to to learn, AI in this case.\nThat's just what I'm familiar with, so it's   a really good first product to get to be really \ngood at it. So that's what I'm building. Nanochat,   which you briefly mentioned, is a capstone project \nof LLM101N, which is a class that I'm building.  That's a really big piece of it.\nBut now I have to build out a lot of   the intermediates, and then I have to hire a small \nteam of TAs and so on and build the entire course.  One more thing that I would say is that many \ntimes, when people think about education,   they think more about what I would say is \na softer component of diffusing knowledge.  I have something very hard and technical in mind.\nIn my mind, education is the very difficult   technical process of building ramps to knowledge.\nIn my mind, nanochat is a ramp to   knowledge because it's very simple.\nIt's the super simplified full-stack thing.  If you give this artifact to someone and they \nlook through it, they're learning a ton of stuff.  It's giving you a lot of what I call eurekas \nper second, which is understanding per second.  That's what I want, lots of eurekas per second.\nSo to me, this is a technical problem of   how do we build these ramps to knowledge.\nSo I almost think of Eureka as maybe not that   different from some of the frontier labs \nor some of the work that's going on there.  I want to figure out how to build these \nramps very efficiently so that people are   never stuck and everything is always \nnot too hard or not too trivial, and   you have just the right material to progress.\nYou're imagining in the short term that instead   of a tutor being able to probe your understanding, \nif you have enough self-awareness to be able to   probe yourself, you're never going to be stuck.\nYou can find the right answer between talking   to the TA or talking to an LLM and \nlooking at the reference implementation.  It sounds like automation or \nAI is not a significant part.  So far, the big alpha here is your \nability to explain AI codified   in the source material of the class.\nThat's fundamentally what the course is.  You always have to be calibrated to \nwhat capability exists in the industry.  A lot of people are going to \npursue just asking ChatGPT, etc.  But I think right now, for example, if you go to \nChatGPT and you say, teach me AI, there's no way.  It's going to give you some slop.\nAI is never going to write nanochat right now.  But nanochat is a really \nuseful intermediate point.  I'm collaborating with AI \nto create all this material,   so AI is still fundamentally very helpful.\nEarlier on, I built CS231n at Stanford,   which I think was the first deep learning \nclass at Stanford, which became very popular.  The difference in building out 231n \nthen and LLM101N now is quite stark.  I feel really empowered by the LLMs as they \nexist right now, but I'm very much in the loop.  They're helping me build the \nmaterials, I go much faster.  They're doing a lot of the boring stuff, etc.\nI feel like I'm developing the course much faster,   and it's LLM-infused, but it's not yet at a \nplace where it can creatively create the content.  I'm still there to do that.\nThe trickiness is always   calibrating yourself to what exists.\nWhen you imagine what is available   through Eureka in a couple of years, it \nseems like the big bottleneck is going to be   finding Karpathys in field after field who can \nconvert their understanding into these ramps.  It would change over time. Right now, \nit would be hiring faculty to help work   hand-in-hand with AI and a team of people \nprobably to build state-of-the-art courses.  Over time maybe some of the TAs can become AIs.\nYou just take all the course materials and then   I think you could serve a very good automated \nTA for the student when they have more basic   questions or something like that.\nBut I think you'll need faculty   for the overall architecture of a \ncourse and making sure that it fits.  So I see a progression of how this will evolve.\nMaybe at some future point I'm not even that   useful and AI is doing most of the \ndesign much better than I could.  But I still think that's going \nto take some time to play out.  Are you imagining that people who have expertise \nin other fields are then contributing courses,   or do you feel like it's quite \nessential to the vision that you,   given your understanding of how you want to \nteach, are the one designing the content?  Sal Khan is narrating all \nthe videos on Khan Academy.  Are you imagining something like that?\nNo, I will hire faculty because there   are domains in which I'm not an expert.\nThat's the only way to offer the state-of-the-art   experience for the student ultimately.\nI do expect that I would hire faculty, but   I will probably stick around in AI for some time.\nI do have something more conventional in mind for   the current capability than what \npeople would probably anticipate.  When I'm building Starfleet Academy, I do probably \nimagine a physical institution, and maybe a tier   below that a digital offering that is not the \nstate-of-the-art experience you would get when   someone comes in physically full-time and we \nwork through material from start to end and   make sure you understand it. That's the physical \noffering. The digital offering is a bunch of stuff   on the internet and maybe some LLM assistant.\nIt's a bit more gimmicky in a tier below, but   at least it's accessible to 8 billion people.\nI think you're basically inventing college   from first principles for the tools that \nare available today and just selecting   for people who have the motivation and the \ninterest of really engaging with material.  There's going to have to be a lot of not \njust education but also re-education.  I would love to help out there because \nthe jobs will probably change quite a bit.  For example, today a lot of people are \ntrying to upskill in AI specifically.  I think it's a really good \ncourse to teach in this respect.  Motivation-wise, before AGI motivation is very \nsimple to solve because people want to make money.  This is how you make money in the industry today.\nPost-AGI is a lot more interesting possibly   because if everything is automated \nand there's nothing to do for anyone,   why would anyone go to a school?\nI often say that pre-AGI education   is useful. Post-AGI education is fun. In \na similar way, people go to the gym today.  We don't need their physical strength \nto manipulate heavy objects because we   have machines that do that.\nThey still go to the gym.  Why do they go to the gym?\nBecause it's fun, it's healthy,   and you look hot when you have a six-pack.\nIt's attractive for people to do that   in a very deep, psychological, \nevolutionary sense for humanity.  Education will play out in the same way.\nYou'll go to school like you go to the gym.  Right now, not that many people learn \nbecause learning is hard. You bounce   from material. Some people overcome that \nbarrier, but for most people, it's hard.  It's a technical problem to solve.\nIt's a technical problem to do what my tutor   did for me when I was learning Korean.\nIt's tractable and buildable,   and someone should build it.\nIt's going to make learning   anything trivial and desirable, and people \nwill do it for fun because it's trivial.  If I had a tutor like that for any arbitrary piece \nof knowledge, it's going to be so much easier to   learn anything, and people will do it.\nThey'll do it for the same   reasons they go to the gym.\nThat sounds different from using…   So post-AGI, you're using this as \nentertainment or as self-betterment.  But it sounded like you had a vision \nalso that this education is relevant to   keeping humanity in control of AI. That sounds \ndifferent. Is it entertaining for some people,   but then empowerment for some others?\nHow do you think about that?  I do think eventually it's a bit of \na losing game, if that makes sense.  It is in the long term.\nIn the long term, which   is longer than maybe most people in the \nindustry think about, it's a losing game.  I do think people can go so far and we've barely \nscratched the surface of how much a person can go.  That's just because people are bouncing off \nof material that's too easy or too hard.  People will be able to go much further.\nAnyone will speak five languages because   why not? Because it's so trivial. Anyone will know \nall the basic curriculum of undergrad, et cetera.  Now that I'm understanding the \nvision, that's very interesting.  It has a perfect analog in gym culture.\nI don't think 100 years   ago anybody would be ripped.\nNobody would have been able to just spontaneously   bench two plates or three plates or something.\nIt's very common now because of this idea of   systematically training and lifting weights in \nthe gym, or systematically training to be able   to run a marathon, which is a capability \nmost humans would not spontaneously have.  You're imagining similar things for \nlearning across many different domains,   much more intensely, deeply, faster.\nExactly. I am betting a bit implicitly   on some of the timelessness of human nature.\nIt will be desirable to do all these things,   and I think people will look up \nto it as they have for millennia.  This will continue to be true.\nThere's some evidence of that historically.  If you look at, for example, aristocrats, or you \nlook at ancient Greece or something like that,   whenever you had little pocket environments \nthat were post-AGI in a certain sense, people   have spent a lot of their time flourishing in a \ncertain way, either physically or cognitively.  I feel okay about the prospects of that.\nIf this is false and I'm wrong and we end up in a   WALL-E or Idiocracy future, then I don't even care \nif there are Dyson spheres. This is a terrible   outcome. I really do care about humanity.\nEveryone has to just be   superhuman in a certain sense.\nIt's still a world in which that is not enabling   us to… It's like the culture world, right?\nYou're not fundamentally going to be able   to transform the trajectory \nof technology or influence   decisions by your own labor or cognition alone.\nMaybe you can influence decisions because the AI   is asking for your approval, but it's not because \nI've invented something or I've come up with a new   design that I'm really influencing the future.\nMaybe. I think there will be a transitional   period where we are going to be \nable to be in the loop and advance   things if we understand a lot of stuff.\nIn the long-term, that probably goes away.  It might even become a sport.\nRight now you have powerlifters   who go extreme in this direction.\nWhat is powerlifting in a cognitive era?  Maybe it's people who are really trying \nto make Olympics out of knowing stuff.  If you have a perfect AI tutor, \nmaybe you can get extremely far.  I feel that the geniuses of \ntoday are barely scratching the   surface of what a human mind can do, I think.\nI love this vision. I also feel like the person   you have the most product-market fit with is me \nbecause my job involves having to learn different   subjects every week, and I am very excited.\nI'm similar, for that matter. A lot of people,   for example, hate school and want to get \nout of it. I really liked school. I loved   learning things, et cetera.\nI wanted to stay in school.  I stayed all the way until Ph.D. and \nthen they wouldn't let me stay longer,   so I went to the industry.\nRoughly speaking, I love learning,   even for the sake of learning, but I also love \nlearning because it's a form of empowerment and   being useful and productive.\nYou also made a point that   was subtle and I want to spell it out.\nWith what’s happened so far with online   courses, why haven't they already enabled us to \nenable every single human to know everything?  They're just so motivation-laden because there are \nno obvious on-ramps and it's so easy to get stuck.  If you had this thing instead—like a really \ngood human tutor—it would just be such an   unlock from a motivation perspective.\nI think so. It feels bad to bounce from   material. It feels bad. You get negative reward \nfrom sinking an amount of time in something and it   doesn't pan out, or being completely bored because \nwhat you're getting is too easy or too hard.  When you do it properly, learning feels good.\nIt's a technical problem to get there.  For a while, it's going to be AI plus human \ncollab, and at some point, maybe it's just AI.  Can I ask some questions about teaching well?\nIf you had to give advice to another educator   in another field that you're curious about to \nmake the kinds of YouTube tutorials you've made.  Maybe it might be especially interesting \nto talk about domains where you can't   test someone's technical understanding by \nhaving them code something up or something.  What advice would you give them?\nThat's a pretty broad topic. There are 10–20 tips   and tricks that I semi-consciously do probably.\nBut a lot of this comes   from my physics background.\nI really, really did enjoy my physics background.  I have a whole rant on how everyone \nshould learn physics in early school   education because early school education is \nnot about accumulating knowledge or memory   for tasks later in the industry.\nIt's about booting up a brain.  Physics uniquely boots up the brain the \nbest because some of the things that they   get you to do in your brain during \nphysics is extremely valuable later.  The idea of building models and abstractions \nand understanding that there's a first-order   approximation that describes most of the system, \nbut then there're second-order, third-order,   fourth-order terms that may or may not be present.\nThe idea that you're observing a very noisy   system, but there are these fundamental \nfrequencies that you can abstract away.  When a physicist walks into the class and \nthey say, \"Assume there's a spherical cow,\"   everyone laughs at that, but this is brilliant.\nIt's brilliant thinking that's very generalizable   across the industry because a cow can be \napproximated as a sphere in a bunch of ways.  There's a really good book, for example, Scale.\nIt's from a physicist talking about biology.  Maybe this is also a book \nI would recommend reading.  You can get a lot of really interesting \napproximations and chart scaling laws of animals.  You can look at their heartbeats and \nthings like that, and they line up with   the size of the animal and things like that.\nYou can talk about an animal as a volume.  You can talk about the heat dissipation of that, \nbecause your heat dissipation grows as the surface   area, which is growing as a square.\nBut your heat creation or generation   is growing as a cube.\nSo I just feel like physicists   have all the right cognitive tools to \napproach problem solving in the world.  So because of that training, I \nalways try to find the first-order   terms or the second-order terms of everything.\nWhen I'm observing a system or a thing, I have a   tangle of a web of ideas or knowledge in my mind.\nI'm trying to find, what is the thing that   matters? What is the first-order component? \nHow can I simplify it? How can I have a   simplest thing that shows that thing, shows it in \naction, and then I can tack on the other terms?  Maybe an example from one of my repos that I \nthink illustrates it well is called micrograd.  I don't know if you're familiar with this.\nSo micrograd is 100 lines of code   that shows backpropagation.\nYou can create neural networks   out of simple operations like plus and times, et \ncetera. Lego blocks of neural networks. You build   up a computational graph and you do a forward \npass and a backward pass to get the gradients.  Now, this is at the heart of \nall neural network learning.  So micrograd is a 100 lines of \npretty interpretable Python code,   and it can do forward and backward arbitrary \nneural networks, but not efficiently.  So micrograd, these 100 lines of Python, \nare everything you need to understand how   neural networks train. Everything else is just \nefficiency. Everything else is efficiency. There's   a huge amount of work to get efficiency.\nYou need your tensors, you lay them out,   you stride them, you make sure \nyour kernels, orchestrating   memory movement correctly, et cetera.\nIt's all just efficiency, roughly speaking.  But the core intellectual piece of neural \nnetwork training is micrograd. It's 100 lines.   You can easily understand it. It's a recursive \napplication of chain rule to derive the gradient,   which allows you to optimize any \narbitrary differentiable function.  So I love finding these small-order terms and \nserving them on a platter and discovering them.  I feel like education is the most intellectually \ninteresting thing because you have a tangle   of understanding and you're trying to lay \nit out in a way that creates a ramp where   everything only depends on the thing before it.\nI find that this untangling of knowledge is just   so intellectually interesting as a cognitive task.\nI love doing it personally, but I just   have a fascination with trying to lay things \nout in a certain way. Maybe that helps me.  It also makes the learning \nexperience so much more motivated.  Your tutorial on the transformer begins \nwith bigrams, literally a lookup table from,   \"Here's the word right now, or here's \nthe previous word, here's the next word.\"  It's literally just a lookup table.\nThat’s the essence of it, yeah.  It’s such a brilliant way, starting with a \nlookup table and then going to a transformer.   Each piece is motivated. Why would you add \nthat? Why would you add the next thing?  You could memorize the attention formula, \nbut having an understanding of why every   single piece is relevant, what problem it solves.\nYou're presenting the pain before you present a   solution, and how clever is that?\nYou want to take the student   through that progression.\nThere are a lot of other small   things that make it nice and engaging and \ninteresting. Always prompting the student.   There's a lot of small things like that are \nimportant and a lot of good educators will do   this. How would you solve this? I'm not going to \npresent the solution before you guess. That would   be wasteful. That's a little bit of a…I don’t \nwant to swear but it’s a dick move towards you   to present you with the solution before I give \nyou a shot to try to come up with it yourself.  Because if you try to come up with it yourself, \nyou get a better understanding of what the action   space is, what the objective is, and then \nwhy only this action fulfills that objective.  You have a chance to try it yourself, and you \nhave an appreciation when I give you the solution.  It maximizes the amount of \nknowledge per new fact added.  Why do you think, by default, people who are \ngenuine experts in their field are often bad   at explaining it to somebody ramping up?\nIt's the curse of knowledge and expertise.  This is a real phenomenon, and I suffered \nfrom it myself as much as I try not to.  But you take certain things for granted, \nand you can't put yourself in the shoes   of new people who are just starting out.\nThis is pervasive and happens to me as   well. One thing that's extremely helpful. \nAs an example, someone was trying to show   me a paper in biology recently, and I just \ninstantly had so many terrible questions.  What I did was I used ChatGPT to ask the \nquestions with the paper in the context window.  It worked through some of the simple things.\nThen I shared the thread to the person who   wrote that paper or worked on that work.\nI felt like if they could see the dumb   questions I had, it might help \nthem explain better in the future.  For my material, I would love it if people \nshared their dumb conversations with ChatGPT   about the stuff that I've created \nbecause it really helps me put myself   again in the shoes of someone who's starting out.\nAnother trick that just works astoundingly well.  If somebody writes a paper or a blog post or an \nannouncement, it is in 100% of cases that just   the narration or the transcription of how they \nwould explain it to you over lunch is way more,   not only understandable, but actually \nalso more accurate and scientific,   in the sense that people have a bias \nto explain things in the most abstract,   jargon-filled way possible and to clear \ntheir throat for four paragraphs before   they explain the central idea.\nBut there's something about   communicating one-on-one with a person \nwhich compels you to just say the thing.  Just say the thing. I saw that \ntweet, I thought it was really good.  I shared it with a bunch of people.\nI noticed this many, many times.  The most prominent example is that I \nremember back in my PhD days doing research.  You read someone's paper, and you \nwork to understand what it's doing.  Then you catch them, you're having beers \nat the conference later, and you ask them,   \"So this paper, what were you doing? What is the \npaper about?\" They will just tell you these three   sentences that perfectly captured the essence \nof that paper and totally give you the idea.  And you didn't have to read the paper.\nIt's only when you're sitting at the table   with a beer or something, and they're \nlike, \"Oh yeah, the paper is just,   you take this idea, you take that idea and try \nthis experiment and you try out this thing.\"  They have a way of just putting it \nconversationally just perfectly.   Why isn't that the abstract?\nExactly. This is coming from the   perspective of how somebody who's trying to \nexplain an idea should formulate it better.  What is your advice as a student to other \nstudents, if you don't have a Karpathy   who is doing the exposition of an idea?\nIf you're reading a paper from somebody   or reading a book, what strategies do \nyou employ to learn material you're   interested in in fields you're not an expert at?\nI don't know that I have unique tips and tricks,   to be honest. It's a painful process. One thing \nthat has always helped me quite a bit is—I   had a small tweet about this—learning things \non demand is pretty nice. Learning depth-wise.   I do feel you need a bit of alternation of \nlearning depth-wise, on demand—you're trying   to achieve a certain project that you're going \nto get a reward from—and learning breadth-wise,   which is just, \"Oh, let's do whatever 101, \nand here's all the things you might need.\"  Which is a lot of school—does breadth-wise \nlearning, like, \"Oh, trust me, you'll need   this later,\" that kind of stuff. Okay, I trust \nyou. I'll learn it because I guess I need it.  But I love the kind of learning \nwhere you'll get a reward out of   doing something, and you're learning on demand.\nThe other thing that I've found extremely helpful.  This is an aspect where education is a bit more \nselfless, but explaining things to people is a   beautiful way to learn something more deeply.\nThis happens to me all the time.  It probably happens to other people too because \nI realize if I don't really understand something,   I can't explain it.\nI'm trying and I'm like,   \"Oh, I don't understand this.\"\nIt's so annoying to come to terms with that.  You can go back and make sure you understood it.\nIt fills these gaps of your understanding.  It forces you to come to terms \nwith them and to reconcile them.  I love to re-explain things and people \nshould be doing that more as well.  That forces you to manipulate the knowledge \nand make sure that you know what you're   talking about when you're explaining it.\nThat's an excellent note to close on. Andrej,   that was great.\nThank you.",
  "fetchedAt": "2026-01-18T18:33:59.369Z"
}