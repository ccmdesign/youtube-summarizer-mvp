{
  "videoId": "mwzk2rlwtZE",
  "language": "en",
  "source": "caption-extractor",
  "segments": [
    {
      "start": 13.755,
      "duration": 2.02,
      "text": "[music]"
    },
    {
      "start": 21.68,
      "duration": 4.24,
      "text": "Hi everyone, we're about to start the"
    },
    {
      "start": 23.92,
      "duration": 4.4,
      "text": "next session. Thank you guys so much for"
    },
    {
      "start": 25.92,
      "duration": 3.92,
      "text": "coming out today. Um, this is going to"
    },
    {
      "start": 28.32,
      "duration": 3.039,
      "text": "be a build your own sales agent"
    },
    {
      "start": 29.84,
      "duration": 3.2,
      "text": "workshop. So, we're going to be walking"
    },
    {
      "start": 31.359,
      "duration": 3.2,
      "text": "through everything you need to know to"
    },
    {
      "start": 33.04,
      "duration": 3.6,
      "text": "build your own voice agent. My name is"
    },
    {
      "start": 34.559,
      "duration": 4.241,
      "text": "Sarah Chang from Cerebras and I am"
    },
    {
      "start": 36.64,
      "duration": 3.759,
      "text": "excited to be joined by Genway. Um, and"
    },
    {
      "start": 38.8,
      "duration": 2.96,
      "text": "we are both part of the DevX team at"
    },
    {
      "start": 40.399,
      "duration": 3.84,
      "text": "Cerebras."
    },
    {
      "start": 41.76,
      "duration": 3.84,
      "text": ">> Yeah, thanks Sarah. Um, so today we're"
    },
    {
      "start": 44.239,
      "duration": 3.84,
      "text": "going to walk through how to build a"
    },
    {
      "start": 45.6,
      "duration": 4.56,
      "text": "voice sales agent that can actually have"
    },
    {
      "start": 48.079,
      "duration": 4.96,
      "text": "a natural conversations with customers"
    },
    {
      "start": 50.16,
      "duration": 4.399,
      "text": "and our sales agents will pull product"
    },
    {
      "start": 53.039,
      "duration": 4.081,
      "text": "contacts from an external source to"
    },
    {
      "start": 54.559,
      "duration": 5.121,
      "text": "respond in real time. So, we're going to"
    },
    {
      "start": 57.12,
      "duration": 5.52,
      "text": "be building an AI agent that can speak,"
    },
    {
      "start": 59.68,
      "duration": 6.96,
      "text": "listen, and respond intelligently"
    },
    {
      "start": 62.64,
      "duration": 6.159,
      "text": "um to your company's sales materials."
    },
    {
      "start": 66.64,
      "duration": 3.76,
      "text": "And we have the full code for you to"
    },
    {
      "start": 68.799,
      "duration": 5.121,
      "text": "follow along with. We have a notebook"
    },
    {
      "start": 70.4,
      "duration": 4.88,
      "text": "that you can scan later um to step ghost"
    },
    {
      "start": 73.92,
      "duration": 4.64,
      "text": "and we'll walk you through it step by"
    },
    {
      "start": 75.28,
      "duration": 5.199,
      "text": "step in just a moment."
    },
    {
      "start": 78.56,
      "duration": 3.599,
      "text": "So, before we get started, let's go"
    },
    {
      "start": 80.479,
      "duration": 3.68,
      "text": "through what you will get out of this"
    },
    {
      "start": 82.159,
      "duration": 4.801,
      "text": "workshop. So you will get free API"
    },
    {
      "start": 84.159,
      "duration": 4.161,
      "text": "credits for Cerebrris livekit cartisia."
    },
    {
      "start": 86.96,
      "duration": 3.12,
      "text": "You will have the quick start. We'll"
    },
    {
      "start": 88.32,
      "duration": 3.68,
      "text": "have again have a full code notebook for"
    },
    {
      "start": 90.08,
      "duration": 3.52,
      "text": "you to follow along with and at the end"
    },
    {
      "start": 92,
      "duration": 3.439,
      "text": "you will have your very own sales agent"
    },
    {
      "start": 93.6,
      "duration": 4.879,
      "text": "that you can hook up to your company's"
    },
    {
      "start": 95.439,
      "duration": 6.32,
      "text": "materials so that you can you know"
    },
    {
      "start": 98.479,
      "duration": 5.28,
      "text": "implement this in production."
    },
    {
      "start": 101.759,
      "duration": 3.521,
      "text": "So here's the starter code that I would"
    },
    {
      "start": 103.759,
      "duration": 3.601,
      "text": "recommend scanning just so you can"
    },
    {
      "start": 105.28,
      "duration": 3.92,
      "text": "follow along. Um, again, this is what"
    },
    {
      "start": 107.36,
      "duration": 3.52,
      "text": "we'll be walking through step by step"
    },
    {
      "start": 109.2,
      "duration": 3.52,
      "text": "today. And there will be individual"
    },
    {
      "start": 110.88,
      "duration": 5.279,
      "text": "modules that you'll be able to just run"
    },
    {
      "start": 112.72,
      "duration": 4.88,
      "text": "and see some good outfits."
    },
    {
      "start": 116.159,
      "duration": 4.441,
      "text": "So, I'll give you a few seconds for"
    },
    {
      "start": 117.6,
      "duration": 3,
      "text": "that."
    },
    {
      "start": 121.28,
      "duration": 4.4,
      "text": "We'll have the QR code later as well, so"
    },
    {
      "start": 123.119,
      "duration": 4.161,
      "text": "not to worry. So, before we get started,"
    },
    {
      "start": 125.68,
      "duration": 3.84,
      "text": "I wanted to talk a little bit about"
    },
    {
      "start": 127.28,
      "duration": 4.56,
      "text": "Cerebrus and, you know, Cerebrus"
    },
    {
      "start": 129.52,
      "duration": 4.24,
      "text": "inferences secret sauce. So, for those"
    },
    {
      "start": 131.84,
      "duration": 4.08,
      "text": "of you who are unfamiliar, we are a"
    },
    {
      "start": 133.76,
      "duration": 4.72,
      "text": "hardware company. We are building an AI"
    },
    {
      "start": 135.92,
      "duration": 3.92,
      "text": "processor that is much larger and much"
    },
    {
      "start": 138.48,
      "duration": 4.56,
      "text": "faster than what you are probably"
    },
    {
      "start": 139.84,
      "duration": 6.08,
      "text": "familiar with with Nvidia GPUs. So out"
    },
    {
      "start": 143.04,
      "duration": 4.8,
      "text": "of curiosity, I'm wondering how many"
    },
    {
      "start": 145.92,
      "duration": 5.12,
      "text": "people here have heard about Cerebras"
    },
    {
      "start": 147.84,
      "duration": 5.92,
      "text": "hardware. Not bad. Okay. Higher than"
    },
    {
      "start": 151.04,
      "duration": 6.08,
      "text": "last year. Okay. Okay. So before we do"
    },
    {
      "start": 153.76,
      "duration": 5.52,
      "text": "go, I want to share um I want to show"
    },
    {
      "start": 157.12,
      "duration": 5.52,
      "text": "everyone [clears throat] the speed of"
    },
    {
      "start": 159.28,
      "duration": 5.76,
      "text": "what we're talking about here. So So"
    },
    {
      "start": 162.64,
      "duration": 4.72,
      "text": "this is just a chat. It's running on"
    },
    {
      "start": 165.04,
      "duration": 3.919,
      "text": "Cerebras. You can choose any. So, we can"
    },
    {
      "start": 167.36,
      "duration": 3.28,
      "text": "host any different model on our"
    },
    {
      "start": 168.959,
      "duration": 4.481,
      "text": "hardware. So, I'm going to choose an"
    },
    {
      "start": 170.64,
      "duration": 3.84,
      "text": "example model like a llama model. And"
    },
    {
      "start": 173.44,
      "duration": 2.159,
      "text": "I'm [snorts] going to give it a prompt."
    },
    {
      "start": 174.48,
      "duration": 2.88,
      "text": "So, I'm going to give it a prompt that"
    },
    {
      "start": 175.599,
      "duration": 4.528,
      "text": "it's intentionally asking it to respond"
    },
    {
      "start": 177.36,
      "duration": 4.787,
      "text": "something a little longer. This go"
    },
    {
      "start": 180.127,
      "duration": 2.02,
      "text": "[clears throat]"
    },
    {
      "start": 182.56,
      "duration": 7.44,
      "text": "funny dad jokes, but make each joke a"
    },
    {
      "start": 187.28,
      "duration": 5.28,
      "text": "couple sentences."
    },
    {
      "start": 190,
      "duration": 4.319,
      "text": "Sentences."
    },
    {
      "start": 192.56,
      "duration": 3.599,
      "text": "And that's how fast it generates. Does"
    },
    {
      "start": 194.319,
      "duration": 5.881,
      "text": "anyone else have a prompt you want to"
    },
    {
      "start": 196.159,
      "duration": 4.041,
      "text": "try? A longer prompt."
    },
    {
      "start": 210.72,
      "duration": 5.68,
      "text": ">> Amazing. There you go."
    },
    {
      "start": 214.879,
      "duration": 2.801,
      "text": "So, really quickly before we get"
    },
    {
      "start": 216.4,
      "duration": 3.04,
      "text": "started, I know we have a lot of"
    },
    {
      "start": 217.68,
      "duration": 4.4,
      "text": "software geeks here, but I do want to"
    },
    {
      "start": 219.44,
      "duration": 4.4,
      "text": "for a second talk about hardware. And I"
    },
    {
      "start": 222.08,
      "duration": 3.76,
      "text": "want to talk a little bit about what"
    },
    {
      "start": 223.84,
      "duration": 5.679,
      "text": "hardware innovations"
    },
    {
      "start": 225.84,
      "duration": 5.84,
      "text": "um make such fast inference possible"
    },
    {
      "start": 229.519,
      "duration": 4.64,
      "text": "especially as we build a new generation"
    },
    {
      "start": 231.68,
      "duration": 4.24,
      "text": "of AI products."
    },
    {
      "start": 234.159,
      "duration": 4.401,
      "text": "And so we're going to a little bit of a"
    },
    {
      "start": 235.92,
      "duration": 4.959,
      "text": "hardware segment, but one of the main"
    },
    {
      "start": 238.56,
      "duration": 4.319,
      "text": "secret sauces for Cerebras is that"
    },
    {
      "start": 240.879,
      "duration": 4.161,
      "text": "Cerebras chips do not have memory"
    },
    {
      "start": 242.879,
      "duration": 4.08,
      "text": "bandwidth issues. And I don't know how"
    },
    {
      "start": 245.04,
      "duration": 4,
      "text": "familiar you guys are with, you know,"
    },
    {
      "start": 246.959,
      "duration": 5.2,
      "text": "GPU architecture, but we're actually"
    },
    {
      "start": 249.04,
      "duration": 5.839,
      "text": "gonna de deep dive really quickly into"
    },
    {
      "start": 252.159,
      "duration": 5.681,
      "text": "how GPU architecture works and how it"
    },
    {
      "start": 254.879,
      "duration": 5.441,
      "text": "compares to what people are doing today."
    },
    {
      "start": 257.84,
      "duration": 4.239,
      "text": "And so for context, this is the hardware"
    },
    {
      "start": 260.32,
      "duration": 4.159,
      "text": "that, you know, all of our inference"
    },
    {
      "start": 262.079,
      "duration": 4.321,
      "text": "runs on. It's the wafer scale engine 3."
    },
    {
      "start": 264.479,
      "duration": 4.72,
      "text": "It is quite literally the size of a"
    },
    {
      "start": 266.4,
      "duration": 5.519,
      "text": "dinner plate. And this has 4 trillion"
    },
    {
      "start": 269.199,
      "duration": 5.44,
      "text": "transistors, 900,000 cores, and very"
    },
    {
      "start": 271.919,
      "duration": 4.881,
      "text": "significant amounts of onchip memory."
    },
    {
      "start": 274.639,
      "duration": 3.921,
      "text": "And so this is the comparison of what"
    },
    {
      "start": 276.8,
      "duration": 4.16,
      "text": "our hardware looks like next to the"
    },
    {
      "start": 278.56,
      "duration": 5.04,
      "text": "NVIDIA GPU. So you can see some of those"
    },
    {
      "start": 280.96,
      "duration": 4.16,
      "text": "metrics line up. So significantly more"
    },
    {
      "start": 283.6,
      "duration": 4.48,
      "text": "transistors."
    },
    {
      "start": 285.12,
      "duration": 5.519,
      "text": "But to actually understand what Cerebras"
    },
    {
      "start": 288.08,
      "duration": 6.72,
      "text": "did with their hardware that is makes it"
    },
    {
      "start": 290.639,
      "duration": 7.201,
      "text": "able to achieve 20x 30x f 70x faster"
    },
    {
      "start": 294.8,
      "duration": 4.56,
      "text": "speeds than in inference on Nvidia GPUs."
    },
    {
      "start": 297.84,
      "duration": 5.04,
      "text": "We're going to actually start by taking"
    },
    {
      "start": 299.36,
      "duration": 6.16,
      "text": "a look at the Nvidia GPU. So this is a"
    },
    {
      "start": 302.88,
      "duration": 4.8,
      "text": "diagram of an H100."
    },
    {
      "start": 305.52,
      "duration": 5.36,
      "text": "And if you look at the red rectangle,"
    },
    {
      "start": 307.68,
      "duration": 6.079,
      "text": "that is a core. And so on the H100"
    },
    {
      "start": 310.88,
      "duration": 5.68,
      "text": "there's about 17,000 cores and each of"
    },
    {
      "start": 313.759,
      "duration": 4.481,
      "text": "these cores is the is what is actually"
    },
    {
      "start": 316.56,
      "duration": 4,
      "text": "doing all of the mathematical"
    },
    {
      "start": 318.24,
      "duration": 4,
      "text": "computations needed in training or"
    },
    {
      "start": 320.56,
      "duration": 4.8,
      "text": "inference or whatever computation you"
    },
    {
      "start": 322.24,
      "duration": 7.679,
      "text": "need to do. So every core has a subset"
    },
    {
      "start": 325.36,
      "duration": 7.279,
      "text": "of the computations um that is assigned."
    },
    {
      "start": 329.919,
      "duration": 4.321,
      "text": "So when you run inference what are some"
    },
    {
      "start": 332.639,
      "duration": 3.601,
      "text": "of the types of things that a core will"
    },
    {
      "start": 334.24,
      "duration": 5.519,
      "text": "need access to to do its computation? it"
    },
    {
      "start": 336.24,
      "duration": 7.2,
      "text": "needs its weight, activations, KV cache,"
    },
    {
      "start": 339.759,
      "duration": 5.681,
      "text": "etc. On the H100, all of these values"
    },
    {
      "start": 343.44,
      "duration": 4.88,
      "text": "are stored offchip. So, they're stored"
    },
    {
      "start": 345.44,
      "duration": 5.28,
      "text": "in an offchip memory. And so, as you can"
    },
    {
      "start": 348.32,
      "duration": 4.719,
      "text": "imagine, during inference, each of these"
    },
    {
      "start": 350.72,
      "duration": 4.88,
      "text": "cores, there's thousands of computations"
    },
    {
      "start": 353.039,
      "duration": 5.041,
      "text": "happening constantly. And each core is"
    },
    {
      "start": 355.6,
      "duration": 5.68,
      "text": "needing to constantly load and offload"
    },
    {
      "start": 358.08,
      "duration": 6,
      "text": "the KV cache, activation, weights, etc."
    },
    {
      "start": 361.28,
      "duration": 4.72,
      "text": "from an off-memory location. And as you"
    },
    {
      "start": 364.08,
      "duration": 5.44,
      "text": "can imagine this creates a very"
    },
    {
      "start": 366,
      "duration": 5.84,
      "text": "significant memory channel um memory"
    },
    {
      "start": 369.52,
      "duration": 5.04,
      "text": "bandwidth bottleneck."
    },
    {
      "start": 371.84,
      "duration": 5.44,
      "text": "What Cerebrris has done instead is that"
    },
    {
      "start": 374.56,
      "duration": 6.32,
      "text": "instead of storing all these values off"
    },
    {
      "start": 377.28,
      "duration": 6.639,
      "text": "chip every single core on the Cerebrus"
    },
    {
      "start": 380.88,
      "duration": 4.879,
      "text": "hardware the WSC3 there's 900,000 cores"
    },
    {
      "start": 383.919,
      "duration": 5.921,
      "text": "which in comparison to 17,000 is already"
    },
    {
      "start": 385.759,
      "duration": 7.201,
      "text": "a lot larger. Um every single core has"
    },
    {
      "start": 389.84,
      "duration": 5.919,
      "text": "direct its own direct onchip memory. So"
    },
    {
      "start": 392.96,
      "duration": 4.64,
      "text": "its own SRAMM. So every single core on"
    },
    {
      "start": 395.759,
      "duration": 4.321,
      "text": "this wafer has a memory right next to"
    },
    {
      "start": 397.6,
      "duration": 4.64,
      "text": "it. And what that means is that all of"
    },
    {
      "start": 400.08,
      "duration": 4.959,
      "text": "the values that every single core needs"
    },
    {
      "start": 402.24,
      "duration": 5.2,
      "text": "for computations like weights, KB cache,"
    },
    {
      "start": 405.039,
      "duration": 4,
      "text": "etc. is directly accessible and much"
    },
    {
      "start": 407.44,
      "duration": 3.52,
      "text": "faster to accessible and it's right"
    },
    {
      "start": 409.039,
      "duration": 4.321,
      "text": "there."
    },
    {
      "start": 410.96,
      "duration": 3.76,
      "text": "And so as you the other and so that's a"
    },
    {
      "start": 413.36,
      "duration": 3.279,
      "text": "little bit that's one example of what"
    },
    {
      "start": 414.72,
      "duration": 3.919,
      "text": "Cerebrus has done on the hardware side."
    },
    {
      "start": 416.639,
      "duration": 3.12,
      "text": "Um, but going back to software, I also"
    },
    {
      "start": 418.639,
      "duration": 2.481,
      "text": "want to talk about really quickly one"
    },
    {
      "start": 419.759,
      "duration": 4.641,
      "text": "thing that Cerebrus implements on the"
    },
    {
      "start": 421.12,
      "duration": 5.44,
      "text": "software side to accelerate inference."
    },
    {
      "start": 424.4,
      "duration": 3.919,
      "text": "And so one way that you can accelerate"
    },
    {
      "start": 426.56,
      "duration": 4.56,
      "text": "inference is through a technique called"
    },
    {
      "start": 428.319,
      "duration": 5.681,
      "text": "spec um standard decode or speculative"
    },
    {
      "start": 431.12,
      "duration": 4.72,
      "text": "decoding. So in standard decoding you"
    },
    {
      "start": 434,
      "duration": 3.68,
      "text": "have one model generate every single"
    },
    {
      "start": 435.84,
      "duration": 3.04,
      "text": "token one at a time. And this is"
    },
    {
      "start": 437.68,
      "duration": 2.88,
      "text": "sequential, right? You have to wait for"
    },
    {
      "start": 438.88,
      "duration": 3.92,
      "text": "the previous token to be generated to"
    },
    {
      "start": 440.56,
      "duration": 5.759,
      "text": "generate the next token."
    },
    {
      "start": 442.8,
      "duration": 6.56,
      "text": "So in speculative decoding, you combine"
    },
    {
      "start": 446.319,
      "duration": 5.681,
      "text": "two models. And what you're doing is you"
    },
    {
      "start": 449.36,
      "duration": 4.72,
      "text": "use a smaller model that's like a draft"
    },
    {
      "start": 452,
      "duration": 4.479,
      "text": "model that can generate all of the"
    },
    {
      "start": 454.08,
      "duration": 5.119,
      "text": "tokens very quickly. And then you use"
    },
    {
      "start": 456.479,
      "duration": 5.44,
      "text": "your larger model to go back and verify"
    },
    {
      "start": 459.199,
      "duration": 4.881,
      "text": "that the output of the smaller model is"
    },
    {
      "start": 461.919,
      "duration": 4.56,
      "text": "correct. And by combining these two"
    },
    {
      "start": 464.08,
      "duration": 4.8,
      "text": "models, you're able to get the speed of"
    },
    {
      "start": 466.479,
      "duration": 4.641,
      "text": "the smaller model and the accuracy of"
    },
    {
      "start": 468.88,
      "duration": 5.999,
      "text": "the larger model. And if you think about"
    },
    {
      "start": 471.12,
      "duration": 8.32,
      "text": "it, your speed is capped by this uh your"
    },
    {
      "start": 474.879,
      "duration": 6.401,
      "text": "like this the speed um is capped by the"
    },
    {
      "start": 479.44,
      "duration": 3.439,
      "text": "speed of the larger model. So you will"
    },
    {
      "start": 481.28,
      "duration": 4,
      "text": "up to the large like the speed will be"
    },
    {
      "start": 482.879,
      "duration": 3.921,
      "text": "up to the larger model um but it will"
    },
    {
      "start": 485.28,
      "duration": 5.039,
      "text": "never go beyond it. So it will only be"
    },
    {
      "start": 486.8,
      "duration": 5.92,
      "text": "ever be faster."
    },
    {
      "start": 490.319,
      "duration": 3.6,
      "text": "So as a kind of a short recap, hardware,"
    },
    {
      "start": 492.72,
      "duration": 3.84,
      "text": "memory, bandwidth, we talked through"
    },
    {
      "start": 493.919,
      "duration": 5.12,
      "text": "that software, specular decoding, but"
    },
    {
      "start": 496.56,
      "duration": 4.56,
      "text": "that was a little side moment and I want"
    },
    {
      "start": 499.039,
      "duration": 3.84,
      "text": "to go"
    },
    {
      "start": 501.12,
      "duration": 4.16,
      "text": "and now back to the workshop. Now that"
    },
    {
      "start": 502.879,
      "duration": 3.521,
      "text": "you have all the context that you need."
    },
    {
      "start": 505.28,
      "duration": 4.08,
      "text": ">> Awesome job."
    },
    {
      "start": 506.4,
      "duration": 4.72,
      "text": ">> Yeah, thanks Sarah. Um, for those who"
    },
    {
      "start": 509.36,
      "duration": 3.359,
      "text": "folks who join in late, you guys can"
    },
    {
      "start": 511.12,
      "duration": 4.799,
      "text": "scan the QR code to get the starter"
    },
    {
      "start": 512.719,
      "duration": 4.721,
      "text": "code. We had it in the early slide, but"
    },
    {
      "start": 515.919,
      "duration": 3.12,
      "text": "um since we'll be teaching you guys how"
    },
    {
      "start": 517.44,
      "duration": 4.88,
      "text": "to build these sales agents, you can"
    },
    {
      "start": 519.039,
      "duration": 5.041,
      "text": "follow along with our code. Um yeah, so"
    },
    {
      "start": 522.32,
      "duration": 3.92,
      "text": "I think in the future, most customer"
    },
    {
      "start": 524.08,
      "duration": 3.759,
      "text": "interactions will probably be AI"
    },
    {
      "start": 526.24,
      "duration": 4.08,
      "text": "powered, but you know, instead of just"
    },
    {
      "start": 527.839,
      "duration": 4.401,
      "text": "typing back and forth with the chatbot,"
    },
    {
      "start": 530.32,
      "duration": 3.68,
      "text": "what the best way to kind of really have"
    },
    {
      "start": 532.24,
      "duration": 4.56,
      "text": "these customer interactions is really"
    },
    {
      "start": 534,
      "duration": 6.48,
      "text": "through real conversations, which is why"
    },
    {
      "start": 536.8,
      "duration": 6.08,
      "text": "voice agents are so powerful."
    },
    {
      "start": 540.48,
      "duration": 4.16,
      "text": "So before we dive deep into it, what"
    },
    {
      "start": 542.88,
      "duration": 4.48,
      "text": "exactly is a voice agent?"
    },
    {
      "start": 544.64,
      "duration": 4.8,
      "text": ">> Absolutely. Um so voice agents are"
    },
    {
      "start": 547.36,
      "duration": 3.919,
      "text": "stateful intelligent systems that can"
    },
    {
      "start": 549.44,
      "duration": 3.68,
      "text": "simultaneously run inference while"
    },
    {
      "start": 551.279,
      "duration": 4.161,
      "text": "constantly listening to you when you're"
    },
    {
      "start": 553.12,
      "duration": 4.96,
      "text": "speaking and they can actually engage in"
    },
    {
      "start": 555.44,
      "duration": 4.88,
      "text": "real and very natural conversations. Um"
    },
    {
      "start": 558.08,
      "duration": 5.52,
      "text": "I would like to highlight four key uh"
    },
    {
      "start": 560.32,
      "duration": 5.84,
      "text": "capabilities. First, they understand and"
    },
    {
      "start": 563.6,
      "duration": 4.4,
      "text": "respond to spoken language. um they"
    },
    {
      "start": 566.16,
      "duration": 4,
      "text": "don't just spit out answers based on"
    },
    {
      "start": 568,
      "duration": 3.92,
      "text": "string matching or keywords but rather"
    },
    {
      "start": 570.16,
      "duration": 4.4,
      "text": "they can actually understand the meaning"
    },
    {
      "start": 571.92,
      "duration": 4.56,
      "text": "behind what people are saying. Um this"
    },
    {
      "start": 574.56,
      "duration": 4.64,
      "text": "also means that they can handle a lot of"
    },
    {
      "start": 576.48,
      "duration": 4.88,
      "text": "complex tasks. So someone might ask like"
    },
    {
      "start": 579.2,
      "duration": 4.319,
      "text": "I'm looking for a product recommendation"
    },
    {
      "start": 581.36,
      "duration": 4.56,
      "text": "and the agent can subsequently kind of"
    },
    {
      "start": 583.519,
      "duration": 4.481,
      "text": "look into the users's purchase history,"
    },
    {
      "start": 585.92,
      "duration": 3.599,
      "text": "the shops's current stock levels and"
    },
    {
      "start": 588,
      "duration": 3.519,
      "text": "recommend something that they actually"
    },
    {
      "start": 589.519,
      "duration": 3.841,
      "text": "like. And you actually might see this"
    },
    {
      "start": 591.519,
      "duration": 5.521,
      "text": "referred in some places called multi-"
    },
    {
      "start": 593.36,
      "duration": 5.52,
      "text": "aent or workflows. Um speech is"
    },
    {
      "start": 597.04,
      "duration": 3.919,
      "text": "obviously the fastest way to communicate"
    },
    {
      "start": 598.88,
      "duration": 4.079,
      "text": "your intent in any system. We're"
    },
    {
      "start": 600.959,
      "duration": 3.921,
      "text": "speaking now I guess [laughter] but you"
    },
    {
      "start": 602.959,
      "duration": 3.841,
      "text": "can just say what you want. There's like"
    },
    {
      "start": 604.88,
      "duration": 4.56,
      "text": "no typing, no clicking through menus and"
    },
    {
      "start": 606.8,
      "duration": 4.4,
      "text": "no learning learning curves. And lastly"
    },
    {
      "start": 609.44,
      "duration": 3.76,
      "text": "um none of this would be possible unless"
    },
    {
      "start": 611.2,
      "duration": 4.4,
      "text": "the agent can keep track of the state of"
    },
    {
      "start": 613.2,
      "duration": 4.319,
      "text": "the conversation. uh which means the"
    },
    {
      "start": 615.6,
      "duration": 3.84,
      "text": "communications obviously is very highly"
    },
    {
      "start": 617.519,
      "duration": 3.601,
      "text": "contextual and your agents needs to have"
    },
    {
      "start": 619.44,
      "duration": 5.44,
      "text": "like state so they can actually hold a"
    },
    {
      "start": 621.12,
      "duration": 6.32,
      "text": "coherent conversation across time."
    },
    {
      "start": 624.88,
      "duration": 4.48,
      "text": "So as you can imagine this makes um"
    },
    {
      "start": 627.44,
      "duration": 3.68,
      "text": "voice agents perfect. You see a lot of"
    },
    {
      "start": 629.36,
      "duration": 4.32,
      "text": "startups happening right now especially"
    },
    {
      "start": 631.12,
      "duration": 4.32,
      "text": "in customer service, sales, tech support"
    },
    {
      "start": 633.68,
      "duration": 5.68,
      "text": "etc. And so today we're going to be"
    },
    {
      "start": 635.44,
      "duration": 5.519,
      "text": "focusing on the sales agent use case."
    },
    {
      "start": 639.36,
      "duration": 3.52,
      "text": "So, first let's talk about what's"
    },
    {
      "start": 640.959,
      "duration": 3.521,
      "text": "actually happening inside a voice agent"
    },
    {
      "start": 642.88,
      "duration": 3.92,
      "text": "when you're having a conversation and"
    },
    {
      "start": 644.48,
      "duration": 4.08,
      "text": "break it down."
    },
    {
      "start": 646.8,
      "duration": 4.24,
      "text": ">> Yeah. So, you guys can see on this"
    },
    {
      "start": 648.56,
      "duration": 5.44,
      "text": "diagram on the right, once speech is"
    },
    {
      "start": 651.04,
      "duration": 6,
      "text": "detected, the voice data is forwarded to"
    },
    {
      "start": 654,
      "duration": 5.2,
      "text": "ST or that's called speech to text. This"
    },
    {
      "start": 657.04,
      "duration": 4.56,
      "text": "listens and converts to your your words"
    },
    {
      "start": 659.2,
      "duration": 5.68,
      "text": "to text in real time. And the last step"
    },
    {
      "start": 661.6,
      "duration": 6,
      "text": "in this process is end of utterance um"
    },
    {
      "start": 664.88,
      "duration": 4.8,
      "text": "or end of turn detection. um being"
    },
    {
      "start": 667.6,
      "duration": 4.799,
      "text": "interrupted by AI every time you pause."
    },
    {
      "start": 669.68,
      "duration": 4.719,
      "text": "It's like very annoying. So, while VAD"
    },
    {
      "start": 672.399,
      "duration": 3.601,
      "text": "can help the system know when you are"
    },
    {
      "start": 674.399,
      "duration": 3.12,
      "text": "and you aren't speaking, it's also very"
    },
    {
      "start": 676,
      "duration": 3.68,
      "text": "important to analyze like what you're"
    },
    {
      "start": 677.519,
      "duration": 3.841,
      "text": "saying, the context of your speech, and"
    },
    {
      "start": 679.68,
      "duration": 3.44,
      "text": "to predict like whether you've done"
    },
    {
      "start": 681.36,
      "duration": 3.84,
      "text": "sharing your thoughts. So, we have"
    },
    {
      "start": 683.12,
      "duration": 4.32,
      "text": "another small smaller model here that"
    },
    {
      "start": 685.2,
      "duration": 4.639,
      "text": "runs quickly on the CPU, which will"
    },
    {
      "start": 687.44,
      "duration": 4.399,
      "text": "instruct the system to wait if it"
    },
    {
      "start": 689.839,
      "duration": 3.68,
      "text": "predicts you're still speaking. So, once"
    },
    {
      "start": 691.839,
      "duration": 3.44,
      "text": "your turn is done, the final text"
    },
    {
      "start": 693.519,
      "duration": 4.32,
      "text": "transcription is forwarded to the next"
    },
    {
      "start": 695.279,
      "duration": 2.56,
      "text": "layer."
    },
    {
      "start": 698.56,
      "duration": 4.88,
      "text": "And then after that phase, we have the"
    },
    {
      "start": 701.12,
      "duration": 4.56,
      "text": "thinking phase. So your entire question"
    },
    {
      "start": 703.44,
      "duration": 4.16,
      "text": "is now passed onto the large language"
    },
    {
      "start": 705.68,
      "duration": 3.839,
      "text": "model. Um, and this is basically, you"
    },
    {
      "start": 707.6,
      "duration": 3.919,
      "text": "know, the brain like understands what"
    },
    {
      "start": 709.519,
      "duration": 3.361,
      "text": "you're asking. So it might need to look"
    },
    {
      "start": 711.519,
      "duration": 3.361,
      "text": "things up, which we'll walk through"
    },
    {
      "start": 712.88,
      "duration": 3.44,
      "text": "later. Um, like checking in this case,"
    },
    {
      "start": 714.88,
      "duration": 3.36,
      "text": "if we're doing a sales call, we'll want"
    },
    {
      "start": 716.32,
      "duration": 4.16,
      "text": "to pull additional context like"
    },
    {
      "start": 718.24,
      "duration": 3.44,
      "text": "documents, your other like more"
    },
    {
      "start": 720.48,
      "duration": 3.84,
      "text": "information about your company"
    },
    {
      "start": 721.68,
      "duration": 4.32,
      "text": "basically."
    },
    {
      "start": 724.32,
      "duration": 4.16,
      "text": ">> Yeah. And then the third and the final"
    },
    {
      "start": 726,
      "duration": 4.32,
      "text": "step is the speaking phase. So as LM"
    },
    {
      "start": 728.48,
      "duration": 3.52,
      "text": "streams response back to the agent, the"
    },
    {
      "start": 730.32,
      "duration": 4.16,
      "text": "agent will immediately starts forwarding"
    },
    {
      "start": 732,
      "duration": 5.6,
      "text": "these LLM tokens to the TTS engine or"
    },
    {
      "start": 734.48,
      "duration": 5.039,
      "text": "text to speech. Um this generated um"
    },
    {
      "start": 737.6,
      "duration": 3.919,
      "text": "audio from TTS streams back to your"
    },
    {
      "start": 739.519,
      "duration": 3.521,
      "text": "client's application in real time and"
    },
    {
      "start": 741.519,
      "duration": 4.801,
      "text": "that's why the agent can actually start"
    },
    {
      "start": 743.04,
      "duration": 5.28,
      "text": "responding when it's still thinking."
    },
    {
      "start": 746.32,
      "duration": 3.92,
      "text": "So the final result is that all of these"
    },
    {
      "start": 748.32,
      "duration": 3.759,
      "text": "components tied together is what's"
    },
    {
      "start": 750.24,
      "duration": 3.44,
      "text": "making, you know, an AI agent that feels"
    },
    {
      "start": 752.079,
      "duration": 4,
      "text": "very responsive, that feels very"
    },
    {
      "start": 753.68,
      "duration": 4.159,
      "text": "cohesive and immediate, even though"
    },
    {
      "start": 756.079,
      "duration": 3.841,
      "text": "there's a lot of complex processing"
    },
    {
      "start": 757.839,
      "duration": 3.761,
      "text": "happening behind the scenes. So there's"
    },
    {
      "start": 759.92,
      "duration": 3.359,
      "text": "a lot of moving pieces. In this case,"
    },
    {
      "start": 761.6,
      "duration": 4.239,
      "text": "we're going to be using LiveKit's agent"
    },
    {
      "start": 763.279,
      "duration": 4.8,
      "text": "SDK to handle all this orchestration for"
    },
    {
      "start": 765.839,
      "duration": 4.641,
      "text": "us. Um, it's going to manage the audio"
    },
    {
      "start": 768.079,
      "duration": 3.921,
      "text": "streams, keep track of the context, and"
    },
    {
      "start": 770.48,
      "duration": 4.479,
      "text": "coordinate all these different AI"
    },
    {
      "start": 772,
      "duration": 4.48,
      "text": "services that we've just talked about."
    },
    {
      "start": 774.959,
      "duration": 3.281,
      "text": "So, now that we have a little bit of"
    },
    {
      "start": 776.48,
      "duration": 3.919,
      "text": "context, um you can access the starter"
    },
    {
      "start": 778.24,
      "duration": 4.08,
      "text": "code here. We shared it already. And if"
    },
    {
      "start": 780.399,
      "duration": 4.081,
      "text": "you want to run the first section right"
    },
    {
      "start": 782.32,
      "duration": 4.4,
      "text": "here, it'll allow you to install all of"
    },
    {
      "start": 784.48,
      "duration": 4.159,
      "text": "the necessary packages. So, if you click"
    },
    {
      "start": 786.72,
      "duration": 3.84,
      "text": "on it, um you'll be able to see some of"
    },
    {
      "start": 788.639,
      "duration": 3.921,
      "text": "the output of the packages being"
    },
    {
      "start": 790.56,
      "duration": 3.76,
      "text": "downloaded. And so, this is going to use"
    },
    {
      "start": 792.56,
      "duration": 4.24,
      "text": "live kit agents with support for"
    },
    {
      "start": 794.32,
      "duration": 6.88,
      "text": "Cartisia, Cilero for voice activity"
    },
    {
      "start": 796.8,
      "duration": 6.479,
      "text": "detection, and openAI compatibility."
    },
    {
      "start": 801.2,
      "duration": 4.56,
      "text": "And so we've very briefly talked about"
    },
    {
      "start": 803.279,
      "duration": 5.921,
      "text": "Cerebras. It is 50 times faster than"
    },
    {
      "start": 805.76,
      "duration": 5.92,
      "text": "GPUs. And"
    },
    {
      "start": 809.2,
      "duration": 6.16,
      "text": "um I'll skip here. And so as a final"
    },
    {
      "start": 811.68,
      "duration": 5.04,
      "text": "note, so for this um for this workshop,"
    },
    {
      "start": 815.36,
      "duration": 4.64,
      "text": "we're actually going to be using Llama"
    },
    {
      "start": 816.72,
      "duration": 5.44,
      "text": "3.3. And if you see in the chart on the"
    },
    {
      "start": 820,
      "duration": 3.839,
      "text": "bottom right, this is a chart from"
    },
    {
      "start": 822.16,
      "duration": 3.52,
      "text": "artificial analysis. Artificial"
    },
    {
      "start": 823.839,
      "duration": 4.161,
      "text": "analysis, if you're unfamiliar, is an"
    },
    {
      "start": 825.68,
      "duration": 4.719,
      "text": "independent benchmark that benchmarks a"
    },
    {
      "start": 828,
      "duration": 5.279,
      "text": "lot of different models, API providers,"
    },
    {
      "start": 830.399,
      "duration": 4.641,
      "text": "etc. um on intelligence, speed, latency,"
    },
    {
      "start": 833.279,
      "duration": 4.24,
      "text": "everything. And so you can see a"
    },
    {
      "start": 835.04,
      "duration": 4.88,
      "text": "comparison here of Cerebrus on the very"
    },
    {
      "start": 837.519,
      "duration": 7.081,
      "text": "left in terms of tokens per second and"
    },
    {
      "start": 839.92,
      "duration": 4.68,
      "text": "any of your other providers like Nvidia."
    },
    {
      "start": 845.199,
      "duration": 5.921,
      "text": "Awesome. Um going back to our code, um"
    },
    {
      "start": 849.279,
      "duration": 4,
      "text": "hopefully everyone has had a second to"
    },
    {
      "start": 851.12,
      "duration": 5.04,
      "text": "kind of install the packages. Um, and"
    },
    {
      "start": 853.279,
      "duration": 5.36,
      "text": "now let's also in we can also install"
    },
    {
      "start": 856.16,
      "duration": 4.16,
      "text": "the live CLI. This is optional for our"
    },
    {
      "start": 858.639,
      "duration": 3.681,
      "text": "work workshop today, but if you want to"
    },
    {
      "start": 860.32,
      "duration": 4.24,
      "text": "use live kit beyond this, um, here are"
    },
    {
      "start": 862.32,
      "duration": 4,
      "text": "the commands depending on your system."
    },
    {
      "start": 864.56,
      "duration": 4.24,
      "text": "Um, in general, we're obviously using"
    },
    {
      "start": 866.32,
      "duration": 4.72,
      "text": "Python notebook today. So, no one has to"
    },
    {
      "start": 868.8,
      "duration": 4.24,
      "text": "battle around your environment when"
    },
    {
      "start": 871.04,
      "duration": 4.159,
      "text": "we're getting started. But again, if you"
    },
    {
      "start": 873.04,
      "duration": 4.159,
      "text": "want to continuously build and deploy uh"
    },
    {
      "start": 875.199,
      "duration": 4.721,
      "text": "the voice agent, the CLI probably is the"
    },
    {
      "start": 877.199,
      "duration": 5.281,
      "text": "easy way easiest way to do it. So just"
    },
    {
      "start": 879.92,
      "duration": 4.4,
      "text": "uh type in LK app create and you can"
    },
    {
      "start": 882.48,
      "duration": 5.08,
      "text": "instantly clone a pre-built agent like"
    },
    {
      "start": 884.32,
      "duration": 3.24,
      "text": "this one."
    },
    {
      "start": 889.04,
      "duration": 5.76,
      "text": "Cool. And um let's talk a little bit"
    },
    {
      "start": 891.68,
      "duration": 5.68,
      "text": "about what exactly LifeKit is and why we"
    },
    {
      "start": 894.8,
      "duration": 5.039,
      "text": "need it for a voice agent. So the"
    },
    {
      "start": 897.36,
      "duration": 5.279,
      "text": "existing internet isn't exactly designed"
    },
    {
      "start": 899.839,
      "duration": 6.481,
      "text": "to build voice agent a uh application."
    },
    {
      "start": 902.639,
      "duration": 5.281,
      "text": "So HTTP stands for hypertext transfer"
    },
    {
      "start": 906.32,
      "duration": 3.759,
      "text": "protocol. So it was designed for"
    },
    {
      "start": 907.92,
      "duration": 3.52,
      "text": "transferring text over a network and"
    },
    {
      "start": 910.079,
      "duration": 2.88,
      "text": "obviously for what we're building we"
    },
    {
      "start": 911.44,
      "duration": 3.6,
      "text": "need to transfer voice data instead of"
    },
    {
      "start": 912.959,
      "duration": 4.641,
      "text": "just text over a network with low"
    },
    {
      "start": 915.04,
      "duration": 4.08,
      "text": "latency. Um and kit is a real-time"
    },
    {
      "start": 917.6,
      "duration": 3.76,
      "text": "infrastructure platform for doing just"
    },
    {
      "start": 919.12,
      "duration": 4.959,
      "text": "that. So instead of using HTTP actually"
    },
    {
      "start": 921.36,
      "duration": 4.56,
      "text": "uses a different protocol called web RTC"
    },
    {
      "start": 924.079,
      "duration": 4.081,
      "text": "to transport voice data between your"
    },
    {
      "start": 925.92,
      "duration": 4.56,
      "text": "client application AI model with less"
    },
    {
      "start": 928.16,
      "duration": 4.08,
      "text": "than 100 millisecond of latency anywhere"
    },
    {
      "start": 930.48,
      "duration": 3.28,
      "text": "in the world which is awesome. It's very"
    },
    {
      "start": 932.24,
      "duration": 3.599,
      "text": "resilient, handles a lot of concurrent"
    },
    {
      "start": 933.76,
      "duration": 3.84,
      "text": "sessions and it's fully open source. So"
    },
    {
      "start": 935.839,
      "duration": 3.44,
      "text": "you can kind of dig into the code and"
    },
    {
      "start": 937.6,
      "duration": 5.039,
      "text": "you can see how it works or even host"
    },
    {
      "start": 939.279,
      "duration": 4.881,
      "text": "infrastructure yourself as well."
    },
    {
      "start": 942.639,
      "duration": 3.2,
      "text": "Um"
    },
    {
      "start": 944.16,
      "duration": 3.6,
      "text": "yeah, so you can use live kit to build"
    },
    {
      "start": 945.839,
      "duration": 3.521,
      "text": "any of type of like voice agents, the"
    },
    {
      "start": 947.76,
      "duration": 2.96,
      "text": "ones that can join your meetings, the"
    },
    {
      "start": 949.36,
      "duration": 3.839,
      "text": "ones you're answering phone calls and"
    },
    {
      "start": 950.72,
      "duration": 4.16,
      "text": "sell centers and call centers and in our"
    },
    {
      "start": 953.199,
      "duration": 3.601,
      "text": "case today an agent that can speak to"
    },
    {
      "start": 954.88,
      "duration": 4.56,
      "text": "prospective customers on your website on"
    },
    {
      "start": 956.8,
      "duration": 4.24,
      "text": "your behalf. And here you can see"
    },
    {
      "start": 959.44,
      "duration": 4.24,
      "text": "connecting it to the original diagram"
    },
    {
      "start": 961.04,
      "duration": 5.039,
      "text": "that we showed. So you see like the LLM,"
    },
    {
      "start": 963.68,
      "duration": 4.399,
      "text": "TTS, ST and all the AI components that"
    },
    {
      "start": 966.079,
      "duration": 3.521,
      "text": "we talked about earlier. And now you can"
    },
    {
      "start": 968.079,
      "duration": 3.361,
      "text": "see, you know, how these actual tools"
    },
    {
      "start": 969.6,
      "duration": 3.599,
      "text": "like Live Kit, Tart, Cartisia, your"
    },
    {
      "start": 971.44,
      "duration": 4,
      "text": "inference provider, all of these things"
    },
    {
      "start": 973.199,
      "duration": 4.721,
      "text": "are actually playing together to help"
    },
    {
      "start": 975.44,
      "duration": 4.399,
      "text": "you create a voice agent. And so the"
    },
    {
      "start": 977.92,
      "duration": 4.56,
      "text": "final component as I mentioned is the"
    },
    {
      "start": 979.839,
      "duration": 4.481,
      "text": "actual speech processing um which so in"
    },
    {
      "start": 982.48,
      "duration": 3.84,
      "text": "addition to cerebrus and lifkit and as I"
    },
    {
      "start": 984.32,
      "duration": 4,
      "text": "mentioned we'll be using cartisia to"
    },
    {
      "start": 986.32,
      "duration": 5.959,
      "text": "turn the voice into text and then at the"
    },
    {
      "start": 988.32,
      "duration": 3.959,
      "text": "end text back to voice."
    },
    {
      "start": 992.8,
      "duration": 4.479,
      "text": "So now that our API keys are set up step"
    },
    {
      "start": 995.279,
      "duration": 4.081,
      "text": "two is all about teaching our AI sales"
    },
    {
      "start": 997.279,
      "duration": 3.841,
      "text": "agent about our business. So when you"
    },
    {
      "start": 999.36,
      "duration": 3.039,
      "text": "train a new employee you have to give it"
    },
    {
      "start": 1001.12,
      "duration": 2.399,
      "text": "information and context on your"
    },
    {
      "start": 1002.399,
      "duration": 2.88,
      "text": "business. And so that's what we're going"
    },
    {
      "start": 1003.519,
      "duration": 4.721,
      "text": "to be doing now."
    },
    {
      "start": 1005.279,
      "duration": 4.8,
      "text": ">> Yeah. Um, I think the challenge a lot of"
    },
    {
      "start": 1008.24,
      "duration": 3.44,
      "text": "the times with LLMs is that they know a"
    },
    {
      "start": 1010.079,
      "duration": 3.921,
      "text": "lot about everything, but they might not"
    },
    {
      "start": 1011.68,
      "duration": 4.24,
      "text": "know many specific things or domain"
    },
    {
      "start": 1014,
      "duration": 3.279,
      "text": "things about your company. Um, and"
    },
    {
      "start": 1015.92,
      "duration": 2.8,
      "text": "they're only really as good as their"
    },
    {
      "start": 1017.279,
      "duration": 3.36,
      "text": "training set. So, if we want to respond"
    },
    {
      "start": 1018.72,
      "duration": 3.52,
      "text": "with any information that isn't common"
    },
    {
      "start": 1020.639,
      "duration": 3.44,
      "text": "public knowledge, we should really try"
    },
    {
      "start": 1022.24,
      "duration": 3.52,
      "text": "and load it into the LLM's context to"
    },
    {
      "start": 1024.079,
      "duration": 3.36,
      "text": "minimize hallucination or any sort of"
    },
    {
      "start": 1025.76,
      "duration": 4.48,
      "text": "canned responses such as, \"I can't help"
    },
    {
      "start": 1027.439,
      "duration": 4.4,
      "text": "with that.\""
    },
    {
      "start": 1030.24,
      "duration": 3.36,
      "text": "So, in this case, we're just going to be"
    },
    {
      "start": 1031.839,
      "duration": 4,
      "text": "feeding the LLM a document with"
    },
    {
      "start": 1033.6,
      "duration": 4,
      "text": "additional information. So, for example,"
    },
    {
      "start": 1035.839,
      "duration": 3.921,
      "text": "we can load our pricing details if"
    },
    {
      "start": 1037.6,
      "duration": 3.839,
      "text": "someone asks about pricing. But we can"
    },
    {
      "start": 1039.76,
      "duration": 4.159,
      "text": "also load information like product"
    },
    {
      "start": 1041.439,
      "duration": 4.561,
      "text": "descriptions, pricing info, key um key"
    },
    {
      "start": 1043.919,
      "duration": 4.721,
      "text": "benefits. And another big thing that we"
    },
    {
      "start": 1046,
      "duration": 5.2,
      "text": "can do is write pre-written responses to"
    },
    {
      "start": 1048.64,
      "duration": 4.08,
      "text": "common objections. So, for example, if"
    },
    {
      "start": 1051.2,
      "duration": 3.52,
      "text": "it's common that someone says it's too"
    },
    {
      "start": 1052.72,
      "duration": 3.68,
      "text": "expensive, you can write a pre-written"
    },
    {
      "start": 1054.72,
      "duration": 3.68,
      "text": "message so that our agent will always"
    },
    {
      "start": 1056.4,
      "duration": 4.08,
      "text": "stay on message and it has the correct"
    },
    {
      "start": 1058.4,
      "duration": 4.32,
      "text": "context. So, if you look at the"
    },
    {
      "start": 1060.48,
      "duration": 3.84,
      "text": "notebook, you can see what that context"
    },
    {
      "start": 1062.72,
      "duration": 3.28,
      "text": "looks like in practice, right? you don't"
    },
    {
      "start": 1064.32,
      "duration": 4.719,
      "text": "have to just give it access to a full"
    },
    {
      "start": 1066,
      "duration": 4.799,
      "text": "document. Um you can see that we've in"
    },
    {
      "start": 1069.039,
      "duration": 4.401,
      "text": "um organized all the information that"
    },
    {
      "start": 1070.799,
      "duration": 4.401,
      "text": "our sales agent needs into a very simple"
    },
    {
      "start": 1073.44,
      "duration": 5.119,
      "text": "structured format for the AI to"
    },
    {
      "start": 1075.2,
      "duration": 5.76,
      "text": "understand and reference."
    },
    {
      "start": 1078.559,
      "duration": 4,
      "text": "So you can see everything that you um a"
    },
    {
      "start": 1080.96,
      "duration": 3.76,
      "text": "good salesperson would need like the"
    },
    {
      "start": 1082.559,
      "duration": 4.321,
      "text": "descriptions and then as we mentioned it"
    },
    {
      "start": 1084.72,
      "duration": 4.48,
      "text": "has these pre-written messages as well"
    },
    {
      "start": 1086.88,
      "duration": 3.919,
      "text": "so that you can control the out um the"
    },
    {
      "start": 1089.2,
      "duration": 4.56,
      "text": "behavior of your voice agent more"
    },
    {
      "start": 1090.799,
      "duration": 5.361,
      "text": "closely."
    },
    {
      "start": 1093.76,
      "duration": 4.24,
      "text": "Um, now we're off to the more exciting"
    },
    {
      "start": 1096.16,
      "duration": 3.36,
      "text": "part, even more exciting part, step"
    },
    {
      "start": 1098,
      "duration": 3.679,
      "text": "three, where we actually create our"
    },
    {
      "start": 1099.52,
      "duration": 3.519,
      "text": "sales agent. So, this is where"
    },
    {
      "start": 1101.679,
      "duration": 3.041,
      "text": "everything that we've just talked about,"
    },
    {
      "start": 1103.039,
      "duration": 4.321,
      "text": "the components, and we're going to wire"
    },
    {
      "start": 1104.72,
      "duration": 5.44,
      "text": "them all together into a working system."
    },
    {
      "start": 1107.36,
      "duration": 4.08,
      "text": "Um, and before you run anything, let's"
    },
    {
      "start": 1110.16,
      "duration": 3.84,
      "text": "actually walk through what is happening"
    },
    {
      "start": 1111.44,
      "duration": 3.92,
      "text": "in the sales agent class. So, in the"
    },
    {
      "start": 1114,
      "duration": 3.12,
      "text": "code, you can see we start by loading"
    },
    {
      "start": 1115.36,
      "duration": 3.679,
      "text": "our contacts by using the load context"
    },
    {
      "start": 1117.12,
      "duration": 4.72,
      "text": "function we defined earlier. And this"
    },
    {
      "start": 1119.039,
      "duration": 4.961,
      "text": "gives us our agent access to all the"
    },
    {
      "start": 1121.84,
      "duration": 6.68,
      "text": "product information, pricing, and"
    },
    {
      "start": 1124,
      "duration": 4.52,
      "text": "objection handlers that we set up."
    },
    {
      "start": 1130.32,
      "duration": 3.32,
      "text": "Oh, sorry."
    },
    {
      "start": 1134.4,
      "duration": 3.68,
      "text": "So, and finally, I want to look at how"
    },
    {
      "start": 1136.08,
      "duration": 3.36,
      "text": "we're implementing everything in code in"
    },
    {
      "start": 1138.08,
      "duration": 4.64,
      "text": "terms of creating the actual sales"
    },
    {
      "start": 1139.44,
      "duration": 5.76,
      "text": "agent. So the there's way more of the"
    },
    {
      "start": 1142.72,
      "duration": 4.88,
      "text": "code in the notebook, but as a high"
    },
    {
      "start": 1145.2,
      "duration": 4.08,
      "text": "level um you want to start there's kind"
    },
    {
      "start": 1147.6,
      "duration": 3.28,
      "text": "of four components. So you want to start"
    },
    {
      "start": 1149.28,
      "duration": 4.16,
      "text": "by you know telling your sales agent"
    },
    {
      "start": 1150.88,
      "duration": 5.039,
      "text": "your voice agent communicating um your"
    },
    {
      "start": 1153.44,
      "duration": 5.04,
      "text": "sales agent commun communicating by"
    },
    {
      "start": 1155.919,
      "duration": 4.481,
      "text": "voice um and give it proper rules like"
    },
    {
      "start": 1158.48,
      "duration": 3.28,
      "text": "you know don't use bullet points because"
    },
    {
      "start": 1160.4,
      "duration": 3.519,
      "text": "everything is spoken aloud. So you want"
    },
    {
      "start": 1161.76,
      "duration": 4.4,
      "text": "to do um a bit of prompting and then"
    },
    {
      "start": 1163.919,
      "duration": 4.081,
      "text": "most importantly only use information"
    },
    {
      "start": 1166.16,
      "duration": 3.44,
      "text": "from the context that you provided. So"
    },
    {
      "start": 1168,
      "duration": 3.12,
      "text": "you want to make be very careful"
    },
    {
      "start": 1169.6,
      "duration": 3.28,
      "text": "especially with voice agents that you"
    },
    {
      "start": 1171.12,
      "duration": 3.439,
      "text": "are not allowing um that you're reducing"
    },
    {
      "start": 1172.88,
      "duration": 3.919,
      "text": "the risk of hallucinations as much as"
    },
    {
      "start": 1174.559,
      "duration": 4.321,
      "text": "possible. And then the super call is"
    },
    {
      "start": 1176.799,
      "duration": 3.76,
      "text": "what's initializing our agent and passes"
    },
    {
      "start": 1178.88,
      "duration": 3.76,
      "text": "all of our configurations to the parent"
    },
    {
      "start": 1180.559,
      "duration": 5.441,
      "text": "agent. And this is setting up our agent"
    },
    {
      "start": 1182.64,
      "duration": 5.52,
      "text": "with the LMC TTS VA and all the"
    },
    {
      "start": 1186,
      "duration": 3.2,
      "text": "instructions working together. And then"
    },
    {
      "start": 1188.16,
      "duration": 2.8,
      "text": "the last thing that we're going to do is"
    },
    {
      "start": 1189.2,
      "duration": 2.96,
      "text": "we're also going to define an onenter"
    },
    {
      "start": 1190.96,
      "duration": 3.76,
      "text": "method which is what's going to start"
    },
    {
      "start": 1192.16,
      "duration": 4.32,
      "text": "the actual conversation. So, as soon as"
    },
    {
      "start": 1194.72,
      "duration": 4.319,
      "text": "someone joins the conversation with the"
    },
    {
      "start": 1196.48,
      "duration": 5.04,
      "text": "agent, instead of sitting in silence, it"
    },
    {
      "start": 1199.039,
      "duration": 3.921,
      "text": "immediately um or this is triggered as"
    },
    {
      "start": 1201.52,
      "duration": 3.6,
      "text": "soon as someone joins the conversation."
    },
    {
      "start": 1202.96,
      "duration": 3.599,
      "text": "So, instead of ever sitting in silence,"
    },
    {
      "start": 1205.12,
      "duration": 3.84,
      "text": "you're going to immediately generate"
    },
    {
      "start": 1206.559,
      "duration": 5.12,
      "text": "that grading um and the good salesperson"
    },
    {
      "start": 1208.96,
      "duration": 5.52,
      "text": "will help."
    },
    {
      "start": 1211.679,
      "duration": 4.24,
      "text": "Yeah. And then we're off to our step"
    },
    {
      "start": 1214.48,
      "duration": 4.079,
      "text": "four. We're actually launching a"
    },
    {
      "start": 1215.919,
      "duration": 5.281,
      "text": "sequence and running the agent. Um,"
    },
    {
      "start": 1218.559,
      "duration": 4.881,
      "text": "think of this entire kind of uh entry"
    },
    {
      "start": 1221.2,
      "duration": 4.24,
      "text": "point function as a start button to our"
    },
    {
      "start": 1223.44,
      "duration": 3.44,
      "text": "agent. And when someone wants to have a"
    },
    {
      "start": 1225.44,
      "duration": 3.2,
      "text": "conversation, obviously it kicks off"
    },
    {
      "start": 1226.88,
      "duration": 4.64,
      "text": "every in the gear and gets the agent"
    },
    {
      "start": 1228.64,
      "duration": 4.88,
      "text": "ready to talk. So this entry point"
    },
    {
      "start": 1231.52,
      "duration": 4.24,
      "text": "function is doing three main things. So"
    },
    {
      "start": 1233.52,
      "duration": 4.32,
      "text": "it's connecting the agent to a virtual"
    },
    {
      "start": 1235.76,
      "duration": 3.6,
      "text": "room where the conversation will happen."
    },
    {
      "start": 1237.84,
      "duration": 3.28,
      "text": "So it's like dialing into a conference"
    },
    {
      "start": 1239.36,
      "duration": 3.52,
      "text": "call. Um, then it's going to create an"
    },
    {
      "start": 1241.12,
      "duration": 3.84,
      "text": "instance of our sales agent with the"
    },
    {
      "start": 1242.88,
      "duration": 4.159,
      "text": "setup that we just configured. And so"
    },
    {
      "start": 1244.96,
      "duration": 3.68,
      "text": "finally, it's going to start a session"
    },
    {
      "start": 1247.039,
      "duration": 4.161,
      "text": "that manages the back and forth"
    },
    {
      "start": 1248.64,
      "duration": 4.64,
      "text": "conversations. And so that is it for the"
    },
    {
      "start": 1251.2,
      "duration": 4.719,
      "text": "basis or like I guess the main framework"
    },
    {
      "start": 1253.28,
      "duration": 4.72,
      "text": "for how you would set up a sales agent."
    },
    {
      "start": 1255.919,
      "duration": 3.521,
      "text": "But to make this project a little more"
    },
    {
      "start": 1258,
      "duration": 3.28,
      "text": "robust, we're actually going to talk"
    },
    {
      "start": 1259.44,
      "duration": 5.44,
      "text": "about one a few ways that you can expand"
    },
    {
      "start": 1261.28,
      "duration": 6.32,
      "text": "your sales agent. So"
    },
    {
      "start": 1264.88,
      "duration": 5.44,
      "text": "here's one example."
    },
    {
      "start": 1267.6,
      "duration": 4.8,
      "text": "Yeah. So one thing you can do um to"
    },
    {
      "start": 1270.32,
      "duration": 7.44,
      "text": "expand our single agent into a multi-"
    },
    {
      "start": 1272.4,
      "duration": 7.6,
      "text": "aent system is um to just you know if"
    },
    {
      "start": 1277.76,
      "duration": 3.68,
      "text": "someone calls asking really deep"
    },
    {
      "start": 1280,
      "duration": 3.28,
      "text": "technical questions about API"
    },
    {
      "start": 1281.44,
      "duration": 3.52,
      "text": "integrations you really want them"
    },
    {
      "start": 1283.28,
      "duration": 3.519,
      "text": "talking to your best technical person"
    },
    {
      "start": 1284.96,
      "duration": 4.32,
      "text": "and not just your spicing pricing"
    },
    {
      "start": 1286.799,
      "duration": 4.321,
      "text": "specialist. Um again all limbs have"
    },
    {
      "start": 1289.28,
      "duration": 4,
      "text": "limited context windows which means that"
    },
    {
      "start": 1291.12,
      "duration": 3.36,
      "text": "similar to people they have limits on"
    },
    {
      "start": 1293.28,
      "duration": 3.44,
      "text": "the amount of things that they can"
    },
    {
      "start": 1294.48,
      "duration": 4.079,
      "text": "actually specialize. Um and here are the"
    },
    {
      "start": 1296.72,
      "duration": 4.959,
      "text": "three other agents in addition to that"
    },
    {
      "start": 1298.559,
      "duration": 5.201,
      "text": "single agent that um the the starter co"
    },
    {
      "start": 1301.679,
      "duration": 3.921,
      "text": "has just helped you guys run. Um three"
    },
    {
      "start": 1303.76,
      "duration": 4.48,
      "text": "of the different agents that we propose"
    },
    {
      "start": 1305.6,
      "duration": 5.28,
      "text": "in this case are um greeting agents um"
    },
    {
      "start": 1308.24,
      "duration": 4.24,
      "text": "our main sales agent who qual qualifies"
    },
    {
      "start": 1310.88,
      "duration": 4,
      "text": "leads. We have a technical specialist"
    },
    {
      "start": 1312.48,
      "duration": 4.48,
      "text": "agent as you can see on the left um who"
    },
    {
      "start": 1314.88,
      "duration": 4.32,
      "text": "are obviously specialized in sol solving"
    },
    {
      "start": 1316.96,
      "duration": 4.4,
      "text": "technical issues is the intent and then"
    },
    {
      "start": 1319.2,
      "duration": 4.8,
      "text": "finally we have the pricing specialist"
    },
    {
      "start": 1321.36,
      "duration": 5.76,
      "text": "agent on the right which handles budget"
    },
    {
      "start": 1324,
      "duration": 4.24,
      "text": "ROI and also deal negotiations. So the"
    },
    {
      "start": 1327.12,
      "duration": 2.96,
      "text": "main thing that you want to think about"
    },
    {
      "start": 1328.24,
      "duration": 4.16,
      "text": "here is you know on a real sales team"
    },
    {
      "start": 1330.08,
      "duration": 3.839,
      "text": "you want or any like multi- aent system"
    },
    {
      "start": 1332.4,
      "duration": 3.44,
      "text": "you want all of your agents to be able"
    },
    {
      "start": 1333.919,
      "duration": 3.841,
      "text": "to do very different things. And so one"
    },
    {
      "start": 1335.84,
      "duration": 5.36,
      "text": "of the key things in this um"
    },
    {
      "start": 1337.76,
      "duration": 5.84,
      "text": "implementation is that we have a um is"
    },
    {
      "start": 1341.2,
      "duration": 3.92,
      "text": "that we have a handoff. So our greeting"
    },
    {
      "start": 1343.6,
      "duration": 3.04,
      "text": "agent is what figuring out what the"
    },
    {
      "start": 1345.12,
      "duration": 4.08,
      "text": "customer actually needs and then being"
    },
    {
      "start": 1346.64,
      "duration": 5.36,
      "text": "able to route to the um to the relevant"
    },
    {
      "start": 1349.2,
      "duration": 4.479,
      "text": "sub agent."
    },
    {
      "start": 1352,
      "duration": 3.2,
      "text": "And the code for all of these different"
    },
    {
      "start": 1353.679,
      "duration": 3.36,
      "text": "agents is fully fleshed out in the"
    },
    {
      "start": 1355.2,
      "duration": 3.2,
      "text": "notebook as well. And then the last"
    },
    {
      "start": 1357.039,
      "duration": 3.201,
      "text": "thing of course is you can is adding"
    },
    {
      "start": 1358.4,
      "duration": 4.24,
      "text": "tool calling. So for example when"
    },
    {
      "start": 1360.24,
      "duration": 5.439,
      "text": "someone a customer asks about technical"
    },
    {
      "start": 1362.64,
      "duration": 5.12,
      "text": "details you know we can properly route"
    },
    {
      "start": 1365.679,
      "duration": 5.281,
      "text": "and then this is also implemented as"
    },
    {
      "start": 1367.76,
      "duration": 5.44,
      "text": "well in the code notebook"
    },
    {
      "start": 1370.96,
      "duration": 5.599,
      "text": "and that is it. So thank you guys so"
    },
    {
      "start": 1373.2,
      "duration": 4.88,
      "text": "much for coming. Um all again all of the"
    },
    {
      "start": 1376.559,
      "duration": 3.12,
      "text": "notebook with all the instructions and"
    },
    {
      "start": 1378.08,
      "duration": 3.36,
      "text": "the step by step is in the notebook that"
    },
    {
      "start": 1379.679,
      "duration": 3.201,
      "text": "we're provided and have built. Um and"
    },
    {
      "start": 1381.44,
      "duration": 3.04,
      "text": "we'll be up here to answer any questions"
    },
    {
      "start": 1382.88,
      "duration": 3.115,
      "text": "that you guys might have. Thank you"
    },
    {
      "start": 1384.48,
      "duration": 3.481,
      "text": "guys."
    },
    {
      "start": 1385.995,
      "duration": 3.986,
      "text": "[applause]"
    },
    {
      "start": 1387.961,
      "duration": 2.02,
      "text": ">> [music]"
    }
  ],
  "fullText": "[music] Hi everyone, we're about to start the next session. Thank you guys so much for coming out today. Um, this is going to be a build your own sales agent workshop. So, we're going to be walking through everything you need to know to build your own voice agent. My name is Sarah Chang from Cerebras and I am excited to be joined by Genway. Um, and we are both part of the DevX team at Cerebras. >> Yeah, thanks Sarah. Um, so today we're going to walk through how to build a voice sales agent that can actually have a natural conversations with customers and our sales agents will pull product contacts from an external source to respond in real time. So, we're going to be building an AI agent that can speak, listen, and respond intelligently um to your company's sales materials. And we have the full code for you to follow along with. We have a notebook that you can scan later um to step ghost and we'll walk you through it step by step in just a moment. So, before we get started, let's go through what you will get out of this workshop. So you will get free API credits for Cerebrris livekit cartisia. You will have the quick start. We'll have again have a full code notebook for you to follow along with and at the end you will have your very own sales agent that you can hook up to your company's materials so that you can you know implement this in production. So here's the starter code that I would recommend scanning just so you can follow along. Um, again, this is what we'll be walking through step by step today. And there will be individual modules that you'll be able to just run and see some good outfits. So, I'll give you a few seconds for that. We'll have the QR code later as well, so not to worry. So, before we get started, I wanted to talk a little bit about Cerebrus and, you know, Cerebrus inferences secret sauce. So, for those of you who are unfamiliar, we are a hardware company. We are building an AI processor that is much larger and much faster than what you are probably familiar with with Nvidia GPUs. So out of curiosity, I'm wondering how many people here have heard about Cerebras hardware. Not bad. Okay. Higher than last year. Okay. Okay. So before we do go, I want to share um I want to show everyone [clears throat] the speed of what we're talking about here. So So this is just a chat. It's running on Cerebras. You can choose any. So, we can host any different model on our hardware. So, I'm going to choose an example model like a llama model. And I'm [snorts] going to give it a prompt. So, I'm going to give it a prompt that it's intentionally asking it to respond something a little longer. This go [clears throat] funny dad jokes, but make each joke a couple sentences. Sentences. And that's how fast it generates. Does anyone else have a prompt you want to try? A longer prompt. >> Amazing. There you go. So, really quickly before we get started, I know we have a lot of software geeks here, but I do want to for a second talk about hardware. And I want to talk a little bit about what hardware innovations um make such fast inference possible especially as we build a new generation of AI products. And so we're going to a little bit of a hardware segment, but one of the main secret sauces for Cerebras is that Cerebras chips do not have memory bandwidth issues. And I don't know how familiar you guys are with, you know, GPU architecture, but we're actually gonna de deep dive really quickly into how GPU architecture works and how it compares to what people are doing today. And so for context, this is the hardware that, you know, all of our inference runs on. It's the wafer scale engine 3. It is quite literally the size of a dinner plate. And this has 4 trillion transistors, 900,000 cores, and very significant amounts of onchip memory. And so this is the comparison of what our hardware looks like next to the NVIDIA GPU. So you can see some of those metrics line up. So significantly more transistors. But to actually understand what Cerebras did with their hardware that is makes it able to achieve 20x 30x f 70x faster speeds than in inference on Nvidia GPUs. We're going to actually start by taking a look at the Nvidia GPU. So this is a diagram of an H100. And if you look at the red rectangle, that is a core. And so on the H100 there's about 17,000 cores and each of these cores is the is what is actually doing all of the mathematical computations needed in training or inference or whatever computation you need to do. So every core has a subset of the computations um that is assigned. So when you run inference what are some of the types of things that a core will need access to to do its computation? it needs its weight, activations, KV cache, etc. On the H100, all of these values are stored offchip. So, they're stored in an offchip memory. And so, as you can imagine, during inference, each of these cores, there's thousands of computations happening constantly. And each core is needing to constantly load and offload the KV cache, activation, weights, etc. from an off-memory location. And as you can imagine this creates a very significant memory channel um memory bandwidth bottleneck. What Cerebrris has done instead is that instead of storing all these values off chip every single core on the Cerebrus hardware the WSC3 there's 900,000 cores which in comparison to 17,000 is already a lot larger. Um every single core has direct its own direct onchip memory. So its own SRAMM. So every single core on this wafer has a memory right next to it. And what that means is that all of the values that every single core needs for computations like weights, KB cache, etc. is directly accessible and much faster to accessible and it's right there. And so as you the other and so that's a little bit that's one example of what Cerebrus has done on the hardware side. Um, but going back to software, I also want to talk about really quickly one thing that Cerebrus implements on the software side to accelerate inference. And so one way that you can accelerate inference is through a technique called spec um standard decode or speculative decoding. So in standard decoding you have one model generate every single token one at a time. And this is sequential, right? You have to wait for the previous token to be generated to generate the next token. So in speculative decoding, you combine two models. And what you're doing is you use a smaller model that's like a draft model that can generate all of the tokens very quickly. And then you use your larger model to go back and verify that the output of the smaller model is correct. And by combining these two models, you're able to get the speed of the smaller model and the accuracy of the larger model. And if you think about it, your speed is capped by this uh your like this the speed um is capped by the speed of the larger model. So you will up to the large like the speed will be up to the larger model um but it will never go beyond it. So it will only be ever be faster. So as a kind of a short recap, hardware, memory, bandwidth, we talked through that software, specular decoding, but that was a little side moment and I want to go and now back to the workshop. Now that you have all the context that you need. >> Awesome job. >> Yeah, thanks Sarah. Um, for those who folks who join in late, you guys can scan the QR code to get the starter code. We had it in the early slide, but um since we'll be teaching you guys how to build these sales agents, you can follow along with our code. Um yeah, so I think in the future, most customer interactions will probably be AI powered, but you know, instead of just typing back and forth with the chatbot, what the best way to kind of really have these customer interactions is really through real conversations, which is why voice agents are so powerful. So before we dive deep into it, what exactly is a voice agent? >> Absolutely. Um so voice agents are stateful intelligent systems that can simultaneously run inference while constantly listening to you when you're speaking and they can actually engage in real and very natural conversations. Um I would like to highlight four key uh capabilities. First, they understand and respond to spoken language. um they don't just spit out answers based on string matching or keywords but rather they can actually understand the meaning behind what people are saying. Um this also means that they can handle a lot of complex tasks. So someone might ask like I'm looking for a product recommendation and the agent can subsequently kind of look into the users's purchase history, the shops's current stock levels and recommend something that they actually like. And you actually might see this referred in some places called multi- aent or workflows. Um speech is obviously the fastest way to communicate your intent in any system. We're speaking now I guess [laughter] but you can just say what you want. There's like no typing, no clicking through menus and no learning learning curves. And lastly um none of this would be possible unless the agent can keep track of the state of the conversation. uh which means the communications obviously is very highly contextual and your agents needs to have like state so they can actually hold a coherent conversation across time. So as you can imagine this makes um voice agents perfect. You see a lot of startups happening right now especially in customer service, sales, tech support etc. And so today we're going to be focusing on the sales agent use case. So, first let's talk about what's actually happening inside a voice agent when you're having a conversation and break it down. >> Yeah. So, you guys can see on this diagram on the right, once speech is detected, the voice data is forwarded to ST or that's called speech to text. This listens and converts to your your words to text in real time. And the last step in this process is end of utterance um or end of turn detection. um being interrupted by AI every time you pause. It's like very annoying. So, while VAD can help the system know when you are and you aren't speaking, it's also very important to analyze like what you're saying, the context of your speech, and to predict like whether you've done sharing your thoughts. So, we have another small smaller model here that runs quickly on the CPU, which will instruct the system to wait if it predicts you're still speaking. So, once your turn is done, the final text transcription is forwarded to the next layer. And then after that phase, we have the thinking phase. So your entire question is now passed onto the large language model. Um, and this is basically, you know, the brain like understands what you're asking. So it might need to look things up, which we'll walk through later. Um, like checking in this case, if we're doing a sales call, we'll want to pull additional context like documents, your other like more information about your company basically. >> Yeah. And then the third and the final step is the speaking phase. So as LM streams response back to the agent, the agent will immediately starts forwarding these LLM tokens to the TTS engine or text to speech. Um this generated um audio from TTS streams back to your client's application in real time and that's why the agent can actually start responding when it's still thinking. So the final result is that all of these components tied together is what's making, you know, an AI agent that feels very responsive, that feels very cohesive and immediate, even though there's a lot of complex processing happening behind the scenes. So there's a lot of moving pieces. In this case, we're going to be using LiveKit's agent SDK to handle all this orchestration for us. Um, it's going to manage the audio streams, keep track of the context, and coordinate all these different AI services that we've just talked about. So, now that we have a little bit of context, um you can access the starter code here. We shared it already. And if you want to run the first section right here, it'll allow you to install all of the necessary packages. So, if you click on it, um you'll be able to see some of the output of the packages being downloaded. And so, this is going to use live kit agents with support for Cartisia, Cilero for voice activity detection, and openAI compatibility. And so we've very briefly talked about Cerebras. It is 50 times faster than GPUs. And um I'll skip here. And so as a final note, so for this um for this workshop, we're actually going to be using Llama 3.3. And if you see in the chart on the bottom right, this is a chart from artificial analysis. Artificial analysis, if you're unfamiliar, is an independent benchmark that benchmarks a lot of different models, API providers, etc. um on intelligence, speed, latency, everything. And so you can see a comparison here of Cerebrus on the very left in terms of tokens per second and any of your other providers like Nvidia. Awesome. Um going back to our code, um hopefully everyone has had a second to kind of install the packages. Um, and now let's also in we can also install the live CLI. This is optional for our work workshop today, but if you want to use live kit beyond this, um, here are the commands depending on your system. Um, in general, we're obviously using Python notebook today. So, no one has to battle around your environment when we're getting started. But again, if you want to continuously build and deploy uh the voice agent, the CLI probably is the easy way easiest way to do it. So just uh type in LK app create and you can instantly clone a pre-built agent like this one. Cool. And um let's talk a little bit about what exactly LifeKit is and why we need it for a voice agent. So the existing internet isn't exactly designed to build voice agent a uh application. So HTTP stands for hypertext transfer protocol. So it was designed for transferring text over a network and obviously for what we're building we need to transfer voice data instead of just text over a network with low latency. Um and kit is a real-time infrastructure platform for doing just that. So instead of using HTTP actually uses a different protocol called web RTC to transport voice data between your client application AI model with less than 100 millisecond of latency anywhere in the world which is awesome. It's very resilient, handles a lot of concurrent sessions and it's fully open source. So you can kind of dig into the code and you can see how it works or even host infrastructure yourself as well. Um yeah, so you can use live kit to build any of type of like voice agents, the ones that can join your meetings, the ones you're answering phone calls and sell centers and call centers and in our case today an agent that can speak to prospective customers on your website on your behalf. And here you can see connecting it to the original diagram that we showed. So you see like the LLM, TTS, ST and all the AI components that we talked about earlier. And now you can see, you know, how these actual tools like Live Kit, Tart, Cartisia, your inference provider, all of these things are actually playing together to help you create a voice agent. And so the final component as I mentioned is the actual speech processing um which so in addition to cerebrus and lifkit and as I mentioned we'll be using cartisia to turn the voice into text and then at the end text back to voice. So now that our API keys are set up step two is all about teaching our AI sales agent about our business. So when you train a new employee you have to give it information and context on your business. And so that's what we're going to be doing now. >> Yeah. Um, I think the challenge a lot of the times with LLMs is that they know a lot about everything, but they might not know many specific things or domain things about your company. Um, and they're only really as good as their training set. So, if we want to respond with any information that isn't common public knowledge, we should really try and load it into the LLM's context to minimize hallucination or any sort of canned responses such as, \"I can't help with that.\" So, in this case, we're just going to be feeding the LLM a document with additional information. So, for example, we can load our pricing details if someone asks about pricing. But we can also load information like product descriptions, pricing info, key um key benefits. And another big thing that we can do is write pre-written responses to common objections. So, for example, if it's common that someone says it's too expensive, you can write a pre-written message so that our agent will always stay on message and it has the correct context. So, if you look at the notebook, you can see what that context looks like in practice, right? you don't have to just give it access to a full document. Um you can see that we've in um organized all the information that our sales agent needs into a very simple structured format for the AI to understand and reference. So you can see everything that you um a good salesperson would need like the descriptions and then as we mentioned it has these pre-written messages as well so that you can control the out um the behavior of your voice agent more closely. Um, now we're off to the more exciting part, even more exciting part, step three, where we actually create our sales agent. So, this is where everything that we've just talked about, the components, and we're going to wire them all together into a working system. Um, and before you run anything, let's actually walk through what is happening in the sales agent class. So, in the code, you can see we start by loading our contacts by using the load context function we defined earlier. And this gives us our agent access to all the product information, pricing, and objection handlers that we set up. Oh, sorry. So, and finally, I want to look at how we're implementing everything in code in terms of creating the actual sales agent. So the there's way more of the code in the notebook, but as a high level um you want to start there's kind of four components. So you want to start by you know telling your sales agent your voice agent communicating um your sales agent commun communicating by voice um and give it proper rules like you know don't use bullet points because everything is spoken aloud. So you want to do um a bit of prompting and then most importantly only use information from the context that you provided. So you want to make be very careful especially with voice agents that you are not allowing um that you're reducing the risk of hallucinations as much as possible. And then the super call is what's initializing our agent and passes all of our configurations to the parent agent. And this is setting up our agent with the LMC TTS VA and all the instructions working together. And then the last thing that we're going to do is we're also going to define an onenter method which is what's going to start the actual conversation. So, as soon as someone joins the conversation with the agent, instead of sitting in silence, it immediately um or this is triggered as soon as someone joins the conversation. So, instead of ever sitting in silence, you're going to immediately generate that grading um and the good salesperson will help. Yeah. And then we're off to our step four. We're actually launching a sequence and running the agent. Um, think of this entire kind of uh entry point function as a start button to our agent. And when someone wants to have a conversation, obviously it kicks off every in the gear and gets the agent ready to talk. So this entry point function is doing three main things. So it's connecting the agent to a virtual room where the conversation will happen. So it's like dialing into a conference call. Um, then it's going to create an instance of our sales agent with the setup that we just configured. And so finally, it's going to start a session that manages the back and forth conversations. And so that is it for the basis or like I guess the main framework for how you would set up a sales agent. But to make this project a little more robust, we're actually going to talk about one a few ways that you can expand your sales agent. So here's one example. Yeah. So one thing you can do um to expand our single agent into a multi- aent system is um to just you know if someone calls asking really deep technical questions about API integrations you really want them talking to your best technical person and not just your spicing pricing specialist. Um again all limbs have limited context windows which means that similar to people they have limits on the amount of things that they can actually specialize. Um and here are the three other agents in addition to that single agent that um the the starter co has just helped you guys run. Um three of the different agents that we propose in this case are um greeting agents um our main sales agent who qual qualifies leads. We have a technical specialist agent as you can see on the left um who are obviously specialized in sol solving technical issues is the intent and then finally we have the pricing specialist agent on the right which handles budget ROI and also deal negotiations. So the main thing that you want to think about here is you know on a real sales team you want or any like multi- aent system you want all of your agents to be able to do very different things. And so one of the key things in this um implementation is that we have a um is that we have a handoff. So our greeting agent is what figuring out what the customer actually needs and then being able to route to the um to the relevant sub agent. And the code for all of these different agents is fully fleshed out in the notebook as well. And then the last thing of course is you can is adding tool calling. So for example when someone a customer asks about technical details you know we can properly route and then this is also implemented as well in the code notebook and that is it. So thank you guys so much for coming. Um all again all of the notebook with all the instructions and the step by step is in the notebook that we're provided and have built. Um and we'll be up here to answer any questions that you guys might have. Thank you guys. [applause] >> [music]",
  "fetchedAt": "2026-01-18T18:34:03.993Z"
}