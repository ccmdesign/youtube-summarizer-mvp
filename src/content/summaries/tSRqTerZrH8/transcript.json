{
  "videoId": "tSRqTerZrH8",
  "language": "en",
  "source": "caption-extractor",
  "segments": [
    {
      "start": 0.654,
      "duration": 2.02,
      "text": "[music]"
    },
    {
      "start": 2.8,
      "duration": 7.025,
      "text": "Light space to"
    },
    {
      "start": 7.68,
      "duration": 3.999,
      "text": "wake up."
    },
    {
      "start": 9.825,
      "duration": 4.814,
      "text": "[music]"
    },
    {
      "start": 11.679,
      "duration": 4.641,
      "text": ">> We are here back uh with Nina Latutina."
    },
    {
      "start": 14.639,
      "duration": 2.241,
      "text": "Welcome from Contextual."
    },
    {
      "start": 16.32,
      "duration": 1.76,
      "text": ">> Thanks."
    },
    {
      "start": 16.88,
      "duration": 3.6,
      "text": ">> We're going to talk about the state of"
    },
    {
      "start": 18.08,
      "duration": 4.32,
      "text": "context engineering in general. But uh"
    },
    {
      "start": 20.48,
      "duration": 4.48,
      "text": "one thing I wanted to also give people a"
    },
    {
      "start": 22.4,
      "duration": 4.639,
      "text": "sense we're not here of the Nurips"
    },
    {
      "start": 24.96,
      "duration": 3.28,
      "text": "flavor discussion and all that. We were"
    },
    {
      "start": 27.039,
      "duration": 2.64,
      "text": "talking about like I was asking which"
    },
    {
      "start": 28.24,
      "duration": 3.359,
      "text": "are the best nurse after parties and you"
    },
    {
      "start": 29.679,
      "duration": 2.88,
      "text": "said Mccor and what's the other one"
    },
    {
      "start": 31.599,
      "duration": 2.401,
      "text": ">> Turing and Nvidia"
    },
    {
      "start": 32.559,
      "duration": 2.961,
      "text": ">> Turing and Nvidia where there was a"
    },
    {
      "start": 34,
      "duration": 4.559,
      "text": "really good fireside chat discussion"
    },
    {
      "start": 35.52,
      "duration": 4.879,
      "text": "which is rare because I I often tend to"
    },
    {
      "start": 38.559,
      "duration": 3.921,
      "text": "shy away from fireside chats"
    },
    {
      "start": 40.399,
      "duration": 4.16,
      "text": ">> uh but apparently the guest was Yedin uh"
    },
    {
      "start": 42.48,
      "duration": 3.84,
      "text": "Troy who is very well-known figure on"
    },
    {
      "start": 44.559,
      "duration": 3.761,
      "text": "these parts. What was it about?"
    },
    {
      "start": 46.32,
      "duration": 4.079,
      "text": ">> Yeah, it was about uh kind of the"
    },
    {
      "start": 48.32,
      "duration": 3.759,
      "text": "overall state of AI. Uh since she's at"
    },
    {
      "start": 50.399,
      "duration": 3.921,
      "text": "Nvidia, they talked about a lot about"
    },
    {
      "start": 52.079,
      "duration": 4.401,
      "text": "scaling laws and the future of AI. I"
    },
    {
      "start": 54.32,
      "duration": 3.52,
      "text": "think Jonathan Sedaris the uh CEO of"
    },
    {
      "start": 56.48,
      "duration": 4.16,
      "text": "Turring who was interviewing her was"
    },
    {
      "start": 57.84,
      "duration": 5.68,
      "text": "just uh at asking fun questions as was"
    },
    {
      "start": 60.64,
      "duration": 4.879,
      "text": "the audience. I would say uh the last"
    },
    {
      "start": 63.52,
      "duration": 4.639,
      "text": "question there was kind of perfect. uh"
    },
    {
      "start": 65.519,
      "duration": 4.96,
      "text": "so she focuses on small language models"
    },
    {
      "start": 68.159,
      "duration": 4.801,
      "text": "and uh the question was you know"
    },
    {
      "start": 70.479,
      "duration": 5.121,
      "text": "wouldn't your research cause Nvidia to"
    },
    {
      "start": 72.96,
      "duration": 6.56,
      "text": "to lose a lot of market cap [laughter]"
    },
    {
      "start": 75.6,
      "duration": 6.24,
      "text": "and in fact she had emailed her uh"
    },
    {
      "start": 79.52,
      "duration": 4.239,
      "text": "research to Jensen Wang before joining"
    },
    {
      "start": 81.84,
      "duration": 3.12,
      "text": "and he was like yeah that's great and I"
    },
    {
      "start": 83.759,
      "duration": 2.321,
      "text": "think that's fair"
    },
    {
      "start": 84.96,
      "duration": 3.199,
      "text": ">> it's really in the world right now"
    },
    {
      "start": 86.08,
      "duration": 4.32,
      "text": ">> well no but actually the implication is"
    },
    {
      "start": 88.159,
      "duration": 3.681,
      "text": "that uh if you have smaller language"
    },
    {
      "start": 90.4,
      "duration": 3.28,
      "text": "models you can do more you can have them"
    },
    {
      "start": 91.84,
      "duration": 3.52,
      "text": "run on phones and so you actually could"
    },
    {
      "start": 93.68,
      "duration": 3.92,
      "text": "end up using more compute"
    },
    {
      "start": 95.36,
      "duration": 4.64,
      "text": "to uh to power more and more small"
    },
    {
      "start": 97.6,
      "duration": 6.24,
      "text": "language models to to do more. So"
    },
    {
      "start": 100,
      "duration": 6,
      "text": ">> yeah um I would say though uh so also at"
    },
    {
      "start": 103.84,
      "duration": 4.239,
      "text": "Europe's the open router team released"
    },
    {
      "start": 106,
      "duration": 3.84,
      "text": "their state of AI survey based on open"
    },
    {
      "start": 108.079,
      "duration": 3.68,
      "text": "router and they had this very"
    },
    {
      "start": 109.84,
      "duration": 3.52,
      "text": "interesting chart on the adoption of"
    },
    {
      "start": 111.759,
      "duration": 3.761,
      "text": "small versus medium versus large"
    },
    {
      "start": 113.36,
      "duration": 4.799,
      "text": "language models. So the cutoff was small"
    },
    {
      "start": 115.52,
      "duration": 5.52,
      "text": "was less than 15 billion and medium was"
    },
    {
      "start": 118.159,
      "duration": 4.801,
      "text": "15 to 70 and large is anything above 70"
    },
    {
      "start": 121.04,
      "duration": 4.16,
      "text": "and you could see the market share of"
    },
    {
      "start": 122.96,
      "duration": 2.88,
      "text": "small models trending down over time in"
    },
    {
      "start": 125.2,
      "duration": 1.04,
      "text": "practice"
    },
    {
      "start": 125.84,
      "duration": 1.919,
      "text": ">> really"
    },
    {
      "start": 126.24,
      "duration": 3.359,
      "text": ">> and the market share of medium trending"
    },
    {
      "start": 127.759,
      "duration": 2.321,
      "text": "up and then large is staying the same."
    },
    {
      "start": 129.599,
      "duration": 2.241,
      "text": ">> Mhm."
    },
    {
      "start": 130.08,
      "duration": 2.48,
      "text": ">> Yeah. And this is open models only of"
    },
    {
      "start": 131.84,
      "duration": 2.16,
      "text": "course"
    },
    {
      "start": 132.56,
      "duration": 3.36,
      "text": ">> and obviously the closed models have"
    },
    {
      "start": 134,
      "duration": 3.36,
      "text": "gone up and up and out. So I don't I"
    },
    {
      "start": 135.92,
      "duration": 2.72,
      "text": "don't it's just a just a data point."
    },
    {
      "start": 137.36,
      "duration": 2.72,
      "text": "Yeah. Yeah, I think I think it was one"
    },
    {
      "start": 138.64,
      "duration": 3.52,
      "text": "of those things where like Apple"
    },
    {
      "start": 140.08,
      "duration": 4,
      "text": "intelligence I think is widely"
    },
    {
      "start": 142.16,
      "duration": 4.24,
      "text": "acknowledged to be a failure this year"
    },
    {
      "start": 144.08,
      "duration": 5.44,
      "text": "and they were kind of the champions of"
    },
    {
      "start": 146.4,
      "duration": 4.72,
      "text": "the small models uh on device movements"
    },
    {
      "start": 149.52,
      "duration": 4.88,
      "text": "uh that really needed to work and it did"
    },
    {
      "start": 151.12,
      "duration": 5.52,
      "text": "not work. Gemini launched uh Gemini Nano"
    },
    {
      "start": 154.4,
      "duration": 4.64,
      "text": "in Chrome this year. I haven't used it."
    },
    {
      "start": 156.64,
      "duration": 5.599,
      "text": "I don't know if y is Yeah, basically"
    },
    {
      "start": 159.04,
      "duration": 5.04,
      "text": "like the small model hope and dream I"
    },
    {
      "start": 162.239,
      "duration": 3.841,
      "text": "think it's it's still in like very small"
    },
    {
      "start": 164.08,
      "duration": 3.6,
      "text": "use cases and not rolled out and I don't"
    },
    {
      "start": 166.08,
      "duration": 2.159,
      "text": "know what needs to happen for it to roll"
    },
    {
      "start": 167.68,
      "duration": 2,
      "text": "out."
    },
    {
      "start": 168.239,
      "duration": 3.28,
      "text": ">> Yeah, I would say that makes a lot of"
    },
    {
      "start": 169.68,
      "duration": 4.4,
      "text": "sense for general purpose model. I think"
    },
    {
      "start": 171.519,
      "duration": 4.8,
      "text": "for uh other component models like let's"
    },
    {
      "start": 174.08,
      "duration": 2.72,
      "text": "say like for a reanker due to latency"
    },
    {
      "start": 176.319,
      "duration": 1.361,
      "text": "constraints"
    },
    {
      "start": 176.8,
      "duration": 2.64,
      "text": ">> of course"
    },
    {
      "start": 177.68,
      "duration": 3.919,
      "text": ">> smaller is better is what I've heard"
    },
    {
      "start": 179.44,
      "duration": 3.68,
      "text": "from other developers but I can see that"
    },
    {
      "start": 181.599,
      "duration": 3.521,
      "text": "being a trend more generally."
    },
    {
      "start": 183.12,
      "duration": 3.52,
      "text": ">> Yeah, you guys had uh a lot of reanker"
    },
    {
      "start": 185.12,
      "duration": 3.28,
      "text": "work last year. I don't know is is that"
    },
    {
      "start": 186.64,
      "duration": 2.64,
      "text": "like as much of a focus this year for"
    },
    {
      "start": 188.4,
      "duration": 2.88,
      "text": "contextual."
    },
    {
      "start": 189.28,
      "duration": 3.599,
      "text": ">> We released the first instruction"
    },
    {
      "start": 191.28,
      "duration": 2,
      "text": "following reanker in March."
    },
    {
      "start": 192.879,
      "duration": 3.121,
      "text": ">> Okay."
    },
    {
      "start": 193.28,
      "duration": 4.8,
      "text": ">> And we updated it recently. So it's"
    },
    {
      "start": 196,
      "duration": 3.599,
      "text": "something that I think we'll keep"
    },
    {
      "start": 198.08,
      "duration": 3.84,
      "text": "digging to keep up with the"
    },
    {
      "start": 199.599,
      "duration": 5.761,
      "text": "state-of-the-art because it is really a"
    },
    {
      "start": 201.92,
      "duration": 5.92,
      "text": "key function within context engineering."
    },
    {
      "start": 205.36,
      "duration": 4.72,
      "text": "So for example, when you're reasoning"
    },
    {
      "start": 207.84,
      "duration": 5.28,
      "text": "over larger and larger databases, you"
    },
    {
      "start": 210.08,
      "duration": 4.799,
      "text": "want to be able to have more recall in"
    },
    {
      "start": 213.12,
      "duration": 3.44,
      "text": "those initial retrievalss, but you want"
    },
    {
      "start": 214.879,
      "duration": 4,
      "text": "more precision for what you're actually"
    },
    {
      "start": 216.56,
      "duration": 4.239,
      "text": "putting into the context window for fear"
    },
    {
      "start": 218.879,
      "duration": 4.241,
      "text": "of context rod and, you know, poor"
    },
    {
      "start": 220.799,
      "duration": 4.481,
      "text": "performance. And so that reanker can"
    },
    {
      "start": 223.12,
      "duration": 4.399,
      "text": "really help you narrow down that initial"
    },
    {
      "start": 225.28,
      "duration": 4.8,
      "text": "retrieval. I did a brief comment that"
    },
    {
      "start": 227.519,
      "duration": 5.201,
      "text": "it's weird that that that was the first"
    },
    {
      "start": 230.08,
      "duration": 4.48,
      "text": "instruction following re-ranker because"
    },
    {
      "start": 232.72,
      "duration": 4,
      "text": "well don't you always wanted to be"
    },
    {
      "start": 234.56,
      "duration": 4.399,
      "text": "instruction following and uh the simple"
    },
    {
      "start": 236.72,
      "duration": 4.239,
      "text": "answer from uh that I've had I've I've"
    },
    {
      "start": 238.959,
      "duration": 4.48,
      "text": "literally asked the search firms this"
    },
    {
      "start": 240.959,
      "duration": 4.081,
      "text": "the startups like um you know those"
    },
    {
      "start": 243.439,
      "duration": 3.36,
      "text": "people who know will know and they"
    },
    {
      "start": 245.04,
      "duration": 3.44,
      "text": "they're all like well you know to be"
    },
    {
      "start": 246.799,
      "duration": 4.16,
      "text": "instruction following you have to have"
    },
    {
      "start": 248.48,
      "duration": 5.28,
      "text": "uh a larger model and that affects our"
    },
    {
      "start": 250.959,
      "duration": 5.041,
      "text": "latency budget and I'm like I don't"
    },
    {
      "start": 253.76,
      "duration": 3.92,
      "text": "think that's the case like like the"
    },
    {
      "start": 256,
      "duration": 3.28,
      "text": "latency for for smaller models are"
    },
    {
      "start": 257.68,
      "duration": 3.36,
      "text": "pretty good these days. I don't know."
    },
    {
      "start": 259.28,
      "duration": 2.96,
      "text": ">> Yeah. So, actually that is like the"
    },
    {
      "start": 261.04,
      "duration": 4.32,
      "text": "biggest complaint I've heard from"
    },
    {
      "start": 262.24,
      "duration": 5.36,
      "text": "developers about our reanker."
    },
    {
      "start": 265.36,
      "duration": 4.96,
      "text": ">> But what we're using it for increasingly"
    },
    {
      "start": 267.6,
      "duration": 4.48,
      "text": "is for dynamic agents and there you're"
    },
    {
      "start": 270.32,
      "duration": 2.96,
      "text": ">> so that's insensitive, right? Just take"
    },
    {
      "start": 272.08,
      "duration": 3.36,
      "text": "however long you want to take. Yeah,"
    },
    {
      "start": 273.28,
      "duration": 3.28,
      "text": "exactly. Yeah, that's fair. I'll get to"
    },
    {
      "start": 275.44,
      "duration": 3.759,
      "text": "right to context engineering. I think"
    },
    {
      "start": 276.56,
      "duration": 4,
      "text": "the other thing that people are there"
    },
    {
      "start": 279.199,
      "duration": 4.321,
      "text": "was a big topic of conversation this"
    },
    {
      "start": 280.56,
      "duration": 6.32,
      "text": "year was quote unquote the uh death of"
    },
    {
      "start": 283.52,
      "duration": 6.72,
      "text": "normal rag and the rise of agentic rag."
    },
    {
      "start": 286.88,
      "duration": 5.2,
      "text": "Do you agree? Uh is it o is the debate"
    },
    {
      "start": 290.24,
      "duration": 3.44,
      "text": "itself overrated because obviously use"
    },
    {
      "start": 292.08,
      "duration": 2.8,
      "text": "whatever in the right context right like"
    },
    {
      "start": 293.68,
      "duration": 2.88,
      "text": "it depends."
    },
    {
      "start": 294.88,
      "duration": 2.319,
      "text": ">> I don't know is is this a meaningful"
    },
    {
      "start": 296.56,
      "duration": 3.12,
      "text": "debate?"
    },
    {
      "start": 297.199,
      "duration": 4.72,
      "text": ">> I mean I think the debate is not so"
    },
    {
      "start": 299.68,
      "duration": 5.68,
      "text": "meaningful. I think like progress is"
    },
    {
      "start": 301.919,
      "duration": 5.201,
      "text": "meaningful but I think to me it's also"
    },
    {
      "start": 305.36,
      "duration": 3.36,
      "text": "like maybe somewhat a decided debate"
    },
    {
      "start": 307.12,
      "duration": 3.919,
      "text": "like agentic rag is just generally"
    },
    {
      "start": 308.72,
      "duration": 6.16,
      "text": "better than rag even that initial"
    },
    {
      "start": 311.039,
      "duration": 5.841,
      "text": "incremental step of making that uh doing"
    },
    {
      "start": 314.88,
      "duration": 3.84,
      "text": "query reformulation. So when you receive"
    },
    {
      "start": 316.88,
      "duration": 3.68,
      "text": "that initial query, being able to bake"
    },
    {
      "start": 318.72,
      "duration": 4.56,
      "text": "it down into subqueries so that you can"
    },
    {
      "start": 320.56,
      "duration": 4.32,
      "text": "better match those queries to documents"
    },
    {
      "start": 323.28,
      "duration": 3.12,
      "text": "that you might want to retrieve and then"
    },
    {
      "start": 324.88,
      "duration": 3.68,
      "text": "combine that for the retrieval. Even"
    },
    {
      "start": 326.4,
      "duration": 4.239,
      "text": "that step improves performance so"
    },
    {
      "start": 328.56,
      "duration": 4.16,
      "text": "dramatically that you know that's kind"
    },
    {
      "start": 330.639,
      "duration": 4,
      "text": "of that became the new baseline."
    },
    {
      "start": 332.72,
      "duration": 3.68,
      "text": ">> Yeah. Um that that is obviously so"
    },
    {
      "start": 334.639,
      "duration": 4.241,
      "text": "helpful and you can farm it out and"
    },
    {
      "start": 336.4,
      "duration": 4.48,
      "text": "parallelize and then re regroup it. Uh"
    },
    {
      "start": 338.88,
      "duration": 3.92,
      "text": "so this year the work bear I did on"
    },
    {
      "start": 340.88,
      "duration": 3.84,
      "text": "sweet grap uh did that like so we we had"
    },
    {
      "start": 342.8,
      "duration": 4.08,
      "text": "like a fast context model where it was"
    },
    {
      "start": 344.72,
      "duration": 4.24,
      "text": "trained uh I think a couple innovations"
    },
    {
      "start": 346.88,
      "duration": 4.08,
      "text": "I think was really interesting. One uh"
    },
    {
      "start": 348.96,
      "duration": 3.92,
      "text": "trained for massively or much more"
    },
    {
      "start": 350.96,
      "duration": 4.16,
      "text": "parallel than normal tool calling for"
    },
    {
      "start": 352.88,
      "duration": 4.319,
      "text": "searching. So you can sort of uh normal"
    },
    {
      "start": 355.12,
      "duration": 4.4,
      "text": "normal parallelism is like one to two"
    },
    {
      "start": 357.199,
      "duration": 4.481,
      "text": "maybe maximum four. We train the"
    },
    {
      "start": 359.52,
      "duration": 4.799,
      "text": "baseline to be six uh parallel searches"
    },
    {
      "start": 361.68,
      "duration": 4.88,
      "text": "at a time and goes up goes up to eight."
    },
    {
      "start": 364.319,
      "duration": 3.761,
      "text": "And then also limited agency that you"
    },
    {
      "start": 366.56,
      "duration": 3.84,
      "text": "don't want your agentic search to run"
    },
    {
      "start": 368.08,
      "duration": 5.119,
      "text": "forever. You you do want it to terminate"
    },
    {
      "start": 370.4,
      "duration": 5.2,
      "text": "at some point and return the answer. So"
    },
    {
      "start": 373.199,
      "duration": 5.84,
      "text": "uh incentivizing the RL to do that I"
    },
    {
      "start": 375.6,
      "duration": 4.56,
      "text": "think was helpful, easy and actually"
    },
    {
      "start": 379.039,
      "duration": 3.6,
      "text": "scaled very well."
    },
    {
      "start": 380.16,
      "duration": 6.96,
      "text": ">> Yes. And actually we have found that uh"
    },
    {
      "start": 382.639,
      "duration": 6.56,
      "text": "having turn limits and limits on the sub"
    },
    {
      "start": 387.12,
      "duration": 5.199,
      "text": "agents uh checking and validating their"
    },
    {
      "start": 389.199,
      "duration": 5.681,
      "text": "work is super important. We recently did"
    },
    {
      "start": 392.319,
      "duration": 4.16,
      "text": "uh my uh myself and a member of my team"
    },
    {
      "start": 394.88,
      "duration": 2.64,
      "text": "recently participated in a context"
    },
    {
      "start": 396.479,
      "duration": 1.761,
      "text": "engineering fund."
    },
    {
      "start": 397.52,
      "duration": 2.56,
      "text": ">> Tell us about it."
    },
    {
      "start": 398.24,
      "duration": 3.84,
      "text": ">> Yeah. So Brian Bishoff and Hamill"
    },
    {
      "start": 400.08,
      "duration": 4.88,
      "text": "Hussein hosted it in San Francisco last"
    },
    {
      "start": 402.08,
      "duration": 6.08,
      "text": "month mid November and they had about uh"
    },
    {
      "start": 404.96,
      "duration": 5.2,
      "text": "just under a 100,000 documents so PDFs"
    },
    {
      "start": 408.16,
      "duration": 5.2,
      "text": "all in the retail space. So it's called"
    },
    {
      "start": 410.16,
      "duration": 8.159,
      "text": "retail universe and they had log files"
    },
    {
      "start": 413.36,
      "duration": 7.119,
      "text": "uh tables giant CSVs files and we used"
    },
    {
      "start": 418.319,
      "duration": 4,
      "text": "our dynamic agent to answer really"
    },
    {
      "start": 420.479,
      "duration": 3.921,
      "text": "challenging queries in this and generate"
    },
    {
      "start": 422.319,
      "duration": 4.481,
      "text": "structured output and some of the very"
    },
    {
      "start": 424.4,
      "duration": 4.639,
      "text": "early steps we noticed is you know with"
    },
    {
      "start": 426.8,
      "duration": 4.239,
      "text": "a data set that large that agent will"
    },
    {
      "start": 429.039,
      "duration": 3.681,
      "text": "want to take that sub agent let's say"
    },
    {
      "start": 431.039,
      "duration": 3.761,
      "text": "the unstructured retrieval or structured"
    },
    {
      "start": 432.72,
      "duration": 3.759,
      "text": "retrieval sub agent will want to take so"
    },
    {
      "start": 434.8,
      "duration": 3.2,
      "text": "many turns to make sure they you know"
    },
    {
      "start": 436.479,
      "duration": 2.72,
      "text": "looked under every rock in the data set"
    },
    {
      "start": 438,
      "duration": 3.52,
      "text": "and we actually don't want that that"
    },
    {
      "start": 439.199,
      "duration": 4.4,
      "text": "data set is pretty large. And then also"
    },
    {
      "start": 441.52,
      "duration": 3.76,
      "text": "it turns out the sub agent would want to"
    },
    {
      "start": 443.599,
      "duration": 4.32,
      "text": "check its work over and over and over."
    },
    {
      "start": 445.28,
      "duration": 4.479,
      "text": "So um so that's kind of like something"
    },
    {
      "start": 447.919,
      "duration": 3.84,
      "text": "that we noticed with large scale. And so"
    },
    {
      "start": 449.759,
      "duration": 3.601,
      "text": "kind of similar to what you found like"
    },
    {
      "start": 451.759,
      "duration": 2.801,
      "text": "you don't want unlimited, you know,"
    },
    {
      "start": 453.36,
      "duration": 3.52,
      "text": "whether you're enforcing that within"
    },
    {
      "start": 454.56,
      "duration": 3.759,
      "text": "like the RL reward structure or with"
    },
    {
      "start": 456.88,
      "duration": 3.68,
      "text": "explicit instructions in the system"
    },
    {
      "start": 458.319,
      "duration": 3.6,
      "text": "prompt. Yeah, I would say very much this"
    },
    {
      "start": 460.56,
      "duration": 3.919,
      "text": "year is kind of the year of the sub"
    },
    {
      "start": 461.919,
      "duration": 5.361,
      "text": "agent for me in terms of like the people"
    },
    {
      "start": 464.479,
      "duration": 6,
      "text": "use using constrained agents to do very"
    },
    {
      "start": 467.28,
      "duration": 4.479,
      "text": "specific things and uh sometimes being"
    },
    {
      "start": 470.479,
      "duration": 3.44,
      "text": "too general actually is like an"
    },
    {
      "start": 471.759,
      "duration": 4,
      "text": "antiattern because well it doesn't go"
    },
    {
      "start": 473.919,
      "duration": 3.521,
      "text": "very far on its own or it's not very"
    },
    {
      "start": 475.759,
      "duration": 3.601,
      "text": "reliable or doesn't have the tools that"
    },
    {
      "start": 477.44,
      "duration": 4.8,
      "text": "it needs blah blah blah blah blah right"
    },
    {
      "start": 479.36,
      "duration": 4.8,
      "text": "uh in some makes sense"
    },
    {
      "start": 482.24,
      "duration": 3.519,
      "text": "also like you also happen to be able to"
    },
    {
      "start": 484.16,
      "duration": 3.28,
      "text": "fine-tune the model towards that"
    },
    {
      "start": 485.759,
      "duration": 4.321,
      "text": "specific task so you can also hill climb"
    },
    {
      "start": 487.44,
      "duration": 5.36,
      "text": "very easily as and and and so I think to"
    },
    {
      "start": 490.08,
      "duration": 4.64,
      "text": "me that is like all the right elements"
    },
    {
      "start": 492.8,
      "duration": 4.959,
      "text": "of AI engineering that I want to see"
    },
    {
      "start": 494.72,
      "duration": 5.039,
      "text": "people do more of and people just needed"
    },
    {
      "start": 497.759,
      "duration": 4,
      "text": "a term for it and I guess I think people"
    },
    {
      "start": 499.759,
      "duration": 3.681,
      "text": "have settled on sub agents as as a term"
    },
    {
      "start": 501.759,
      "duration": 3.361,
      "text": "just briefly staying on the hackathon"
    },
    {
      "start": 503.44,
      "duration": 3.28,
      "text": "are there any other alternative"
    },
    {
      "start": 505.12,
      "duration": 2.56,
      "text": "approaches do you found interesting that"
    },
    {
      "start": 506.72,
      "duration": 2.96,
      "text": "you want to shout out"
    },
    {
      "start": 507.68,
      "duration": 3.519,
      "text": ">> I actually have a blog I'll write uh"
    },
    {
      "start": 509.68,
      "duration": 3.599,
      "text": "sooner about our experience"
    },
    {
      "start": 511.199,
      "duration": 4.64,
      "text": ">> blog it I love it"
    },
    {
      "start": 513.279,
      "duration": 4.961,
      "text": ">> and so I think the leading team was"
    },
    {
      "start": 515.839,
      "duration": 4.32,
      "text": "using Mix Brad and Claude Second place"
    },
    {
      "start": 518.24,
      "duration": 4.159,
      "text": "was using cursor"
    },
    {
      "start": 520.159,
      "duration": 2.88,
      "text": ">> mace spread. I've uh seen that a couple"
    },
    {
      "start": 522.399,
      "duration": 2.081,
      "text": "times."
    },
    {
      "start": 523.039,
      "duration": 2.8,
      "text": ">> Yeah, I think they also have a reanker"
    },
    {
      "start": 524.48,
      "duration": 1.76,
      "text": "and some other open source models."
    },
    {
      "start": 525.839,
      "duration": 0.881,
      "text": ">> Okay."
    },
    {
      "start": 526.24,
      "duration": 1.279,
      "text": ">> Um"
    },
    {
      "start": 526.72,
      "duration": 2.32,
      "text": ">> interesting name."
    },
    {
      "start": 527.519,
      "duration": 3.44,
      "text": ">> Yeah."
    },
    {
      "start": 529.04,
      "duration": 4.08,
      "text": ">> And I forget what the other team was."
    },
    {
      "start": 530.959,
      "duration": 3.681,
      "text": "And and there was also a human doing the"
    },
    {
      "start": 533.12,
      "duration": 2.08,
      "text": "challenge and so there was a human"
    },
    {
      "start": 534.64,
      "duration": 2.08,
      "text": "benchmark."
    },
    {
      "start": 535.2,
      "duration": 4.48,
      "text": ">> Oh yeah. How did human do?"
    },
    {
      "start": 536.72,
      "duration": 4.799,
      "text": ">> I think they got 23 points. We got I"
    },
    {
      "start": 539.68,
      "duration": 3.2,
      "text": "think about 25 and the winner had about"
    },
    {
      "start": 541.519,
      "duration": 2.961,
      "text": "29. So it was like very"
    },
    {
      "start": 542.88,
      "duration": 2,
      "text": ">> Okay. Super human. Yeah."
    },
    {
      "start": 544.48,
      "duration": 2.88,
      "text": ">> Yeah."
    },
    {
      "start": 544.88,
      "duration": 4,
      "text": ">> Yeah. Yeah. Uh very cool. Okay. So let's"
    },
    {
      "start": 547.36,
      "duration": 3.599,
      "text": "get right into the meat uh which is"
    },
    {
      "start": 548.88,
      "duration": 3.92,
      "text": "context engineering. This very very big"
    },
    {
      "start": 550.959,
      "duration": 4.081,
      "text": "year for context engineering. Your"
    },
    {
      "start": 552.8,
      "duration": 4.96,
      "text": "company is contextual which like could"
    },
    {
      "start": 555.04,
      "duration": 4.4,
      "text": "not be more on the nose. How do you"
    },
    {
      "start": 557.76,
      "duration": 4.639,
      "text": "describe this year in context"
    },
    {
      "start": 559.44,
      "duration": 5.6,
      "text": "engineering? It's been a very fast year"
    },
    {
      "start": 562.399,
      "duration": 4.721,
      "text": "because convex engineering really took"
    },
    {
      "start": 565.04,
      "duration": 4.799,
      "text": "hold six months ago and that actually"
    },
    {
      "start": 567.12,
      "duration": 4.159,
      "text": "feels like a year. And so I think one"
    },
    {
      "start": 569.839,
      "duration": 4.321,
      "text": "thing that stands out to me is that"
    },
    {
      "start": 571.279,
      "duration": 5.12,
      "text": "there are a lot of design patterns that"
    },
    {
      "start": 574.16,
      "duration": 5.679,
      "text": "um are kind of bubbling up but there"
    },
    {
      "start": 576.399,
      "duration": 5.921,
      "text": "isn't like a uniform design that folks"
    },
    {
      "start": 579.839,
      "duration": 4.081,
      "text": "are using as the as the architecture."
    },
    {
      "start": 582.32,
      "duration": 3.76,
      "text": "And I think there's a lot of"
    },
    {
      "start": 583.92,
      "duration": 4.16,
      "text": "optimization and efficiencies to gain."
    },
    {
      "start": 586.08,
      "duration": 4,
      "text": "So I think with a lot of new development"
    },
    {
      "start": 588.08,
      "duration": 4.24,
      "text": "you kind of start by you know letting"
    },
    {
      "start": 590.08,
      "duration": 3.92,
      "text": "the agent use as many tokens as it wants"
    },
    {
      "start": 592.32,
      "duration": 3.92,
      "text": "and then later you figure out how to"
    },
    {
      "start": 594,
      "duration": 4.88,
      "text": "constrain that and how to optimize it uh"
    },
    {
      "start": 596.24,
      "duration": 5.599,
      "text": "let's say like with uh key value caching"
    },
    {
      "start": 598.88,
      "duration": 4.88,
      "text": "or other approaches that can help um"
    },
    {
      "start": 601.839,
      "duration": 3.921,
      "text": "really scale the technology. So I think"
    },
    {
      "start": 603.76,
      "duration": 4.16,
      "text": "it's kind of to me maybe more in a"
    },
    {
      "start": 605.76,
      "duration": 4.24,
      "text": "prototyping stage and I'm expecting next"
    },
    {
      "start": 607.92,
      "duration": 2.96,
      "text": "year we'll really see scale for context"
    },
    {
      "start": 610,
      "duration": 2.8,
      "text": "engineering."
    },
    {
      "start": 610.88,
      "duration": 3.68,
      "text": ">> What does scale look like? what will we"
    },
    {
      "start": 612.8,
      "duration": 3.52,
      "text": "be able to do end of next year that we"
    },
    {
      "start": 614.56,
      "duration": 2.719,
      "text": "cannot do this year or that we're not"
    },
    {
      "start": 616.32,
      "duration": 2.8,
      "text": "doing this year."
    },
    {
      "start": 617.279,
      "duration": 5.12,
      "text": ">> I think the kinds of tasks that we'll be"
    },
    {
      "start": 619.12,
      "duration": 5.44,
      "text": "able to solve are going to increase. So,"
    },
    {
      "start": 622.399,
      "duration": 3.788,
      "text": "uh I think for example, we're already"
    },
    {
      "start": 624.56,
      "duration": 4.24,
      "text": "seeing the start of that. There was in"
    },
    {
      "start": 626.187,
      "duration": 4.293,
      "text": "[clears throat] the how benchmarks there"
    },
    {
      "start": 628.8,
      "duration": 3.52,
      "text": "was one about uh"
    },
    {
      "start": 630.48,
      "duration": 3.68,
      "text": ">> this is the the chroma piece."
    },
    {
      "start": 632.32,
      "duration": 4.32,
      "text": ">> Uh no, this is from Princeton."
    },
    {
      "start": 634.16,
      "duration": 4.4,
      "text": ">> Okay. No, I'm not familiar with it. uh"
    },
    {
      "start": 636.64,
      "duration": 4.96,
      "text": "how I forget now what it stands for but"
    },
    {
      "start": 638.56,
      "duration": 6.399,
      "text": "it's a set of benchmarks for uh really"
    },
    {
      "start": 641.6,
      "duration": 5.28,
      "text": "evaluating longer running agentic tasks"
    },
    {
      "start": 644.959,
      "duration": 3.841,
      "text": "and in this case there was one where"
    },
    {
      "start": 646.88,
      "duration": 4.24,
      "text": "they were evaluating recreating a"
    },
    {
      "start": 648.8,
      "duration": 4.56,
      "text": "research paper and that benchmark came"
    },
    {
      "start": 651.12,
      "duration": 3.04,
      "text": "out in October and it was saturated"
    },
    {
      "start": 653.36,
      "duration": 2.159,
      "text": "earlier this week"
    },
    {
      "start": 654.16,
      "duration": 2.239,
      "text": ">> oh jeez by cloud code"
    },
    {
      "start": 655.519,
      "duration": 4.161,
      "text": ">> oh jeez"
    },
    {
      "start": 656.399,
      "duration": 5.921,
      "text": ">> yeah so and in fact uh they needed to"
    },
    {
      "start": 659.68,
      "duration": 5.599,
      "text": "have humans run the eval do the"
    },
    {
      "start": 662.32,
      "duration": 4.48,
      "text": "evaluation because the solution was like"
    },
    {
      "start": 665.279,
      "duration": 2.881,
      "text": "somewhat like maybe a different approach"
    },
    {
      "start": 666.8,
      "duration": 3.44,
      "text": "that a human would take somewhat"
    },
    {
      "start": 668.16,
      "duration": 4.4,
      "text": "superhuman. You know, the com the common"
    },
    {
      "start": 670.24,
      "duration": 4.56,
      "text": "pattern you see now where actually like"
    },
    {
      "start": 672.56,
      "duration": 4.24,
      "text": "the the gold data set has some errors in"
    },
    {
      "start": 674.8,
      "duration": 2.32,
      "text": "it and so the model correct."
    },
    {
      "start": 676.8,
      "duration": 3.12,
      "text": ">> Yeah."
    },
    {
      "start": 677.12,
      "duration": 3.92,
      "text": ">> Um because if it gets 100 then you're"
    },
    {
      "start": 679.92,
      "duration": 3.2,
      "text": "like, \"Oh, well there something's"
    },
    {
      "start": 681.04,
      "duration": 3.68,
      "text": "wrong.\" Like it's it's a canary for well"
    },
    {
      "start": 683.12,
      "duration": 1.92,
      "text": "something's actually wrong. Yeah."
    },
    {
      "start": 684.72,
      "duration": 1.76,
      "text": ">> Yeah."
    },
    {
      "start": 685.04,
      "duration": 2.96,
      "text": ">> So we should do it on purpose."
    },
    {
      "start": 686.48,
      "duration": 3.84,
      "text": ">> Yeah. And I think we'll just see these"
    },
    {
      "start": 688,
      "duration": 4.24,
      "text": "benchmarks, you know, these new, you"
    },
    {
      "start": 690.32,
      "duration": 4.24,
      "text": "know, really well thought out and really"
    },
    {
      "start": 692.24,
      "duration": 5.279,
      "text": "challenging benchmarks come out and then"
    },
    {
      "start": 694.56,
      "duration": 3.36,
      "text": "very quickly be uh saturated."
    },
    {
      "start": 697.519,
      "duration": 1.681,
      "text": ">> Yeah."
    },
    {
      "start": 697.92,
      "duration": 2.8,
      "text": ">> And we'll continue to see that I think"
    },
    {
      "start": 699.2,
      "duration": 4.48,
      "text": "for more and more challenging tasks."
    },
    {
      "start": 700.72,
      "duration": 4.559,
      "text": ">> Do you find in like just general deell"
    },
    {
      "start": 703.68,
      "duration": 3.839,
      "text": "work and marketing and just like"
    },
    {
      "start": 705.279,
      "duration": 4.721,
      "text": "leadership of a category like do you"
    },
    {
      "start": 707.519,
      "duration": 4.32,
      "text": "find it useful to maintain a benchmark"
    },
    {
      "start": 710,
      "duration": 3.68,
      "text": "or like to to have like oh this is the"
    },
    {
      "start": 711.839,
      "duration": 4.081,
      "text": "contextual benchmark that everyone"
    },
    {
      "start": 713.68,
      "duration": 3.599,
      "text": "should should adopt? I I struggle with"
    },
    {
      "start": 715.92,
      "duration": 3.28,
      "text": "this because obviously a lot of"
    },
    {
      "start": 717.279,
      "duration": 3.841,
      "text": "benchmarks come from research and not"
    },
    {
      "start": 719.2,
      "duration": 2.72,
      "text": "industry but I feel like industry should"
    },
    {
      "start": 721.12,
      "duration": 2.24,
      "text": "have a role."
    },
    {
      "start": 721.92,
      "duration": 4.32,
      "text": ">> Yeah, actually I mean we've been using"
    },
    {
      "start": 723.36,
      "duration": 5.12,
      "text": "that data set from the uh hackathon I"
    },
    {
      "start": 726.24,
      "duration": 3.92,
      "text": "described earlier a benchmark somewhat."
    },
    {
      "start": 728.48,
      "duration": 4.72,
      "text": "I think it's a it's a really interesting"
    },
    {
      "start": 730.16,
      "duration": 5.84,
      "text": "data set because most benchmarks use a"
    },
    {
      "start": 733.2,
      "duration": 5.6,
      "text": "very small set of data to train or to"
    },
    {
      "start": 736,
      "duration": 4.399,
      "text": "inference about and this actually"
    },
    {
      "start": 738.8,
      "duration": 3.039,
      "text": "requires reasoning over"
    },
    {
      "start": 740.399,
      "duration": 2.801,
      "text": ">> yeah this will never fit in a context"
    },
    {
      "start": 741.839,
      "duration": 2.56,
      "text": "window right yes so"
    },
    {
      "start": 743.2,
      "duration": 3.439,
      "text": ">> yeah so I think"
    },
    {
      "start": 744.399,
      "duration": 4.081,
      "text": ">> you know how many tokens it is or you"
    },
    {
      "start": 746.639,
      "duration": 3.281,
      "text": "say there's 100 thousand documents"
    },
    {
      "start": 748.48,
      "duration": 2.32,
      "text": ">> but I don't know how many tokens that"
    },
    {
      "start": 749.92,
      "duration": 1.2,
      "text": "translates to"
    },
    {
      "start": 750.8,
      "duration": 3.76,
      "text": ">> oh"
    },
    {
      "start": 751.12,
      "duration": 4.32,
      "text": ">> I say like thousand each 4,000 each"
    },
    {
      "start": 754.56,
      "duration": 2.64,
      "text": ">> probably more"
    },
    {
      "start": 755.44,
      "duration": 2.32,
      "text": ">> probably more okay so that's many"
    },
    {
      "start": 757.2,
      "duration": 1.84,
      "text": "billions"
    },
    {
      "start": 757.76,
      "duration": 3.44,
      "text": ">> yes Yeah. Okay. Cool."
    },
    {
      "start": 759.04,
      "duration": 3.76,
      "text": ">> Yes. So I think um I would love to see"
    },
    {
      "start": 761.2,
      "duration": 3.759,
      "text": "benchmarks. Yeah. I would love to see"
    },
    {
      "start": 762.8,
      "duration": 3.839,
      "text": "more industry benchmarks because those"
    },
    {
      "start": 764.959,
      "duration": 4.481,
      "text": "would help us actually evaluate at the"
    },
    {
      "start": 766.639,
      "duration": 4.161,
      "text": "scale and not have a toy example as you"
    },
    {
      "start": 769.44,
      "duration": 3.28,
      "text": "know many benchmarks are."
    },
    {
      "start": 770.8,
      "duration": 3.52,
      "text": ">> Yeah. Amazing. And you know just like"
    },
    {
      "start": 772.72,
      "duration": 2.96,
      "text": "the the sort the lore of context"
    },
    {
      "start": 774.32,
      "duration": 4.48,
      "text": "engineering obviously we got a shout out"
    },
    {
      "start": 775.68,
      "duration": 5.76,
      "text": "to Dex who uh did a couple great talks"
    },
    {
      "start": 778.8,
      "duration": 4.96,
      "text": "this year maybe maybe three. I think"
    },
    {
      "start": 781.44,
      "duration": 3.68,
      "text": "also Drew Bernig al written a lot about"
    },
    {
      "start": 783.76,
      "duration": 3.36,
      "text": "the failure modes of context"
    },
    {
      "start": 785.12,
      "duration": 4.24,
      "text": "engineering. What's sticking with the"
    },
    {
      "start": 787.12,
      "duration": 4.08,
      "text": "people that you talk to, right? Like"
    },
    {
      "start": 789.36,
      "duration": 3.84,
      "text": "context rot is it's a it's a well"
    },
    {
      "start": 791.2,
      "duration": 3.92,
      "text": "established term. Shout out to Chroma."
    },
    {
      "start": 793.2,
      "duration": 4.879,
      "text": "Anything else? Context poisoning. I"
    },
    {
      "start": 795.12,
      "duration": 4.56,
      "text": "haven't heard as much, right? So that"
    },
    {
      "start": 798.079,
      "duration": 3.281,
      "text": "that term is not sticking. What what"
    },
    {
      "start": 799.68,
      "duration": 2.959,
      "text": "else is like the topic of conversation"
    },
    {
      "start": 801.36,
      "duration": 3.44,
      "text": "that everyone should know?"
    },
    {
      "start": 802.639,
      "duration": 3.921,
      "text": ">> Yeah. I mean, I think I see context rot"
    },
    {
      "start": 804.8,
      "duration": 3.76,
      "text": "cited in every blog about"
    },
    {
      "start": 806.56,
      "duration": 4.16,
      "text": ">> Yeah. Yeah. And I I would call up my L"
    },
    {
      "start": 808.56,
      "duration": 3.6,
      "text": "on that. Like I told Jeff, I was like,"
    },
    {
      "start": 810.72,
      "duration": 2.96,
      "text": "\"Are you sure this needs to be written"
    },
    {
      "start": 812.16,
      "duration": 3.84,
      "text": "because everyone knows this?\" He's like,"
    },
    {
      "start": 813.68,
      "duration": 2.88,
      "text": "\"No, everyone. Everyone doesn't know"
    },
    {
      "start": 816,
      "duration": 1.279,
      "text": "this."
    },
    {
      "start": 816.56,
      "duration": 2.64,
      "text": ">> Well, exactly."
    },
    {
      "start": 817.279,
      "duration": 3.921,
      "text": ">> It was like obvious to us, but you know."
    },
    {
      "start": 819.2,
      "duration": 4.96,
      "text": ">> Yes. I think it's very intuitive, but I"
    },
    {
      "start": 821.2,
      "duration": 4,
      "text": "think actually having the metrics and"
    },
    {
      "start": 824.16,
      "duration": 1.919,
      "text": "results to show for"
    },
    {
      "start": 825.2,
      "duration": 1.439,
      "text": ">> Yes. He did the work."
    },
    {
      "start": 826.079,
      "duration": 1.841,
      "text": ">> Yes. Exactly."
    },
    {
      "start": 826.639,
      "duration": 2.561,
      "text": ">> He did the work. You like a lot of"
    },
    {
      "start": 827.92,
      "duration": 2.719,
      "text": "people have intuitions just based on"
    },
    {
      "start": 829.2,
      "duration": 2.879,
      "text": "using a model. But actually, if you can"
    },
    {
      "start": 830.639,
      "duration": 3.121,
      "text": "put in a model saying put a number"
    },
    {
      "start": 832.079,
      "duration": 4.801,
      "text": "saying like, well, okay, you know, of"
    },
    {
      "start": 833.76,
      "duration": 5.439,
      "text": "this million token context at 700,000"
    },
    {
      "start": 836.88,
      "duration": 3.36,
      "text": "tokens, your retrieval is actually like"
    },
    {
      "start": 839.199,
      "duration": 2.961,
      "text": "30%."
    },
    {
      "start": 840.24,
      "duration": 4,
      "text": ">> Exactly. Yeah. So now you can compare it"
    },
    {
      "start": 842.16,
      "duration": 4.32,
      "text": "to other performance gaps and kind of"
    },
    {
      "start": 844.24,
      "duration": 4.08,
      "text": "see what's having a bigger impact. I"
    },
    {
      "start": 846.48,
      "duration": 4.32,
      "text": "think uh Anthropic has had some really"
    },
    {
      "start": 848.32,
      "duration": 6,
      "text": "great blogs in this space. I would say"
    },
    {
      "start": 850.8,
      "duration": 5.92,
      "text": "uh there's one on some design and"
    },
    {
      "start": 854.32,
      "duration": 4.48,
      "text": "architecture choices that they put out"
    },
    {
      "start": 856.72,
      "duration": 4.4,
      "text": "that was really interesting uh fairly"
    },
    {
      "start": 858.8,
      "duration": 3.76,
      "text": "fairly early. I would say, you know, it"
    },
    {
      "start": 861.12,
      "duration": 4,
      "text": "didn't come out this year, but I think"
    },
    {
      "start": 862.56,
      "duration": 3.68,
      "text": "MCP has been a huge driver of context"
    },
    {
      "start": 865.12,
      "duration": 5.04,
      "text": "engineering"
    },
    {
      "start": 866.24,
      "duration": 5.599,
      "text": ">> driver and also a flaw I would say as"
    },
    {
      "start": 870.16,
      "duration": 2.64,
      "text": "well. Let's talk about it."
    },
    {
      "start": 871.839,
      "duration": 3.921,
      "text": ">> Yeah,"
    },
    {
      "start": 872.8,
      "duration": 5.279,
      "text": ">> because MCP is this giant JSON thing up"
    },
    {
      "start": 875.76,
      "duration": 5.199,
      "text": "front. You stuffing in the descriptions"
    },
    {
      "start": 878.079,
      "duration": 5.12,
      "text": "of all the tools and so it's very"
    },
    {
      "start": 880.959,
      "duration": 4.32,
      "text": "quickly you get into straight up context"
    },
    {
      "start": 883.199,
      "duration": 3.601,
      "text": "rot when you have like 10 tools,"
    },
    {
      "start": 885.279,
      "duration": 2.8,
      "text": "especially if the tools are fat."
    },
    {
      "start": 886.8,
      "duration": 2.96,
      "text": ">> Yeah. So there's been some really"
    },
    {
      "start": 888.079,
      "duration": 4.241,
      "text": "interesting work on uh some really"
    },
    {
      "start": 889.76,
      "duration": 4.16,
      "text": "interesting blogs on tool use. Uh Manis"
    },
    {
      "start": 892.32,
      "duration": 4.319,
      "text": "had one that was kind of more general"
    },
    {
      "start": 893.92,
      "duration": 4.96,
      "text": "but had some best practices around tools"
    },
    {
      "start": 896.639,
      "duration": 3.681,
      "text": "and Anthropic has written more on on"
    },
    {
      "start": 898.88,
      "duration": 2.319,
      "text": "tool use patterns. So"
    },
    {
      "start": 900.32,
      "duration": 1.12,
      "text": ">> Cloudflare."
    },
    {
      "start": 901.199,
      "duration": 1.521,
      "text": ">> Yeah,"
    },
    {
      "start": 901.44,
      "duration": 3.839,
      "text": ">> that's the other one. Yeah."
    },
    {
      "start": 902.72,
      "duration": 6.16,
      "text": ">> Yeah. We actually uh funny enough going"
    },
    {
      "start": 905.279,
      "duration": 6.961,
      "text": "back to our reanker I actually set up uh"
    },
    {
      "start": 908.88,
      "duration": 6.079,
      "text": "just a prototype of selecting which MCP"
    },
    {
      "start": 912.24,
      "duration": 4.719,
      "text": "servers to use. uh being able to select"
    },
    {
      "start": 914.959,
      "duration": 3.041,
      "text": "those servers is already like a context"
    },
    {
      "start": 916.959,
      "duration": 1.281,
      "text": "challenge because there's so many of"
    },
    {
      "start": 918,
      "duration": 1.199,
      "text": "them."
    },
    {
      "start": 918.24,
      "duration": 1.92,
      "text": ">> It's a sub agent."
    },
    {
      "start": 919.199,
      "duration": 2.961,
      "text": ">> Yeah."
    },
    {
      "start": 920.16,
      "duration": 3.28,
      "text": ">> Interesting. Is that something people"
    },
    {
      "start": 922.16,
      "duration": 2.56,
      "text": "are very excited about? They're"
    },
    {
      "start": 923.44,
      "duration": 2.16,
      "text": "deploying at scale. You see a lot of"
    },
    {
      "start": 924.72,
      "duration": 3.2,
      "text": "traction."
    },
    {
      "start": 925.6,
      "duration": 3.84,
      "text": ">> I think similarly, so I think yes, we've"
    },
    {
      "start": 927.92,
      "duration": 4.8,
      "text": "definitely seen a lot of great use for"
    },
    {
      "start": 929.44,
      "duration": 5.839,
      "text": "it. We have our dynamic agent um use MCP"
    },
    {
      "start": 932.72,
      "duration": 4,
      "text": "servers as well and I think earlier in"
    },
    {
      "start": 935.279,
      "duration": 2.721,
      "text": "the year I made some really fun demos"
    },
    {
      "start": 936.72,
      "duration": 3.2,
      "text": "being able to just really quickly"
    },
    {
      "start": 938,
      "duration": 3.199,
      "text": "combine tools for a prototype. Yeah. So"
    },
    {
      "start": 939.92,
      "duration": 4.56,
      "text": "I think it's really helped people"
    },
    {
      "start": 941.199,
      "duration": 5.601,
      "text": "prototype faster and show value in an"
    },
    {
      "start": 944.48,
      "duration": 5.279,
      "text": "early version to then kind of build out"
    },
    {
      "start": 946.8,
      "duration": 5.279,
      "text": "in larger scale. And I think for us, for"
    },
    {
      "start": 949.759,
      "duration": 4.64,
      "text": "me personally, like in my dynamic agent"
    },
    {
      "start": 952.079,
      "duration": 5.12,
      "text": "configs, I'm moving more toward API"
    },
    {
      "start": 954.399,
      "duration": 4.24,
      "text": "calls and something a little bit more uh"
    },
    {
      "start": 957.199,
      "duration": 3.361,
      "text": "once I've kind of maybe been able to"
    },
    {
      "start": 958.639,
      "duration": 3.76,
      "text": "prototype with an MCP server and figure"
    },
    {
      "start": 960.56,
      "duration": 3.6,
      "text": "out how I'm going to use this. Uh I"
    },
    {
      "start": 962.399,
      "duration": 2.88,
      "text": "think then you can cause you can reduce"
    },
    {
      "start": 964.16,
      "duration": 1.44,
      "text": "the complexity."
    },
    {
      "start": 965.279,
      "duration": 1.68,
      "text": ">> Yeah."
    },
    {
      "start": 965.6,
      "duration": 4.72,
      "text": ">> And reduce that dependency."
    },
    {
      "start": 966.959,
      "duration": 5.361,
      "text": ">> Yeah. Um, mentioning MCP, I don't know"
    },
    {
      "start": 970.32,
      "duration": 3.6,
      "text": "how far you want to go into this, but"
    },
    {
      "start": 972.32,
      "duration": 3.84,
      "text": "um, the MCP gateway finally launched"
    },
    {
      "start": 973.92,
      "duration": 5.279,
      "text": "from Anthropic. There's a bunch of MCP"
    },
    {
      "start": 976.16,
      "duration": 6.08,
      "text": "sort of services that are doing various"
    },
    {
      "start": 979.199,
      "duration": 5.521,
      "text": "sort of discovery and being a registry."
    },
    {
      "start": 982.24,
      "duration": 3.76,
      "text": "What should people know? What are people"
    },
    {
      "start": 984.72,
      "duration": 2.16,
      "text": "betting on? What's what's what's"
    },
    {
      "start": 986,
      "duration": 3.759,
      "text": "actually working"
    },
    {
      "start": 986.88,
      "duration": 6.319,
      "text": ">> in terms of MCP servers or"
    },
    {
      "start": 989.759,
      "duration": 5.2,
      "text": ">> directories, gateway, off, anything of"
    },
    {
      "start": 993.199,
      "duration": 3.44,
      "text": "that nature? basically like everything"
    },
    {
      "start": 994.959,
      "duration": 2.641,
      "text": "that happens after the initial launch of"
    },
    {
      "start": 996.639,
      "duration": 2.801,
      "text": "SCP. Yeah,"
    },
    {
      "start": 997.6,
      "duration": 4.08,
      "text": ">> like there's been a bunch of work."
    },
    {
      "start": 999.44,
      "duration": 4.88,
      "text": "There's MCP UI, but I don't know if"
    },
    {
      "start": 1001.68,
      "duration": 5.04,
      "text": "that's really strictly contexting. So,"
    },
    {
      "start": 1004.32,
      "duration": 5.439,
      "text": ">> yeah, we added our MCP server to the"
    },
    {
      "start": 1006.72,
      "duration": 4.32,
      "text": "registry and it really seemed like uh"
    },
    {
      "start": 1009.759,
      "duration": 1.601,
      "text": ">> the official anthropic one."
    },
    {
      "start": 1011.04,
      "duration": 0.719,
      "text": ">> Yeah."
    },
    {
      "start": 1011.36,
      "duration": 1.76,
      "text": ">> Okay."
    },
    {
      "start": 1011.759,
      "duration": 3.361,
      "text": ">> Yeah. And it really seemed like that"
    },
    {
      "start": 1013.12,
      "duration": 2.639,
      "text": "registry is meant to be read by agents,"
    },
    {
      "start": 1015.12,
      "duration": 0.959,
      "text": "not humans."
    },
    {
      "start": 1015.759,
      "duration": 0.721,
      "text": ">> Yeah."
    },
    {
      "start": 1016.079,
      "duration": 1.68,
      "text": ">> Yeah."
    },
    {
      "start": 1016.48,
      "duration": 2.96,
      "text": ">> This is like a GitHub repo."
    },
    {
      "start": 1017.759,
      "duration": 3.921,
      "text": ">> Yeah. Yeah. Uh, which is interesting"
    },
    {
      "start": 1019.44,
      "duration": 5.519,
      "text": "because I think yes, there's definitely"
    },
    {
      "start": 1021.68,
      "duration": 4.72,
      "text": "value to having strong agent experience"
    },
    {
      "start": 1024.959,
      "duration": 3.6,
      "text": "and I think that's going to be an area"
    },
    {
      "start": 1026.4,
      "duration": 4.88,
      "text": "that's going to be growing uh over the"
    },
    {
      "start": 1028.559,
      "duration": 4.24,
      "text": "next year for sure is being able to let"
    },
    {
      "start": 1031.28,
      "duration": 3.12,
      "text": "agents just like do things without a"
    },
    {
      "start": 1032.799,
      "duration": 3.201,
      "text": "human in the loop. But I do think for"
    },
    {
      "start": 1034.4,
      "duration": 3.6,
      "text": "MCP servers, I think you kind of still"
    },
    {
      "start": 1036,
      "duration": 3.439,
      "text": "want a human to make that selection of"
    },
    {
      "start": 1038,
      "duration": 2.799,
      "text": "what you're going to include in your"
    },
    {
      "start": 1039.439,
      "duration": 4.081,
      "text": "list of tools. You want to check, you"
    },
    {
      "start": 1040.799,
      "duration": 5.441,
      "text": "know, the security and uh other other"
    },
    {
      "start": 1043.52,
      "duration": 5.6,
      "text": "things before you would let it run."
    },
    {
      "start": 1046.24,
      "duration": 4.319,
      "text": ">> Yeah. Amazing. Okay. So more broadly I"
    },
    {
      "start": 1049.12,
      "duration": 3.2,
      "text": "guess any other things you call out in"
    },
    {
      "start": 1050.559,
      "duration": 3.201,
      "text": "like the state of engineering any uh"
    },
    {
      "start": 1052.32,
      "duration": 3.2,
      "text": "state of context engineering any other"
    },
    {
      "start": 1053.76,
      "duration": 3.36,
      "text": "good work by other companies you c shut"
    },
    {
      "start": 1055.52,
      "duration": 3.279,
      "text": "it out manners briefly"
    },
    {
      "start": 1057.12,
      "duration": 3.52,
      "text": ">> yeah I think there's some been there's"
    },
    {
      "start": 1058.799,
      "duration": 4.801,
      "text": "been some really interesting research"
    },
    {
      "start": 1060.64,
      "duration": 3.919,
      "text": "let's say in uh optimizing the system"
    },
    {
      "start": 1063.6,
      "duration": 1.6,
      "text": "prompt okay"
    },
    {
      "start": 1064.559,
      "duration": 2.24,
      "text": ">> so"
    },
    {
      "start": 1065.2,
      "duration": 3.28,
      "text": ">> this is the uh continual prompt learning"
    },
    {
      "start": 1066.799,
      "duration": 2.801,
      "text": "from arise or"
    },
    {
      "start": 1068.48,
      "duration": 2.079,
      "text": ">> no I'm thinking ja"
    },
    {
      "start": 1069.6,
      "duration": 2.959,
      "text": ">> japa"
    },
    {
      "start": 1070.559,
      "duration": 4,
      "text": ">> um I think people are very excited about"
    },
    {
      "start": 1072.559,
      "duration": 4.161,
      "text": "japa the way that I explain it you can"
    },
    {
      "start": 1074.559,
      "duration": 3.921,
      "text": "feel free to correct me is it's kind of"
    },
    {
      "start": 1076.72,
      "duration": 6.319,
      "text": "like an evolution ition of the original"
    },
    {
      "start": 1078.48,
      "duration": 7.199,
      "text": "DSPI idea where you set objectives and"
    },
    {
      "start": 1083.039,
      "duration": 5.361,
      "text": "let LLMs optimize their own prompts by"
    },
    {
      "start": 1085.679,
      "duration": 5.041,
      "text": "looking at output and thinking about and"
    },
    {
      "start": 1088.4,
      "duration": 3.76,
      "text": "reasoning about like what what should"
    },
    {
      "start": 1090.72,
      "duration": 3.92,
      "text": "they continue to add in the prompts to"
    },
    {
      "start": 1092.16,
      "duration": 5.92,
      "text": "improve their evals. Um so it's like a"
    },
    {
      "start": 1094.64,
      "duration": 5.2,
      "text": "nice like sort of pietorch like model of"
    },
    {
      "start": 1098.08,
      "duration": 3.28,
      "text": "a training noob but it's only in the"
    },
    {
      "start": 1099.84,
      "duration": 2.88,
      "text": "prompts not in the weights. Mhm."
    },
    {
      "start": 1101.36,
      "duration": 3.28,
      "text": ">> You can also obviously extend it to the"
    },
    {
      "start": 1102.72,
      "duration": 4.16,
      "text": "weights. And I think the other thing"
    },
    {
      "start": 1104.64,
      "duration": 3.919,
      "text": "about why it's called Jeepa, there's an"
    },
    {
      "start": 1106.88,
      "duration": 3.76,
      "text": "evolutionary element or genetic"
    },
    {
      "start": 1108.559,
      "duration": 3.921,
      "text": "evolutionary element where you roll out"
    },
    {
      "start": 1110.64,
      "duration": 3.84,
      "text": "multiple samples and you select the the"
    },
    {
      "start": 1112.48,
      "duration": 3.28,
      "text": "best survivors from that."
    },
    {
      "start": 1114.48,
      "duration": 3.199,
      "text": ">> Anything else I missed?"
    },
    {
      "start": 1115.76,
      "duration": 4.56,
      "text": ">> Uh no, that captured it really well and"
    },
    {
      "start": 1117.679,
      "duration": 5.841,
      "text": "actually joged my memory of ACE."
    },
    {
      "start": 1120.32,
      "duration": 4.8,
      "text": ">> Yeah. So uh so actually a dented context"
    },
    {
      "start": 1123.52,
      "duration": 3.36,
      "text": "engineering that approach actually has"
    },
    {
      "start": 1125.12,
      "duration": 4.08,
      "text": "shown better benchmark performance on"
    },
    {
      "start": 1126.88,
      "duration": 4.24,
      "text": "financial and other complex document"
    },
    {
      "start": 1129.2,
      "duration": 3.68,
      "text": "sets and the approach they've taken is"
    },
    {
      "start": 1131.12,
      "duration": 3.28,
      "text": "quite interesting. So basically if you"
    },
    {
      "start": 1132.88,
      "duration": 3.28,
      "text": "take an approach like Jeepa and you"
    },
    {
      "start": 1134.4,
      "duration": 3.84,
      "text": "basically like maybe throw out the whole"
    },
    {
      "start": 1136.16,
      "duration": 3.84,
      "text": "prompt and start over or you do like"
    },
    {
      "start": 1138.24,
      "duration": 3.439,
      "text": "many many steps and you kind of like"
    },
    {
      "start": 1140,
      "duration": 2.96,
      "text": "compress and expand and compress and"
    },
    {
      "start": 1141.679,
      "duration": 3.12,
      "text": "expand you're kind of going to lose some"
    },
    {
      "start": 1142.96,
      "duration": 4.24,
      "text": "information and you can see a"
    },
    {
      "start": 1144.799,
      "duration": 4.88,
      "text": "significant drop off in performance. And"
    },
    {
      "start": 1147.2,
      "duration": 4.96,
      "text": "so they're using this agentic approach"
    },
    {
      "start": 1149.679,
      "duration": 4.801,
      "text": "to just make smaller tweaks in the in"
    },
    {
      "start": 1152.16,
      "duration": 3.28,
      "text": "the current prompt rather than rewriting"
    },
    {
      "start": 1154.48,
      "duration": 1.28,
      "text": "from scratch."
    },
    {
      "start": 1155.44,
      "duration": 1.92,
      "text": ">> Yeah,"
    },
    {
      "start": 1155.76,
      "duration": 3.36,
      "text": ">> that's among their innovations for that"
    },
    {
      "start": 1157.36,
      "duration": 3.84,
      "text": "approach that I think is actually kind"
    },
    {
      "start": 1159.12,
      "duration": 5.28,
      "text": "of goes along with what Seth said about"
    },
    {
      "start": 1161.2,
      "duration": 4.719,
      "text": "the KV cache and really like I think um"
    },
    {
      "start": 1164.4,
      "duration": 2.88,
      "text": "agents can get pretty confused pretty"
    },
    {
      "start": 1165.919,
      "duration": 3.601,
      "text": "quickly and that can really degrade"
    },
    {
      "start": 1167.28,
      "duration": 5.04,
      "text": "performance and cause hallucinations."
    },
    {
      "start": 1169.52,
      "duration": 5.279,
      "text": "And so I think wherever you can like one"
    },
    {
      "start": 1172.32,
      "duration": 4.479,
      "text": "use the KV cache for both efficiency but"
    },
    {
      "start": 1174.799,
      "duration": 4.24,
      "text": "also for some sort of like more stable"
    },
    {
      "start": 1176.799,
      "duration": 4.161,
      "text": "environment for the agent to be able to"
    },
    {
      "start": 1179.039,
      "duration": 2.64,
      "text": "take um you know more and more actions"
    },
    {
      "start": 1180.96,
      "duration": 4.8,
      "text": "in."
    },
    {
      "start": 1181.679,
      "duration": 7.201,
      "text": ">> Yeah. How much KV cache decision making"
    },
    {
      "start": 1185.76,
      "duration": 5.52,
      "text": "is there for contest engineering? I feel"
    },
    {
      "start": 1188.88,
      "duration": 4.08,
      "text": "like obviously the answer is the stuff"
    },
    {
      "start": 1191.28,
      "duration": 3.04,
      "text": "that doesn't change put it up front and"
    },
    {
      "start": 1192.96,
      "duration": 4.88,
      "text": "the stuff that does change a lot put it"
    },
    {
      "start": 1194.32,
      "duration": 5.68,
      "text": "at the bottom right. I don't care. And I"
    },
    {
      "start": 1197.84,
      "duration": 4.64,
      "text": "think that's mostly because most agents"
    },
    {
      "start": 1200,
      "duration": 4.559,
      "text": "that I care about are multi-turn and so"
    },
    {
      "start": 1202.48,
      "duration": 4.16,
      "text": "the cash is the whole turn the the the"
    },
    {
      "start": 1204.559,
      "duration": 3.521,
      "text": "the five turns that happened before and"
    },
    {
      "start": 1206.64,
      "duration": 4.64,
      "text": "I'm not changing the system prompt that"
    },
    {
      "start": 1208.08,
      "duration": 5.12,
      "text": "much, right? So cash KB cash would you"
    },
    {
      "start": 1211.28,
      "duration": 5.12,
      "text": "save money if you have you're serving"
    },
    {
      "start": 1213.2,
      "duration": 4.88,
      "text": "the same prompt to a thousand customers."
    },
    {
      "start": 1216.4,
      "duration": 3.6,
      "text": "I guess I guess that's it. But we don't"
    },
    {
      "start": 1218.08,
      "duration": 3.839,
      "text": "do that. So I don't know."
    },
    {
      "start": 1220,
      "duration": 4.24,
      "text": ">> Yeah. I think it can also improve"
    },
    {
      "start": 1221.919,
      "duration": 5.12,
      "text": "performance. But I think as"
    },
    {
      "start": 1224.24,
      "duration": 4.64,
      "text": "conversations take more and more turns,"
    },
    {
      "start": 1227.039,
      "duration": 3.841,
      "text": "I haven't really seen a system that"
    },
    {
      "start": 1228.88,
      "duration": 3.12,
      "text": "handles this well. Like for cursor, I"
    },
    {
      "start": 1230.88,
      "duration": 2.56,
      "text": "get to a certain point in the"
    },
    {
      "start": 1232,
      "duration": 2.88,
      "text": "conversation, I'm just I'm opening a new"
    },
    {
      "start": 1233.44,
      "duration": 4.16,
      "text": "window even if I wasn't done with that"
    },
    {
      "start": 1234.88,
      "duration": 4,
      "text": "conversation because the context bloats"
    },
    {
      "start": 1237.6,
      "duration": 3.28,
      "text": "and you"
    },
    {
      "start": 1238.88,
      "duration": 3.12,
      "text": ">> Yeah. So, so um you know, Dex would call"
    },
    {
      "start": 1240.88,
      "duration": 2.799,
      "text": "this like intentional context"
    },
    {
      "start": 1242,
      "duration": 3.28,
      "text": "compression, intentional frequent"
    },
    {
      "start": 1243.679,
      "duration": 3.441,
      "text": "context compression because like you"
    },
    {
      "start": 1245.28,
      "duration": 2.639,
      "text": "don't trust the model to do compaction"
    },
    {
      "start": 1247.12,
      "duration": 3.12,
      "text": "just yet. Mhm."
    },
    {
      "start": 1247.919,
      "duration": 5.201,
      "text": ">> I would say that both Enthropic and I"
    },
    {
      "start": 1250.24,
      "duration": 4.72,
      "text": "think OpenAI and maybe Gemini as well,"
    },
    {
      "start": 1253.12,
      "duration": 4.96,
      "text": "they're all doing like compaction inside"
    },
    {
      "start": 1254.96,
      "duration": 4.719,
      "text": "the model with the the uh each literally"
    },
    {
      "start": 1258.08,
      "duration": 3.12,
      "text": "each of their frontier releases. I don't"
    },
    {
      "start": 1259.679,
      "duration": 2.561,
      "text": "know if that's interesting to you to you"
    },
    {
      "start": 1261.2,
      "duration": 1.359,
      "text": "or Yeah."
    },
    {
      "start": 1262.24,
      "duration": 2.08,
      "text": ">> Yeah."
    },
    {
      "start": 1262.559,
      "duration": 4.881,
      "text": ">> Any evalu?"
    },
    {
      "start": 1264.32,
      "duration": 5.839,
      "text": ">> Well, actually I did a I like what I"
    },
    {
      "start": 1267.44,
      "duration": 5.76,
      "text": "like to call an embodied eval of Chad"
    },
    {
      "start": 1270.159,
      "duration": 5.601,
      "text": "GPT last winter. I use it as a training"
    },
    {
      "start": 1273.2,
      "duration": 3.92,
      "text": "coach for a snowboarding mogul race."
    },
    {
      "start": 1275.76,
      "duration": 2.72,
      "text": ">> What is a mogul race? You you would"
    },
    {
      "start": 1277.12,
      "duration": 1.84,
      "text": "snowboard around mogul."
    },
    {
      "start": 1278.48,
      "duration": 2.4,
      "text": ">> Yeah."
    },
    {
      "start": 1278.96,
      "duration": 3.28,
      "text": ">> Like a double diamond mogul run. You lap"
    },
    {
      "start": 1280.88,
      "duration": 2.08,
      "text": "25 times."
    },
    {
      "start": 1282.24,
      "duration": 3.04,
      "text": ">> And um"
    },
    {
      "start": 1282.96,
      "duration": 4.32,
      "text": ">> 25. That's a lot."
    },
    {
      "start": 1285.28,
      "duration": 3.68,
      "text": ">> Yeah. It's about 40,000 vertical feet."
    },
    {
      "start": 1287.28,
      "duration": 3.68,
      "text": "Actually, 23. So, right. I mean,"
    },
    {
      "start": 1288.96,
      "duration": 4,
      "text": "training is very important to be able to"
    },
    {
      "start": 1290.96,
      "duration": 3.199,
      "text": "do this and to do it safely."
    },
    {
      "start": 1292.96,
      "duration": 2.4,
      "text": ">> It's also a workout. I don't know how"
    },
    {
      "start": 1294.159,
      "duration": 2.721,
      "text": "calories do you burn on that."
    },
    {
      "start": 1295.36,
      "duration": 3.199,
      "text": ">> Oh, wow. [laughter]"
    },
    {
      "start": 1296.88,
      "duration": 4.48,
      "text": "They give you a big pasta dinner at the"
    },
    {
      "start": 1298.559,
      "duration": 4.721,
      "text": "end. And so, this was a really long-term"
    },
    {
      "start": 1301.36,
      "duration": 3.92,
      "text": "project. It was like three, four months."
    },
    {
      "start": 1303.28,
      "duration": 4,
      "text": "And also it required me taking action in"
    },
    {
      "start": 1305.28,
      "duration": 4.32,
      "text": "the real world in an embodied way and"
    },
    {
      "start": 1307.28,
      "duration": 4.96,
      "text": "then uh having that loop and I just had"
    },
    {
      "start": 1309.6,
      "duration": 4.079,
      "text": "to like um close that window and like"
    },
    {
      "start": 1312.24,
      "duration": 3.439,
      "text": "restart and then I lost a lot of"
    },
    {
      "start": 1313.679,
      "duration": 5.281,
      "text": "training info. So I think like that's"
    },
    {
      "start": 1315.679,
      "duration": 6.321,
      "text": "kind of made me I think pre proactively"
    },
    {
      "start": 1318.96,
      "duration": 4.719,
      "text": "uh limit the number of turns. So maybe"
    },
    {
      "start": 1322,
      "duration": 3.679,
      "text": "I'm missing some of the progress that's"
    },
    {
      "start": 1323.679,
      "duration": 4.48,
      "text": "happened since then. This season I'm"
    },
    {
      "start": 1325.679,
      "duration": 3.601,
      "text": "evaluating multiple models to see who is"
    },
    {
      "start": 1328.159,
      "duration": 2.481,
      "text": "the best coach."
    },
    {
      "start": 1329.28,
      "duration": 3.12,
      "text": ">> Yeah. But then you have to like copy"
    },
    {
      "start": 1330.64,
      "duration": 3.44,
      "text": "paste a good data entry on all these"
    },
    {
      "start": 1332.4,
      "duration": 2.8,
      "text": "models, right? It's that's not great."
    },
    {
      "start": 1334.08,
      "duration": 2.64,
      "text": "Unless you have your own custom"
    },
    {
      "start": 1335.2,
      "duration": 3.12,
      "text": "interface or are you just going to"
    },
    {
      "start": 1336.72,
      "duration": 2.959,
      "text": "chatb.com"
    },
    {
      "start": 1338.32,
      "duration": 2.64,
      "text": "cloudi?"
    },
    {
      "start": 1339.679,
      "duration": 2.561,
      "text": ">> I think I'm going to do like initial"
    },
    {
      "start": 1340.96,
      "duration": 2.079,
      "text": "interviews on Ella Marina."
    },
    {
      "start": 1342.24,
      "duration": 2.799,
      "text": ">> Okay. I see. I see."
    },
    {
      "start": 1343.039,
      "duration": 2.561,
      "text": ">> And then kind of maybe pick one or two."
    },
    {
      "start": 1345.039,
      "duration": 1.041,
      "text": ">> Yeah. Yeah."
    },
    {
      "start": 1345.6,
      "duration": 2.959,
      "text": ">> Yeah."
    },
    {
      "start": 1346.08,
      "duration": 4.32,
      "text": ">> Interesting. Okay. Cool. Um automated"
    },
    {
      "start": 1348.559,
      "duration": 3.6,
      "text": "context engineering is really great. Any"
    },
    {
      "start": 1350.4,
      "duration": 3.68,
      "text": "specifically for code? I guess I don't"
    },
    {
      "start": 1352.159,
      "duration": 4,
      "text": "know how how much you guys encounter"
    },
    {
      "start": 1354.08,
      "duration": 4.16,
      "text": "code. Uh it sounds like sounds like Yes."
    },
    {
      "start": 1356.159,
      "duration": 4.241,
      "text": "Um and is it very different than like"
    },
    {
      "start": 1358.24,
      "duration": 3.52,
      "text": "legal and retail and support all these"
    },
    {
      "start": 1360.4,
      "duration": 4.399,
      "text": "other domains?"
    },
    {
      "start": 1361.76,
      "duration": 6.24,
      "text": ">> Yeah, so our goal has been to create a"
    },
    {
      "start": 1364.799,
      "duration": 5.041,
      "text": "platform that's really end to end and"
    },
    {
      "start": 1368,
      "duration": 5.28,
      "text": "easy to get up and running for any"
    },
    {
      "start": 1369.84,
      "duration": 5.92,
      "text": "domain and any use case. And you know I"
    },
    {
      "start": 1373.28,
      "duration": 4.56,
      "text": "think we saw that firsthand uh with kind"
    },
    {
      "start": 1375.76,
      "duration": 3.919,
      "text": "of a you know a recent beta at that"
    },
    {
      "start": 1377.84,
      "duration": 3.12,
      "text": "hackathon I mentioned for e-commerce"
    },
    {
      "start": 1379.679,
      "duration": 3.36,
      "text": "although we do have customers in that"
    },
    {
      "start": 1380.96,
      "duration": 4.719,
      "text": "domain but I haven't uh really"
    },
    {
      "start": 1383.039,
      "duration": 4.88,
      "text": "interacted with that work very much. And"
    },
    {
      "start": 1385.679,
      "duration": 4.801,
      "text": "so code has been one of the domains"
    },
    {
      "start": 1387.919,
      "duration": 5.361,
      "text": "we've worked with as well. And so we"
    },
    {
      "start": 1390.48,
      "duration": 7.199,
      "text": "actually just used our platform for uh"
    },
    {
      "start": 1393.28,
      "duration": 5.92,
      "text": "test code generation for devices. And"
    },
    {
      "start": 1397.679,
      "duration": 3.761,
      "text": "what we found is kind of the same"
    },
    {
      "start": 1399.2,
      "duration": 5.599,
      "text": "approach we take where we have a"
    },
    {
      "start": 1401.44,
      "duration": 5.359,
      "text": "multimodal ingestion and an ability to"
    },
    {
      "start": 1404.799,
      "duration": 4.401,
      "text": "get the hierarchy of the document"
    },
    {
      "start": 1406.799,
      "duration": 4,
      "text": "contents and then retrieval pipeline"
    },
    {
      "start": 1409.2,
      "duration": 4.32,
      "text": "that includes you know filters,"
    },
    {
      "start": 1410.799,
      "duration": 5.12,
      "text": "rerankers, hybrid search. All of that"
    },
    {
      "start": 1413.52,
      "duration": 3.92,
      "text": "combined is a really great starting"
    },
    {
      "start": 1415.919,
      "duration": 4.401,
      "text": "point and then we're able to hill climb"
    },
    {
      "start": 1417.44,
      "duration": 5.44,
      "text": "very quickly and uh actually had"
    },
    {
      "start": 1420.32,
      "duration": 5.76,
      "text": "state-of-the-art or I guess uh the"
    },
    {
      "start": 1422.88,
      "duration": 5.679,
      "text": "highest uh humanbased evals for that"
    },
    {
      "start": 1426.08,
      "duration": 4.959,
      "text": "customer compared to you know coding"
    },
    {
      "start": 1428.559,
      "duration": 5.12,
      "text": "platforms. So I think it's really I mean"
    },
    {
      "start": 1431.039,
      "duration": 4.561,
      "text": "it requires a bit of customization but I"
    },
    {
      "start": 1433.679,
      "duration": 3.441,
      "text": "think context engineering you know"
    },
    {
      "start": 1435.6,
      "duration": 3.04,
      "text": "applies to code the same way that it"
    },
    {
      "start": 1437.12,
      "duration": 3.36,
      "text": "applies to other domains."
    },
    {
      "start": 1438.64,
      "duration": 3.68,
      "text": ">> Yeah. Awesome. A little bit of"
    },
    {
      "start": 1440.48,
      "duration": 3.28,
      "text": "prediction corner here. What do you"
    },
    {
      "start": 1442.32,
      "duration": 4.08,
      "text": "think is underrated now in concept"
    },
    {
      "start": 1443.76,
      "duration": 4.72,
      "text": "engineering that will be a big topic"
    },
    {
      "start": 1446.4,
      "duration": 3.68,
      "text": "conversation next year"
    },
    {
      "start": 1448.48,
      "duration": 3.28,
      "text": ">> like basically what's underrated right"
    },
    {
      "start": 1450.08,
      "duration": 5.4,
      "text": "like what what what should people talk"
    },
    {
      "start": 1451.76,
      "duration": 3.72,
      "text": "about more but they're not"
    },
    {
      "start": 1455.52,
      "duration": 5.84,
      "text": ">> I think really the full system so I"
    },
    {
      "start": 1459.84,
      "duration": 4,
      "text": "think right now what people are talking"
    },
    {
      "start": 1461.36,
      "duration": 4,
      "text": "about are innovations in one part of the"
    },
    {
      "start": 1463.84,
      "duration": 3.6,
      "text": "system or another like let's say"
    },
    {
      "start": 1465.36,
      "duration": 6.24,
      "text": "different components like a memory"
    },
    {
      "start": 1467.44,
      "duration": 5.68,
      "text": "system or a reanker or you know design"
    },
    {
      "start": 1471.6,
      "duration": 4.559,
      "text": "patterns around compressing context"
    },
    {
      "start": 1473.12,
      "duration": 6.08,
      "text": "things like that and I think in the next"
    },
    {
      "start": 1476.159,
      "duration": 5.841,
      "text": "year we'll have full systems that can be"
    },
    {
      "start": 1479.2,
      "duration": 4.8,
      "text": "kind of a design pattern rather than and"
    },
    {
      "start": 1482,
      "duration": 3.52,
      "text": "and being having a discussion at that"
    },
    {
      "start": 1484,
      "duration": 3.84,
      "text": "level rather than"
    },
    {
      "start": 1485.52,
      "duration": 5.12,
      "text": ">> components. Yeah. Amazing. This is your"
    },
    {
      "start": 1487.84,
      "duration": 4.48,
      "text": "fifth new I would say like well for"
    },
    {
      "start": 1490.64,
      "duration": 3.68,
      "text": "people who've been long-timers I think"
    },
    {
      "start": 1492.32,
      "duration": 3.92,
      "text": "the last time that Nur was in San Diego"
    },
    {
      "start": 1494.32,
      "duration": 3.44,
      "text": "was the Florida Nurips. I don't know if"
    },
    {
      "start": 1496.24,
      "duration": 2.64,
      "text": "you you were there for that."
    },
    {
      "start": 1497.76,
      "duration": 4.48,
      "text": ">> What year was that?"
    },
    {
      "start": 1498.88,
      "duration": 6.72,
      "text": ">> Like 2017 or something. Yeah. Um, how"
    },
    {
      "start": 1502.24,
      "duration": 4.559,
      "text": "did you reflect on the scene changing"
    },
    {
      "start": 1505.6,
      "duration": 2.88,
      "text": "when you come back?"
    },
    {
      "start": 1506.799,
      "duration": 5.601,
      "text": ">> Yeah, it's so interesting. So, my first"
    },
    {
      "start": 1508.48,
      "duration": 4.319,
      "text": "Nuripss was in 2016 in Barcelona."
    },
    {
      "start": 1512.4,
      "duration": 1.12,
      "text": ">> So,"
    },
    {
      "start": 1512.799,
      "duration": 3.76,
      "text": ">> that's nice."
    },
    {
      "start": 1513.52,
      "duration": 4.48,
      "text": ">> Yeah. Uh, so I was uh actually I had"
    },
    {
      "start": 1516.559,
      "duration": 2.961,
      "text": "just finished my graduate research in"
    },
    {
      "start": 1518,
      "duration": 3.679,
      "text": "neuroscience in reward learning and"
    },
    {
      "start": 1519.52,
      "duration": 4.48,
      "text": "decision-m I was doing a posttock at"
    },
    {
      "start": 1521.679,
      "duration": 4.24,
      "text": "Berkeley, but I had a poster from my"
    },
    {
      "start": 1524,
      "duration": 4.88,
      "text": "graduate work. So I flew out for a few"
    },
    {
      "start": 1525.919,
      "duration": 4.88,
      "text": "days and it was uh really not what I"
    },
    {
      "start": 1528.88,
      "duration": 3.84,
      "text": "expected from a research conference"
    },
    {
      "start": 1530.799,
      "duration": 2.721,
      "text": "having mostly attended neuroscience"
    },
    {
      "start": 1532.72,
      "duration": 2.24,
      "text": "research conferences."
    },
    {
      "start": 1533.52,
      "duration": 3.44,
      "text": ">> Oh yeah. How's it different?"
    },
    {
      "start": 1534.96,
      "duration": 4.48,
      "text": ">> So this is neuroinformation processing."
    },
    {
      "start": 1536.96,
      "duration": 4.64,
      "text": ">> I know. I know. So I've run into so many"
    },
    {
      "start": 1539.44,
      "duration": 5.52,
      "text": "neuroscientists here as well and people"
    },
    {
      "start": 1541.6,
      "duration": 4.8,
      "text": "from that um earlier career stage. Uh I"
    },
    {
      "start": 1544.96,
      "duration": 3.04,
      "text": "was not expecting all the industry"
    },
    {
      "start": 1546.4,
      "duration": 2.48,
      "text": "parties. That's just not a thing in"
    },
    {
      "start": 1548,
      "duration": 1.36,
      "text": "neuroscience."
    },
    {
      "start": 1548.88,
      "duration": 1.6,
      "text": ">> Yeah."
    },
    {
      "start": 1549.36,
      "duration": 4.72,
      "text": ">> Yeah. Yeah."
    },
    {
      "start": 1550.48,
      "duration": 4.88,
      "text": ">> And it was just such a smaller, you"
    },
    {
      "start": 1554.08,
      "duration": 2.64,
      "text": "know, it seemed like a large conference"
    },
    {
      "start": 1555.36,
      "duration": 3.52,
      "text": "and everyone there was like, \"Oh, yeah."
    },
    {
      "start": 1556.72,
      "duration": 3.28,
      "text": "It used to be much much"
    },
    {
      "start": 1558.88,
      "duration": 4.56,
      "text": ">> quieter,"
    },
    {
      "start": 1560,
      "duration": 5.12,
      "text": ">> smaller.\" And I think now I'm just like,"
    },
    {
      "start": 1563.44,
      "duration": 3.52,
      "text": "you know, there's so many people. It's"
    },
    {
      "start": 1565.12,
      "duration": 4,
      "text": "their first year and I'm just like not"
    },
    {
      "start": 1566.96,
      "duration": 3.12,
      "text": "seeing some of the same folks I ran into"
    },
    {
      "start": 1569.12,
      "duration": 2.159,
      "text": "in the earlier years."
    },
    {
      "start": 1570.08,
      "duration": 3.68,
      "text": ">> They're still around. You just have to"
    },
    {
      "start": 1571.279,
      "duration": 4.721,
      "text": "find them in parties and like places"
    },
    {
      "start": 1573.76,
      "duration": 4.72,
      "text": "like La Lounge. Very cool. Uh, any calls"
    },
    {
      "start": 1576,
      "duration": 4.159,
      "text": "to action? How can people help you, find"
    },
    {
      "start": 1578.48,
      "duration": 3.6,
      "text": "you, anything like that?"
    },
    {
      "start": 1580.159,
      "duration": 4.081,
      "text": ">> Yeah, we have some really exciting"
    },
    {
      "start": 1582.08,
      "duration": 4.64,
      "text": "updates coming in the domain of context"
    },
    {
      "start": 1584.24,
      "duration": 5.2,
      "text": "engineering. So, you know, follow me,"
    },
    {
      "start": 1586.72,
      "duration": 5.68,
      "text": "Nina Latina, on Twitter or LinkedIn or"
    },
    {
      "start": 1589.44,
      "duration": 3.44,
      "text": "contextual AI and uh just uh"
    },
    {
      "start": 1592.4,
      "duration": 1.279,
      "text": ">> stay tuned."
    },
    {
      "start": 1592.88,
      "duration": 1.919,
      "text": ">> Yeah, stay tuned."
    },
    {
      "start": 1593.679,
      "duration": 4.12,
      "text": ">> Awesome. Thank you."
    },
    {
      "start": 1594.799,
      "duration": 3,
      "text": ">> Thanks."
    },
    {
      "start": 1603.458,
      "duration": 2.02,
      "text": ">> [music]"
    }
  ],
  "fullText": "[music] Light space to wake up. [music] >> We are here back uh with Nina Latutina. Welcome from Contextual. >> Thanks. >> We're going to talk about the state of context engineering in general. But uh one thing I wanted to also give people a sense we're not here of the Nurips flavor discussion and all that. We were talking about like I was asking which are the best nurse after parties and you said Mccor and what's the other one >> Turing and Nvidia >> Turing and Nvidia where there was a really good fireside chat discussion which is rare because I I often tend to shy away from fireside chats >> uh but apparently the guest was Yedin uh Troy who is very well-known figure on these parts. What was it about? >> Yeah, it was about uh kind of the overall state of AI. Uh since she's at Nvidia, they talked about a lot about scaling laws and the future of AI. I think Jonathan Sedaris the uh CEO of Turring who was interviewing her was just uh at asking fun questions as was the audience. I would say uh the last question there was kind of perfect. uh so she focuses on small language models and uh the question was you know wouldn't your research cause Nvidia to to lose a lot of market cap [laughter] and in fact she had emailed her uh research to Jensen Wang before joining and he was like yeah that's great and I think that's fair >> it's really in the world right now >> well no but actually the implication is that uh if you have smaller language models you can do more you can have them run on phones and so you actually could end up using more compute to uh to power more and more small language models to to do more. So >> yeah um I would say though uh so also at Europe's the open router team released their state of AI survey based on open router and they had this very interesting chart on the adoption of small versus medium versus large language models. So the cutoff was small was less than 15 billion and medium was 15 to 70 and large is anything above 70 and you could see the market share of small models trending down over time in practice >> really >> and the market share of medium trending up and then large is staying the same. >> Mhm. >> Yeah. And this is open models only of course >> and obviously the closed models have gone up and up and out. So I don't I don't it's just a just a data point. Yeah. Yeah, I think I think it was one of those things where like Apple intelligence I think is widely acknowledged to be a failure this year and they were kind of the champions of the small models uh on device movements uh that really needed to work and it did not work. Gemini launched uh Gemini Nano in Chrome this year. I haven't used it. I don't know if y is Yeah, basically like the small model hope and dream I think it's it's still in like very small use cases and not rolled out and I don't know what needs to happen for it to roll out. >> Yeah, I would say that makes a lot of sense for general purpose model. I think for uh other component models like let's say like for a reanker due to latency constraints >> of course >> smaller is better is what I've heard from other developers but I can see that being a trend more generally. >> Yeah, you guys had uh a lot of reanker work last year. I don't know is is that like as much of a focus this year for contextual. >> We released the first instruction following reanker in March. >> Okay. >> And we updated it recently. So it's something that I think we'll keep digging to keep up with the state-of-the-art because it is really a key function within context engineering. So for example, when you're reasoning over larger and larger databases, you want to be able to have more recall in those initial retrievalss, but you want more precision for what you're actually putting into the context window for fear of context rod and, you know, poor performance. And so that reanker can really help you narrow down that initial retrieval. I did a brief comment that it's weird that that that was the first instruction following re-ranker because well don't you always wanted to be instruction following and uh the simple answer from uh that I've had I've I've literally asked the search firms this the startups like um you know those people who know will know and they they're all like well you know to be instruction following you have to have uh a larger model and that affects our latency budget and I'm like I don't think that's the case like like the latency for for smaller models are pretty good these days. I don't know. >> Yeah. So, actually that is like the biggest complaint I've heard from developers about our reanker. >> But what we're using it for increasingly is for dynamic agents and there you're >> so that's insensitive, right? Just take however long you want to take. Yeah, exactly. Yeah, that's fair. I'll get to right to context engineering. I think the other thing that people are there was a big topic of conversation this year was quote unquote the uh death of normal rag and the rise of agentic rag. Do you agree? Uh is it o is the debate itself overrated because obviously use whatever in the right context right like it depends. >> I don't know is is this a meaningful debate? >> I mean I think the debate is not so meaningful. I think like progress is meaningful but I think to me it's also like maybe somewhat a decided debate like agentic rag is just generally better than rag even that initial incremental step of making that uh doing query reformulation. So when you receive that initial query, being able to bake it down into subqueries so that you can better match those queries to documents that you might want to retrieve and then combine that for the retrieval. Even that step improves performance so dramatically that you know that's kind of that became the new baseline. >> Yeah. Um that that is obviously so helpful and you can farm it out and parallelize and then re regroup it. Uh so this year the work bear I did on sweet grap uh did that like so we we had like a fast context model where it was trained uh I think a couple innovations I think was really interesting. One uh trained for massively or much more parallel than normal tool calling for searching. So you can sort of uh normal normal parallelism is like one to two maybe maximum four. We train the baseline to be six uh parallel searches at a time and goes up goes up to eight. And then also limited agency that you don't want your agentic search to run forever. You you do want it to terminate at some point and return the answer. So uh incentivizing the RL to do that I think was helpful, easy and actually scaled very well. >> Yes. And actually we have found that uh having turn limits and limits on the sub agents uh checking and validating their work is super important. We recently did uh my uh myself and a member of my team recently participated in a context engineering fund. >> Tell us about it. >> Yeah. So Brian Bishoff and Hamill Hussein hosted it in San Francisco last month mid November and they had about uh just under a 100,000 documents so PDFs all in the retail space. So it's called retail universe and they had log files uh tables giant CSVs files and we used our dynamic agent to answer really challenging queries in this and generate structured output and some of the very early steps we noticed is you know with a data set that large that agent will want to take that sub agent let's say the unstructured retrieval or structured retrieval sub agent will want to take so many turns to make sure they you know looked under every rock in the data set and we actually don't want that that data set is pretty large. And then also it turns out the sub agent would want to check its work over and over and over. So um so that's kind of like something that we noticed with large scale. And so kind of similar to what you found like you don't want unlimited, you know, whether you're enforcing that within like the RL reward structure or with explicit instructions in the system prompt. Yeah, I would say very much this year is kind of the year of the sub agent for me in terms of like the people use using constrained agents to do very specific things and uh sometimes being too general actually is like an antiattern because well it doesn't go very far on its own or it's not very reliable or doesn't have the tools that it needs blah blah blah blah blah right uh in some makes sense also like you also happen to be able to fine-tune the model towards that specific task so you can also hill climb very easily as and and and so I think to me that is like all the right elements of AI engineering that I want to see people do more of and people just needed a term for it and I guess I think people have settled on sub agents as as a term just briefly staying on the hackathon are there any other alternative approaches do you found interesting that you want to shout out >> I actually have a blog I'll write uh sooner about our experience >> blog it I love it >> and so I think the leading team was using Mix Brad and Claude Second place was using cursor >> mace spread. I've uh seen that a couple times. >> Yeah, I think they also have a reanker and some other open source models. >> Okay. >> Um >> interesting name. >> Yeah. >> And I forget what the other team was. And and there was also a human doing the challenge and so there was a human benchmark. >> Oh yeah. How did human do? >> I think they got 23 points. We got I think about 25 and the winner had about 29. So it was like very >> Okay. Super human. Yeah. >> Yeah. >> Yeah. Yeah. Uh very cool. Okay. So let's get right into the meat uh which is context engineering. This very very big year for context engineering. Your company is contextual which like could not be more on the nose. How do you describe this year in context engineering? It's been a very fast year because convex engineering really took hold six months ago and that actually feels like a year. And so I think one thing that stands out to me is that there are a lot of design patterns that um are kind of bubbling up but there isn't like a uniform design that folks are using as the as the architecture. And I think there's a lot of optimization and efficiencies to gain. So I think with a lot of new development you kind of start by you know letting the agent use as many tokens as it wants and then later you figure out how to constrain that and how to optimize it uh let's say like with uh key value caching or other approaches that can help um really scale the technology. So I think it's kind of to me maybe more in a prototyping stage and I'm expecting next year we'll really see scale for context engineering. >> What does scale look like? what will we be able to do end of next year that we cannot do this year or that we're not doing this year. >> I think the kinds of tasks that we'll be able to solve are going to increase. So, uh I think for example, we're already seeing the start of that. There was in [clears throat] the how benchmarks there was one about uh >> this is the the chroma piece. >> Uh no, this is from Princeton. >> Okay. No, I'm not familiar with it. uh how I forget now what it stands for but it's a set of benchmarks for uh really evaluating longer running agentic tasks and in this case there was one where they were evaluating recreating a research paper and that benchmark came out in October and it was saturated earlier this week >> oh jeez by cloud code >> oh jeez >> yeah so and in fact uh they needed to have humans run the eval do the evaluation because the solution was like somewhat like maybe a different approach that a human would take somewhat superhuman. You know, the com the common pattern you see now where actually like the the gold data set has some errors in it and so the model correct. >> Yeah. >> Um because if it gets 100 then you're like, \"Oh, well there something's wrong.\" Like it's it's a canary for well something's actually wrong. Yeah. >> Yeah. >> So we should do it on purpose. >> Yeah. And I think we'll just see these benchmarks, you know, these new, you know, really well thought out and really challenging benchmarks come out and then very quickly be uh saturated. >> Yeah. >> And we'll continue to see that I think for more and more challenging tasks. >> Do you find in like just general deell work and marketing and just like leadership of a category like do you find it useful to maintain a benchmark or like to to have like oh this is the contextual benchmark that everyone should should adopt? I I struggle with this because obviously a lot of benchmarks come from research and not industry but I feel like industry should have a role. >> Yeah, actually I mean we've been using that data set from the uh hackathon I described earlier a benchmark somewhat. I think it's a it's a really interesting data set because most benchmarks use a very small set of data to train or to inference about and this actually requires reasoning over >> yeah this will never fit in a context window right yes so >> yeah so I think >> you know how many tokens it is or you say there's 100 thousand documents >> but I don't know how many tokens that translates to >> oh >> I say like thousand each 4,000 each >> probably more >> probably more okay so that's many billions >> yes Yeah. Okay. Cool. >> Yes. So I think um I would love to see benchmarks. Yeah. I would love to see more industry benchmarks because those would help us actually evaluate at the scale and not have a toy example as you know many benchmarks are. >> Yeah. Amazing. And you know just like the the sort the lore of context engineering obviously we got a shout out to Dex who uh did a couple great talks this year maybe maybe three. I think also Drew Bernig al written a lot about the failure modes of context engineering. What's sticking with the people that you talk to, right? Like context rot is it's a it's a well established term. Shout out to Chroma. Anything else? Context poisoning. I haven't heard as much, right? So that that term is not sticking. What what else is like the topic of conversation that everyone should know? >> Yeah. I mean, I think I see context rot cited in every blog about >> Yeah. Yeah. And I I would call up my L on that. Like I told Jeff, I was like, \"Are you sure this needs to be written because everyone knows this?\" He's like, \"No, everyone. Everyone doesn't know this. >> Well, exactly. >> It was like obvious to us, but you know. >> Yes. I think it's very intuitive, but I think actually having the metrics and results to show for >> Yes. He did the work. >> Yes. Exactly. >> He did the work. You like a lot of people have intuitions just based on using a model. But actually, if you can put in a model saying put a number saying like, well, okay, you know, of this million token context at 700,000 tokens, your retrieval is actually like 30%. >> Exactly. Yeah. So now you can compare it to other performance gaps and kind of see what's having a bigger impact. I think uh Anthropic has had some really great blogs in this space. I would say uh there's one on some design and architecture choices that they put out that was really interesting uh fairly fairly early. I would say, you know, it didn't come out this year, but I think MCP has been a huge driver of context engineering >> driver and also a flaw I would say as well. Let's talk about it. >> Yeah, >> because MCP is this giant JSON thing up front. You stuffing in the descriptions of all the tools and so it's very quickly you get into straight up context rot when you have like 10 tools, especially if the tools are fat. >> Yeah. So there's been some really interesting work on uh some really interesting blogs on tool use. Uh Manis had one that was kind of more general but had some best practices around tools and Anthropic has written more on on tool use patterns. So >> Cloudflare. >> Yeah, >> that's the other one. Yeah. >> Yeah. We actually uh funny enough going back to our reanker I actually set up uh just a prototype of selecting which MCP servers to use. uh being able to select those servers is already like a context challenge because there's so many of them. >> It's a sub agent. >> Yeah. >> Interesting. Is that something people are very excited about? They're deploying at scale. You see a lot of traction. >> I think similarly, so I think yes, we've definitely seen a lot of great use for it. We have our dynamic agent um use MCP servers as well and I think earlier in the year I made some really fun demos being able to just really quickly combine tools for a prototype. Yeah. So I think it's really helped people prototype faster and show value in an early version to then kind of build out in larger scale. And I think for us, for me personally, like in my dynamic agent configs, I'm moving more toward API calls and something a little bit more uh once I've kind of maybe been able to prototype with an MCP server and figure out how I'm going to use this. Uh I think then you can cause you can reduce the complexity. >> Yeah. >> And reduce that dependency. >> Yeah. Um, mentioning MCP, I don't know how far you want to go into this, but um, the MCP gateway finally launched from Anthropic. There's a bunch of MCP sort of services that are doing various sort of discovery and being a registry. What should people know? What are people betting on? What's what's what's actually working >> in terms of MCP servers or >> directories, gateway, off, anything of that nature? basically like everything that happens after the initial launch of SCP. Yeah, >> like there's been a bunch of work. There's MCP UI, but I don't know if that's really strictly contexting. So, >> yeah, we added our MCP server to the registry and it really seemed like uh >> the official anthropic one. >> Yeah. >> Okay. >> Yeah. And it really seemed like that registry is meant to be read by agents, not humans. >> Yeah. >> Yeah. >> This is like a GitHub repo. >> Yeah. Yeah. Uh, which is interesting because I think yes, there's definitely value to having strong agent experience and I think that's going to be an area that's going to be growing uh over the next year for sure is being able to let agents just like do things without a human in the loop. But I do think for MCP servers, I think you kind of still want a human to make that selection of what you're going to include in your list of tools. You want to check, you know, the security and uh other other things before you would let it run. >> Yeah. Amazing. Okay. So more broadly I guess any other things you call out in like the state of engineering any uh state of context engineering any other good work by other companies you c shut it out manners briefly >> yeah I think there's some been there's been some really interesting research let's say in uh optimizing the system prompt okay >> so >> this is the uh continual prompt learning from arise or >> no I'm thinking ja >> japa >> um I think people are very excited about japa the way that I explain it you can feel free to correct me is it's kind of like an evolution ition of the original DSPI idea where you set objectives and let LLMs optimize their own prompts by looking at output and thinking about and reasoning about like what what should they continue to add in the prompts to improve their evals. Um so it's like a nice like sort of pietorch like model of a training noob but it's only in the prompts not in the weights. Mhm. >> You can also obviously extend it to the weights. And I think the other thing about why it's called Jeepa, there's an evolutionary element or genetic evolutionary element where you roll out multiple samples and you select the the best survivors from that. >> Anything else I missed? >> Uh no, that captured it really well and actually joged my memory of ACE. >> Yeah. So uh so actually a dented context engineering that approach actually has shown better benchmark performance on financial and other complex document sets and the approach they've taken is quite interesting. So basically if you take an approach like Jeepa and you basically like maybe throw out the whole prompt and start over or you do like many many steps and you kind of like compress and expand and compress and expand you're kind of going to lose some information and you can see a significant drop off in performance. And so they're using this agentic approach to just make smaller tweaks in the in the current prompt rather than rewriting from scratch. >> Yeah, >> that's among their innovations for that approach that I think is actually kind of goes along with what Seth said about the KV cache and really like I think um agents can get pretty confused pretty quickly and that can really degrade performance and cause hallucinations. And so I think wherever you can like one use the KV cache for both efficiency but also for some sort of like more stable environment for the agent to be able to take um you know more and more actions in. >> Yeah. How much KV cache decision making is there for contest engineering? I feel like obviously the answer is the stuff that doesn't change put it up front and the stuff that does change a lot put it at the bottom right. I don't care. And I think that's mostly because most agents that I care about are multi-turn and so the cash is the whole turn the the the the five turns that happened before and I'm not changing the system prompt that much, right? So cash KB cash would you save money if you have you're serving the same prompt to a thousand customers. I guess I guess that's it. But we don't do that. So I don't know. >> Yeah. I think it can also improve performance. But I think as conversations take more and more turns, I haven't really seen a system that handles this well. Like for cursor, I get to a certain point in the conversation, I'm just I'm opening a new window even if I wasn't done with that conversation because the context bloats and you >> Yeah. So, so um you know, Dex would call this like intentional context compression, intentional frequent context compression because like you don't trust the model to do compaction just yet. Mhm. >> I would say that both Enthropic and I think OpenAI and maybe Gemini as well, they're all doing like compaction inside the model with the the uh each literally each of their frontier releases. I don't know if that's interesting to you to you or Yeah. >> Yeah. >> Any evalu? >> Well, actually I did a I like what I like to call an embodied eval of Chad GPT last winter. I use it as a training coach for a snowboarding mogul race. >> What is a mogul race? You you would snowboard around mogul. >> Yeah. >> Like a double diamond mogul run. You lap 25 times. >> And um >> 25. That's a lot. >> Yeah. It's about 40,000 vertical feet. Actually, 23. So, right. I mean, training is very important to be able to do this and to do it safely. >> It's also a workout. I don't know how calories do you burn on that. >> Oh, wow. [laughter] They give you a big pasta dinner at the end. And so, this was a really long-term project. It was like three, four months. And also it required me taking action in the real world in an embodied way and then uh having that loop and I just had to like um close that window and like restart and then I lost a lot of training info. So I think like that's kind of made me I think pre proactively uh limit the number of turns. So maybe I'm missing some of the progress that's happened since then. This season I'm evaluating multiple models to see who is the best coach. >> Yeah. But then you have to like copy paste a good data entry on all these models, right? It's that's not great. Unless you have your own custom interface or are you just going to chatb.com cloudi? >> I think I'm going to do like initial interviews on Ella Marina. >> Okay. I see. I see. >> And then kind of maybe pick one or two. >> Yeah. Yeah. >> Yeah. >> Interesting. Okay. Cool. Um automated context engineering is really great. Any specifically for code? I guess I don't know how how much you guys encounter code. Uh it sounds like sounds like Yes. Um and is it very different than like legal and retail and support all these other domains? >> Yeah, so our goal has been to create a platform that's really end to end and easy to get up and running for any domain and any use case. And you know I think we saw that firsthand uh with kind of a you know a recent beta at that hackathon I mentioned for e-commerce although we do have customers in that domain but I haven't uh really interacted with that work very much. And so code has been one of the domains we've worked with as well. And so we actually just used our platform for uh test code generation for devices. And what we found is kind of the same approach we take where we have a multimodal ingestion and an ability to get the hierarchy of the document contents and then retrieval pipeline that includes you know filters, rerankers, hybrid search. All of that combined is a really great starting point and then we're able to hill climb very quickly and uh actually had state-of-the-art or I guess uh the highest uh humanbased evals for that customer compared to you know coding platforms. So I think it's really I mean it requires a bit of customization but I think context engineering you know applies to code the same way that it applies to other domains. >> Yeah. Awesome. A little bit of prediction corner here. What do you think is underrated now in concept engineering that will be a big topic conversation next year >> like basically what's underrated right like what what what should people talk about more but they're not >> I think really the full system so I think right now what people are talking about are innovations in one part of the system or another like let's say different components like a memory system or a reanker or you know design patterns around compressing context things like that and I think in the next year we'll have full systems that can be kind of a design pattern rather than and and being having a discussion at that level rather than >> components. Yeah. Amazing. This is your fifth new I would say like well for people who've been long-timers I think the last time that Nur was in San Diego was the Florida Nurips. I don't know if you you were there for that. >> What year was that? >> Like 2017 or something. Yeah. Um, how did you reflect on the scene changing when you come back? >> Yeah, it's so interesting. So, my first Nuripss was in 2016 in Barcelona. >> So, >> that's nice. >> Yeah. Uh, so I was uh actually I had just finished my graduate research in neuroscience in reward learning and decision-m I was doing a posttock at Berkeley, but I had a poster from my graduate work. So I flew out for a few days and it was uh really not what I expected from a research conference having mostly attended neuroscience research conferences. >> Oh yeah. How's it different? >> So this is neuroinformation processing. >> I know. I know. So I've run into so many neuroscientists here as well and people from that um earlier career stage. Uh I was not expecting all the industry parties. That's just not a thing in neuroscience. >> Yeah. >> Yeah. Yeah. >> And it was just such a smaller, you know, it seemed like a large conference and everyone there was like, \"Oh, yeah. It used to be much much >> quieter, >> smaller.\" And I think now I'm just like, you know, there's so many people. It's their first year and I'm just like not seeing some of the same folks I ran into in the earlier years. >> They're still around. You just have to find them in parties and like places like La Lounge. Very cool. Uh, any calls to action? How can people help you, find you, anything like that? >> Yeah, we have some really exciting updates coming in the domain of context engineering. So, you know, follow me, Nina Latina, on Twitter or LinkedIn or contextual AI and uh just uh >> stay tuned. >> Yeah, stay tuned. >> Awesome. Thank you. >> Thanks. >> [music]",
  "fetchedAt": "2026-01-18T18:34:30.873Z"
}